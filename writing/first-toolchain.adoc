
= Running Rust on a FPGA
:last-update-label!:
:imagesdir: images
:source-highlighter: rouge
:rouge-style: github
// We define C++ here, because the ++ is also used as a delimiter in asciidoc
:cpp: C++
:docinfo: shared,private-footer
// :source-highlighter: highlight.js
// :highlightjs-languages: rust, cpp, console

// TODO: This sections sucks. Rewrite it.

Usually one would write code for a FPGA in a hardware description language (HDL) such as Verilog or VHDL. However, this is a very low-level language, and it is difficult to write code in it. This is where HLS comes in.

HLS is a process that allows developers to write high-level code, such as C/C++ or Rust code, and then use a toolchain to automatically convert that code into RTL (register-transfer level) code, which can be used to program an FPGA. This is different from traditional FPGA programming, where developers write RTL code directly.


// TODO: Elaborate
//  Why Rust for HLS: Explain why Rust is a good choice for HLS and how its features such as memory safety and thread safety align with the requirements of HLS.


== Hands on with bambu

The PandA projects maintains and develops a usable framework for research in the Hardware/Software Co-Design area. As a part of that project they also publish a HLS tool called bambu. While bambu is mostly used for research, it is also one of the most complete free and open-source HLS tools available.

=== Introducing the toolchain

It can use the clang or gcc compilers as a frontend and synthesize their output to verilog. As clang is based on LLVM, it can also load LLVM IR directly. The Rust compiler is also based on LLVM and has an option to output LLVM IR. We can then use the generated LLVM IR as an input for bambu. This way it is possible to perform High-level synthesis on Rust code.

So our toolchain uses rustc to compile rust code to LLVM IR, then uses the bambu to generate Verilog. We could then use an RTL synthesis tool like yosys to generate gate-level logic that can be deployed to a FPGA. We will not cover the last step of the toolchain here, because it is not really relevant to the topic of this blog post.

[pikchr]
....
   arrow right 150% "Rust" "Source"
   box rad 10px "Rust Compiler" "Compiler" "(rustc)" fit
   arrow right 190% "LLVM IR" "Intermediate"
   box rad 10px "PandA Bambu" "HLS Synthesizer" "(bambu)" fit
   arrow right 130% "Verilog" "RTL"
   box rad 10px "YOSYS" "RTL Synthesizer" "(yosys)" fit
   arrow right 200% "Json" "Gate-level logic"
....


=== Using bambu for C++

Bambu is build to synthesize C/C++ code.

The example for a function that finds that minimum or maximum in an array is a good example of how to write code that synthesizes well. It is just very C-like code, that mutates its inputs and has a interface that is not very Rust-like.

[source,cpp]
----
include::src/min_max_cpp.cpp[]
----

The function takes a pointer to an array of integers, the number of elements in the array, and two pointers to integers. The function will find the minimum and maximum in the array and write them to the memory locations pointed to by `out_max` and `out_min`.

We synthesize the function using bambu and use bambus integrated test runner to test the function against some testcases. Later we will use the same testcases for the Rust version of the function. The optimization level is set to `-O2` for now, which is the lowest we can go, because the Rust version requires at least some optimizations to be enabled.

.Testcases:
* input=[0,1,2,3,4] num_elements=5 out_max=0 out_min=4
* input=[15,10,5] num_elements=3 out_max=15 out_min=5 

.Synthesizing and testing the C++ function
[%collapsible]
====
[source,console]
----
include::results/min_max_cpp_clang_intro.log[]
----
====

The most relevant aspects of the output are the average number of clock cycles and the total area used. The area is an important metric, because it is the most expensive resource on FPGAs. The number of clock cycles is also important, because it is a reasonable indicator of how fast the function is. 

The tests are executed using verilator, so they are not completly representative of the actual hardware, because Verilator does not simulate timings beyond the clock cycles. So it could be that a design requires fewer cycles, but can only be run at a lower clock frequency. That said, comparing clock cycles should be a somewhat accurate.

=== Using bambu for Rust

When using Rust with bambu, we need to take a few things into account.

First bambu cannot synthesize anything that unwinds the stack, at least when using the LLVM IR backend. Bambu also cannot synthesize terminating the process. This is a bit unfortunate, because unwinding and terminating are the two way that Rust uses to deal with panics. So we must make sure that our code can not panic. When optimizations are disabled, the Rust compiler sometimes inserts exception handlers into the LLVM IR, even though they are not used. On higher optimization levels, the compiler is smart enough to remove those exception handlers. 

Second, our function can not use IO and basically every other impure feature. This includes things like `std::io::println`. This does not affect us in this example, but keep it in mind when writing your own code.

Third, we need to make sure that the function interface is preserved. We can specify that we want the name to be preserved by attaching the `#[no_mangle]` attribute macro to our function. We also need a C-compatible interface, so we need to mark the function as `extern "C"`. While this is not strictly necessary, it is needed for this case, because we use bambus testbench generator to generate testcases for our function. The testbench generator only supports a C interface to call our function.

// TODO: Find example
// Luckily, Rust and LLVM are quite good at understanding out code, and they can often prove that our code can not panic. 

Fourth bambu does not support all LLVM intrinsics. Most notably bambu does not support the llvm vector reduce intrinsics, which Rust uses for vectorizing loops. There are probably other intrinsics that are not supported, but I have not encountered more yet. If we encounter an unsupported intrinsic, our easiest option is to try another optimization level or see if the Rust compiler has an option to disable the use of that intrinsic. If that does not work we can try to modify the generated LLVM IR to remove the intrinsic. A more advanced option is to write a pass for the LLVM IR that replaces the intrinsic with something that is supported by bambu. We won't do that in this blog post, but it is something to keep in mind.

=== Converting the min_max example to Rust

Translating `min_max_cpp` to Rust is not very difficult. We need to access the input array in rust using `array.get_unchecked(i)` instead of `array[i]`, because the latter is always bounds checked. If we used the checked version, our code could panic, which is not synthesizable. Finally, we need to mark the function as `#[no_mangle]`, so that the function name is preserved.

[source,rust]
----
include::src/min_max_rust.rs[]
----

If we compile that code to LLVM IR and try to use bambu with it we get an error that `llvm.vector.reduce.smax.v4i32` is not supported. As mentioned above bambu does not support llvm vector instructions. They probably get inserted by the https://llvm.org/docs/Vectorizers.html#the-loop-vectorizer[LLVM Loop Vectorizer]. We can disable that vectorization pass by passing `-C no-vectorize-loops` to rustc.

By default the Rust compiler generates code that unwinds the stack on panic. The generated LLVM IR will also have a exception handling personality function added to every function in LLVM IR. This is not synthesizable. We tell the compiler to instead terminate the program on panic by passing `-C panic=abort` to rustc.

For that reason we should also make sure that debug assertions are disabled. Debug assertions add extra safety checks, which can make it easier to find and fix bugs, but these checks usually manifest as panics. We can disable them by passing the `-C debug-assertions=off` flag. On every optimization level other than `0` they are automatically disabled, but we want to be sure.

We also need to disable overflow checks, because they will generate panics if overflows occur during arithmetic operations. We can do that by passing `-C overflow-checks=off` to rustc.

It is probably good idea to tell the rust compiler that we are not targeting a specific cpu architecture by passing `-C target-cpu=generic`.

The final command to compile the Rust code to LLVM IR is:

[source,bash]
----
rustc --emit=llvm-ir --crate-type=lib src/min_max_rust.rs -o min_max_rust.ll -C opt-level=0 -C overflow-checks=off -C no-vectorize-loops -C target-cpu=generic -C panic=abort
----

Bambu fails to synthesize the Rust code if all optimizations are disabled. Enabling at least some level of optimization in rustc or enableing a level higher than 1 in bambu fixes the problem. I assume this is because Rust still generates exception handlers in LLVM if optimizations are disabled. We will set the optimization level to `-O2` in bambu, because that way we can compare the results with the {cpp} function which also used that level.

.Synthesizing and testing the rust function
[%collapsible]
====
[source,console]
----
include::results/min_max_rust_intro.log[]
----
====

It is quite interesting to see that the rust version of the function is apparently identical to the {cpp} version. My best guess is that rustc and clang both generated nearly identical LLVM IR, because the function is basically the same.

I would have assumed that the rust version would be slower, because the {cpp} version had optimizations enabled during compilation and synthesis, while we only had synthesis optimizations enabled for the rust version. Also bambu was designed to synthesize {cpp} code, so I assumed it might be better at synthesizing {cpp} code than rust code. I think we get this result, because bambu treats also treats both versions just as LLVM IR, so it does not have any special knowledge about {cpp}. We will do a more detailed comparison of the {cpp} and rust versions later, where the GCC backend is also included, maybe that yields better results.

=== Using idiomatic Rust

While the previous example technically was Rust, it was not very Rust-like.

A more idiomatic Rust version of the same function using slices and iterators.

[source,rust]
----
include::src/min_max_rust_idiomatic.rs[]
----

The new version makes use of Rust's standard library and functional constructs such as iterators and the `fold` method. Functional programming is widely used in Rust and considered idiomatic because it is expressive and encourages best practices like immutability and explicitness. It is also safer than manually indexing into the input pointer with the `offset` method as in the previous version. Additionally, the use of a struct to return the minimum and maximum values is a more natural way to represent the output in Rust, as opposed to using two separate output pointers.

Usually idiomatic Rust-like Rust code is also faster than C-like Rust code, because the compiler can do more optimizations. So lets see if that is the case here. Bambu cannot synthesize this function when the LLVM IR was not optimized by Rust, so we will set the optimization level to `1` in Rust. This makes this result less comparable to the previous one, but it shouldn't make a big difference, because Rust does only basic optimizations at that level.

.Synthesizing and testing the idiomatic rust function
[%collapsible]
====
[source,console]
----
include::results/min_max_rust_idiomatic_intro.log[]
----
====

The results look promising so far. This version is faster and way smaller than the previous two, but that could be because it had the rust compiler set to a higher optimization level. Lets see what happens when we synthesize our functions with all optimizations enabled.

=== Comparing the C++ and Rust versions

For the comparison we will use the same testcases as before. We will test every version of the function once with optimizations set for maximum speed and once with optimizations set for minimum size. For the C++ version we will also compare how the results change when we use gcc instead of clang.

We will consider the synthesis a blackbox, so we wont compare the generated code. We will only compare the execution time and resource usage of the generated code.

.CPP with clang and `-O5`
[%collapsible]
====
[source,console]
----
include::results/min_max_cpp_clang_speed.log[]
----
====

.CPP with clang and `-Os`
[%collapsible]
====
[source,console]
----
include::results/min_max_cpp_clang_size.log[]
----
====

.CPP with gcc and `-O5`
[%collapsible]
====
[source,console]
----
include::results/min_max_cpp_gcc_speed.log[]
----
====

.CPP with gcc and `-Os`
[%collapsible]
====
[source,console]
----
include::results/min_max_cpp_gcc_size.log[]
----
====

.Rust with `-O5` and `-C opt-level=3`
[%collapsible]
====
[source,console]
----
include::results/min_max_rust_speed.log[]
----

When using rust optimization levels _3_ or _2_ the bambu opt level does not matter for this example.
====

.Rust with `-Os` and `-C opt-level=s`
[%collapsible]
====
[source,console]
----
include::results/min_max_rust_size.log[]
----

When using rust optimization levels _s_ the bambu opt level does not matter for this example.
====



.Idiomatic rust with `-Os` and `-C opt-level=s`
[%collapsible]
====
[source,console]
----
include::results/min_max_rust_idiomatic_size.log[]
----
====


.Idiomatic rust with `-O5` and `-C opt-level=3`
[%collapsible]
====
[source,console]
----
include::results/min_max_rust_idiomatic_speed.log[]
----
====

While the idiomatic version is not faster than the non-idiomatic version, it takes up the smallest area of all synthesized design. It is also more readable and easier to understand.

The area of our design is a important metric for us, because bigger circuits mean that we need a bigger more expensive FPGA. Also we can fit more things on the FPGA, if we have more area left. So area is basically the cost of our design.

While the exact amount of resources the circuit takes up depends on how we synthesize the design for hardware, bambu gives us a good estimate of how much area the design will take up. The following chart shows the area of the designs we synthesized:

:vega-lite-filename: chart_estimated_area.vl.json
include::vega-chart.adoc[]



The designs seem to divide into two groups, one with the big designs (> 900 logic elements) and one with the small designs (< 400 logic elements). The big designs are the clang designs and the rust designs optimized for speed. The small designs are the rust designs optimized for size and the idiomatic rust design.

The first thing we can see is that the clang design is does not seem to change, whether we optimize for speed or for area. In the other charts we will see, that they are show the same performance. While we have not looked at the generated circuits, we can assume that they are identical. The same is also true for the gcc designs. This could be because the clang compiler has a tendency to always generate bigger circuits. The gcc compiler on the other hand has a tendency to generate smaller circuits. It could also be because I am stupid and wrote a horseshit Makefile that passes the wrong flags to the compiler. I will fix that now.

With the rust toolchain we do not have such similarities. The versions optimized for size are smaller than the versions optimized for speed. It is interesting to see that the idiomatic rust design is smaller than the C-style Rust desing on both optimization levels. This is probably because the compiler can do more optimizations on the idiomatic version. The smallest design of the bunch is the idiomatic rust design optimized for size.



:vega-lite-filename: chart_average_cycles.vl.json
include::vega-chart.adoc[]


:vega-lite-filename: chart_max_frequency.vl.json
include::vega-chart.adoc[]

:vega-lite-filename: chart_performance.vl.json
include::vega-chart.adoc[]

:vega-lite-filename: chart_area_speed.vl.json
include::vega-chart.adoc[]

:vega-lite-filename: chart_speed.vl.json
include::vega-chart.adoc[]

== TODO: Evaluate the tests
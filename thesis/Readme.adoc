:doctype: book
:last-update-label!:
:imagesdir: images
:source-highlighter: highlight.js
// Available themes: https://highlightjs.org/static/demo/
// :highlightjs-theme: ../../11.8.0/styles/dark
:highlightjs-theme: thesis
:highlightjsdir: highlightjs
:toclevels: 2
:stem:
:toc: macro
:sectanchors:
:notitle:
:title-page: false
:stylesheet: Readme.css
:toclevels: 3
:kroki-server-url: http://localhost:8000
:listing-caption: Listing
:kroki-fetch-diagram: true
:kroki-default-options: inline
:xrefstyle: short
ifdef::env-web-pdf[]
:docinfo: shared-footer
endif::env-web-pdf[]

image::logo_hda.svg[role=logo]

[.university.text-center]
Darmstadt University of Applied Sciences

[.faculty.text-center]
Faculty of Computer Science

[discrete#main-title]
= Is high-level synthesis from Rust possible using existing tools?

[.description.text-center]
Submitted in partial fulfillment of the requirements for the degree of +
Bachelor of Science (B.Sc.)

[.presented-by.text-center]
by +
*Lennart Eichhorn* +
[small]+Matriculation number: 759253+ +


[.other-people]
First Examiner:: Prof. Dr. Stefan Rapp
Second Examiner:: Prof. Dr. Ronald Charles Moore

<<<

CAUTION: Make sure that the conclusion does not introduce new things.

CAUTION: What is missing? What would you like to see?

CAUTION: Weird expressions.

CAUTION: Weird order of chapters, chapter names?

CAUTION: The thing that is used as a source for HLS is a specification. Use that word more.

CAUTION: Clear up if we are doing FPGA firmware dev or HLS

<<<

[discrete]
== Erklärung

Ich versichere hiermit, dass ich die vorliegende Arbeit selbstständig verfasst
und keine anderen als die im Literaturverzeichnis angegebenen Quellen benutzt habe.

Alle Stellen, die wörtlich oder sinngemäß aus veröffentlichten oder noch
nicht veröffentlichten Quellen entnommen sind, sind als solche kenntlich
gemacht.

Die Zeichnungen oder Abbildungen in dieser Arbeit sind von mir selbst
erstellt worden oder mit einem entsprechenden Quellennachweis versehen.

Diese Arbeit ist in gleicher oder ähnlicher Form noch bei keiner anderen
Prüfungsbehörde eingereicht worden.

_Darmstadt, 5. Juli 2023_

[.signature-required]
Lennart Eichhorn

<<<

[discrete]
== Abstract

// A summary of the contents in English of about one page. The following
// points should be addressed in particular:

// * Motivation: Why did this work come about? Why is the topic of the
// work interesting (for the general public)? The motivation should be
// abstracted as far as possible from the specific tasks that may be given
// by a company.
// * Content: What is the content of this thesis? What exactly is covered in
// the thesis? The methodology and working method should be briefly
// discussed here.
// * Results: What are the results of this work? A brief overview of the
// most actual results as a teaser to read the complete thesis.

// Motivation
This thesis explores whether using Rust as a source language for high-level synthesis is possible. PandA Bambu is an open-source research framework for high-level synthesis. It primarily supports C or {cpp} as input languages via Clang or GCC as front ends. The Clang front end makes it possible to process programs in LLVM's intermediate representation (LLVM IR). Rust is a popular modern systems programming language that uses LLVM as a backend for code generation. Because of this, it can output LLVM IR. This should enable us to use Rust as a source language for HLS. 

// Content
This thesis investigates what a toolchain that performs HLS from Rust with Bambu would look like,
which restrictions apply to the Rust code that is used to generate LLVM IR for HLS,
how the resulting designs compare to similar designs generated from C or {cpp} code,
whether the rust ecosystem can be used, and
how to integrate the toolchain with the rust-hdl project.

//Results
This evaluation shows that it is possible to use Rust as a source language for high-level synthesis. A library for using HLS in rust-hdl, a Rust-based HDL, was developed and used to generate designs from different algorithms. When comparing them to designs generated from equivalent {cpp} code, they were similar in terms of size and performance. Most of Rust is supported for HLS with Bambu, with the biggest restriction being that the specification is not allowed to panic. The rust ecosystem can be used, and the library supports synthesizing specifications with dependencies on other crates.

// Mini-discussion / future work
There is potential for improvements as the Rust compiler is not optimized for generating LLVM IR for HLS, and Bambu is mostly optimized for LLVM IR generated by Clang. Our library is somewhat modular and could be used to integrate other high-level synthesis tools with rust.


[discrete]
== Zusammenfassung

// Motivation
Das Ziel dieser Arbeit ist es zu untersuchen, ob es möglich ist Rust als Quellsprache für High-Level-Synthese (HLS) zu verwenden. PandA Bambu ist ein Open-Source Forschungsframework für HLS. Es unterstützt primär C oder {cpp} als Eingabesprachen über Clang, bzw GCC als Frontend. Das Clang Frontend ermöglicht es Programme in LLVM Intermediate Representation (LLVM IR) zu verarbeiten. Rust ist eine beliebte moderne Systemsprache, die LLVM als Backend für Codegenerierung verwendet. Deshalb sollte es möglich sein Rust als Quellsprache für HLS zu verwenden.

//Inhalt
Diese Arbeit untersucht wie eine Toolchain aussehen würde, die HLS aus Rust mit Bambu durchführt,
welche Einschränkungen auf den Rust Code, der verwendet wird, um LLVM IR für HLS zu generieren, zutreffen,
wie die resultierenden Designs im Vergleich zu ähnlichen Designs, die aus C oder {cpp} Code generiert wurden, sind,
ob das Rust Ökosystem verwendet werden kann, und
wie die Toolchain mit dem rust-hdl Projekt integriert werden kann.

//Ergebnisse
Es wird gezeigt, dass es möglich ist Rust als Quellsprache für High-Level-Synthese zu verwenden. Eine Bibliothek für die Verwendung von HLS in rust-hdl, einer Rust basierten HDL, wurde entwickelt und verwendet um Designs aus verschiedenen Algorithmen zu generieren. Im Vergleich zu Designs, die aus äquivalentem {cpp} Code generiert wurden, waren sie im Bezug auf Größe und Performance in den meisten Fällen ähnlich. Die meisten Features von Rust werden unterstützt. Eine der größten Einschränkung ist, dass die Spezifikation keine nicht behebaren Fehler zur Laufzeit auslösen darf (`panic!`). Das Rust Ökosystem kann verwendet werden und die Bibliothek unterstützt die Synthese von Spezifikationen mit Abhängigkeiten zu anderen Crates.

// Mini-Diskussion / zukünftige Arbeiten
Es gibt Potenzial für Verbesserungen, da der Rust Compiler nicht für die Generierung von LLVM IR für HLS optimiert ist und Bambu größtenteils auf LLVM IR optimiert ist, das von Clang generiert wurde. Die Bibliothek ist relativ modular und könnte verwendet werden um andere High-Level-Synthese-Tools die LLVM unterstützen zu erforschen.



toc::[]

<<<

// Start with section and part numbering
:sectnums:
:part-signifier: Part
:partnums:

= Thesis

== Introduction

The popularity of the Rust programming language is rising, and it is one of the most admired new programming languages. It integrates modern tooling like standardized dependency management, testing, documentation generation, formatting, and building. In the future, the adoption will probably increase further, and it could replace {cpp} as the most common system programming language <<Bug22>>.
It is possible that Rust also provides benefits in domains other than systems programming. It has been shown that Rust can be used for other domains such as GPU programming, web development, or logic programming <<Sah22>> <<Byc22>> <<Kyr22>>. These fields can profit from some of the benefits of Rust <<Bug22>> <<Cos19>>.

This paper explores how Rust can be used in the domain of FPGA firmware development. Usually, FPGA firmware is developed in a Hardware Description Language (HDL) such as Verilog or VHDL. In these languages, the programmer has to describe the hardware in detail. This is a low-level approach that can lead to efficient designs, but it is quite time-consuming <<Mil20>>. The rust-hdl project enables us to express hardware descriptions in Rust similar to traditional HDLs <<rusthdl>>. In addition to manually writing hardware descriptions in an HDL, it is also possible to use high-level synthesis (HLS) to generate hardware descriptions in HDLs from an algorithmic description written in high-level programming languages. This provides increased productivity at the cost of slightly less optimized designs <<Mil20>>. This process is currently mostly used with C or {cpp} because they are the most common system programming languages. There are multiple HLS tools available that can synthesize HDL code from {cpp} code. Some of these tools are based on the LLVM compiler infrastructure <<Nan16>>. There is one reported use of Rust as an HLS language, but that is of no use to us as their tools are not available <<Har22>>. As Rust is also based on LLVM, it is possible that it can be used with these tools too. 

// To determine the feasibility of utilizing Rust as a source language for High-Level Synthesis (HLS).
An investigation was conducted to identify HLS tools compatible with Rust. A modular approach was devised for seamlessly integrating HLS tools with rust-hdl <<rusthdl>>. By employing this approach, a proof-of-concept integration was accomplished with the PandA Bambu HLS framework, showcasing the viability of employing Rust as a source language for HLS. Performance evaluation entailed comparing the resultant designs for various algorithms with designs generated from equivalent C code. It was observed that, in the majority of cases, the solution yielded designs comparable to those derived from {cpp}-based workflows.

== State of the art

It is assumed that you have some knowledge of the Rust programming language and systems programming. This section provides an overview of the topics that are relevant to this paper. It also provides an overview of the related work that has been done for HLS from Rust.

=== Target platforms

// TODO: Maybe custom IC instead of ASIC
There are two target platforms for logic design: field-programmable gate arrays (FPGA) and application-specific integrated circuits (ASIC).

==== Short refresher on CPUs

CPUs process their instructions one by one. They are purpose-built machines that are carefully designed for processing lots of instructions in sequence. Traditional programming languages reflect this design for the most part. They are designed to make it easy for humans to write programs that can be executed sequentially, one instruction at a time. There are approaches to parallelism, but they are either limited to having multiple threads of execution that run from top to bottom simultaneously. Or they have some instructions that perform the same operation on a fixed amount of data elements at the same time <<citation-needed>>.

==== What is an FPGA

CAUTION: Is this intro better?

CAUTION: Fix citations

Field-programmable gate arrays (FPGA) are off-the-shelf reconfigurable integrated circuits (IC) capable of implementing any digital circuit <<Bot21>>. As the name field programmable gate array implies, they consist of programmable logic gates with programmable connections between them. So while a CPU is a circuit designed to do one thing, processing instructions in sequence, an FPGA is a circuit designed to emulate other logic circuits <<citation-needed>>. Design cost and time with FPGAs are much lower than for custom circuits. Another benefit of using FPGAs is that the circuit can be upgraded after deployment. The downsides of using FPGAs are lower performance and higher power consumption compared to custom circuits. They are considered compelling options for small to medium-sized projects, as they do not have the high upfront cost of application-specific integrated circuits (ASIC). <<Bot21>>

// FPGA as a prototyping platform
So instead of producing the circuit into a chip, it is possible to program an FPGA that emulates the circuit <<Bot21>>. For example, a design describing the logic gates and connections that make up a CPU can be programmed to the FPGA, so it will behave exactly like the CPU and can process the same instructions. This is mostly used for prototyping because the CPU design can be verified in hardware with external peripherals, like memory and I/O devices. Such a 'soft' CPU will be much bigger and, therefore, slower than a real CPU because a circuit emulated on an FPGA takes up more space than a purpose-built integrated circuit. If everything works as expected, the design can be taped out and manufactured as an ASIC <<citation-needed>>.
//  Hardware designers designed a circuit in a hardware description language (HDL) and simulated it as the first stage of verification. If that works, it can be deployed to an FPGA and tested there.

// // FPGA as computational accelerators
In the past, FPGAs were mostly used like this for prototyping hardware development. This is still the most common use case for FPGAs, but it is not the only one. In recent years the usage of FPGAs as pseudo-general-purpose computational accelerators has become more relevant. Here you do not use them to prototype circuits that will eventually be taped out but as the final platform. FPGAs used in this context are known as _computational FPGAs_. It is somewhat comparable to the use of GPUs as computing accelerators. But where GPUs excel at tasks that perform the same operations in parallel on massive amounts of data, FPGAs can be used for some kind of computations with irregular parallelism with static structure. In opposition to GPUs, it is not yet clear what an appropriate abstraction for the computational pattern used with computational FPGAs is <<citation-needed>>.


// What is an ASIC / Custom accelerator
// TODO: This is a stub
// Another solution is to produce a custom chip that implements the operations in hardware.

// What is an FPGA
// Instead of producing the circuit into a chip, it is possible to program the FPGA to emulate the circuit. FPGAs are off-the-shelf reconfigurable ICs capable of implementing any digital circuit.  Design cost and time with FPGAs are much lower than for custom chips. Another benefit of using FPGAs is that the circuit can be upgraded after deployment. The downsides of using FPGAs are lower performance and higher power consumption compared to custom chips. They are considered compelling options for small to medium-sized projects. Because FPGAs are off-the-shelf components, they don't have the high upfront cost of ASICs. <<Bot21>>


// CN: FPGAs are suitable for small to medium-sized projects because they don't have the high upfront cost of ASICs.
// CN: They use up more space than ASICs, so they are more expensive than the same ASIC produced at high volume.

// TODO: Early FPGA diagram
// TODO: improve note

NOTE: Different FPGA vendors use different names for the same things <<Lah19>>. This paper uses LB, made of LEs, made of LUTs, and FFs.

CAUTION: Actually do what is described in the note above

// How do basic FPGAs work?
At the core, FPGAs are programmable logic elements that can represent a simple boolean logic function. Every logic element has around 4-6 input bits (vendor specific). The inputs are used to address a programmable lookup table. The lookup table contains the truth table of the boolean logic function. The output of the lookup table is coupled with an output register of the LE. This way, the LE can also act as a flip-flop. <<Bot21>>

Multiple logic elements are grouped into a logic block (LB). A logic block provides a local interconnect between the logic elements. The local interconnect is used to connect the inputs of the logic block and the outputs of the logic elements to the inputs of the logic elements. The local interconnect is usually realized as a programmable crossbar interconnect. <<Bot21>>

An FPGA is made up of multiple logic blocks that are connected by programmable routing. There are multiple ways in which global routing can be realized, but the most popular one is island-style architecture. This architecture is based on a two-dimensional grid of horizontal and vertical wires with logic blocks between them. Each logic block has programmable connections to the wires beside it. The horizontal and vertical wires are connected by programmable switches at their crossings. <<Bot21>>

// TODO: If there is much time left: Insert a figure on island-style architecture

// FPGAs consist of configurable logic blocks (CLB) that are connected by programmable interconnects. The CLBs consist of multiple basic logic elements (BLE) and a local interconnect. At the core of each BLE is a Lookup table (LUT)<<Lah19>> <<Bot21>> 

// How are FPGAs programmed
// TODO: Citation needed
All the programmable components are controlled by configuration stored in SRAM cells. FPGAs are programmed with a bitstream that contains the individual bits for every SRAM cell in the correct order. They usually have a serial interface where the FPGA can accept the bitstream and program itself. <<citation-needed>>

// The logic blocks are connected by a network of programmable interconnects. The logic blocks and interconnects are programmed by a bitstream. The bitstream is a binary file that configures the FPGA. The bitstream is generated from a hardware description language (HDL) design. <<Bot21>>

// Explain the concept of the critical path
Every path that the data takes through the circuit needs a certain amount of time. The longer the path and the more things are in it, the longer it takes to take the path. A critical path is a path that must meet certain timing requirements for the design to function properly. <<Cri95>> If the circuit, for example, is clocked, then some actions should probably be finished before the next clock cycle. If they do not, the clock needs to be slowed down, or the circuit will behave in unexpected ways. For this reason, the critical path limits the maximum frequency. <<Bot21>>

// What is the difference in modern FPGAs
Modern FPGAs improve critical path delay by hardening certain features of the FPGA. Hardening means that a feature is implemented in a fixed way instead of being programmable. Which features are hardened depends on the FPGA model. All modern FPGAs include hardened circuitry for arithmetic operations in their logic blocks. The exact details of the hardening depend on the FPGA model, but it usually involves a fast path for the carry bit between logic blocks. This is at least three times faster than a pure LUT-based implementation. <<Bot21>>

DSP blocks are another common feature of modern FPGAs. They minimize the number of soft logic operations needed to implement common DSP algorithms. Like LBs, they are connected to programmable routing. Depending on the FPGA model, they can be configured to perform operations like multiplication, addition, subtraction, accumulation, and/or multiplication-accumulation in various sizes. <<Bot21>> <<Lah19>>

Bigger FPGA designs almost always require a memory buffer. Building a memory buffer out of logic blocks is possible but not very efficient. Modern FPGAs include hardened memory blocks that can be used as memory buffers. They are called block RAM (BRAM). BRAMs are over 100 times denser than soft memory made from LUTs. BRAMs are about 25% of the area of modern FPGAs. <<Bot21>> <<Lah19>>

// What are the steps to convert an RTL design to an FPGA
// TODO: Mapping the netlist to the FPGA architecture is called technology mapping. Citation needed
Converting an RTL design to a bitstream is a multi-step process. The first step is synthesis. The RTL design is converted to a structural gate-level description. This is called a netlist. The netlist is then mapped to the block types available on the specific FPGA model. A series of complex optimizations can also be performed at this stage. After that, the blocks are placed on specific blocks on the FPGA, and the connections between them are routed. Minimizing the routing distance between logic blocks is crucial because routing accounts for over 50% of the critical path delay. As it is now known how every part of the FPGA should behave, the bitstream can be generated. <<Bot21>> 

All these steps are performed by a CAD tool usually provided by the FPGA vendor. For the biggest FPGA vendors, these tools are integrated into their respective IDE. <<citation-needed>> Recent developments in the open-source community have led to the development of open-source tools that can perform these steps. <<citation-needed>>

// TODO: Mention timing analyzers and shit


// How do they compare to ASICs

// Notable FPGA vendors?

// Context of this paper

=== Netlists are synthesized into hardware

// TODO: This is a stub
All hardware design approaches generate a structural gate-level description called netlist, typically in a subset of SystemVerilog. This netlist is then used to generate the actual hardware design in a mostly automated fashion. How this step actually works depends on what kind of Hardware is targeted, e.g., FPGA, ASIC, or fully custom chips.
//<<Fla20>>

This step is basically manufacturing a design. The focus of this paper is only on design and not on manufacturing, so we will not go into detail about this step. It is just important to know that a netlist is something that can be manufactured/deployed to an FPGA.

=== Design using traditional Hardware Description Languages
// What are common HDLs, and what are they used for
// TODO: Systema verilog is most common nowadays
Traditionally, logic design is done in hardware description languages (HDL). There are two established ones, Verilog and VHDL. VHDL has a slightly higher level of abstraction and some features that make it easier to manage bigger projects. In some ways, the relation between Verilog and VHDL is comparable to the relation between C and {cpp} in terms of features and abstraction. Both can be used to model the structure of hardware equally efficiently, so the choice is mostly a matter of personal preference. <<Smi96>>

// TODO: Mention timing

// What do you describe in HDLs
// TODO: Citation needed
Most commonly HDLs are used to describe circuits in a register-transfer level (RTL) abstraction. On RTL, these languages describe registers that can hold state and the combinational (time-independent) logic that connects them. In HDLs, the registers and combinational logic can be bundled into a module to make it reusable. These modules can then be connected to form a larger circuit. This is the basic structure of an HDL design.

// What distinguishes them from programming languages

// What is a typical HDL design flow
A typical HDL design flow consists of four phases, design, verification, synthesis, and implementation. In the design phase, the circuit is designed in an HDL. The design is then verified in various ways. For verifying the behavior of a design, test benches are defined. These test benches (usually also written in HDL) instantiate the module of the design under test (DUT), exercise the inputs, and verify that the outputs behave as expected. A logic simulator is used to execute the test benches. This is comparable to unit testing in programming languages. After the design is verified, a logic synthesis tool is used to synthesize the design into an optimized gate-level logic description (netlist). A formal equivalence tool can then be used to verify that the netlist is equivalent to the original design. In the implementation phase, the netlist is mapped to the target hardware. <<Fla20>>
 
// What are the problems with HDLs
The use of HDLs is mostly limited to hardware engineers

// What are the possible solutions

==== SystemVerilog sample

In SystemVerilog, units of logic can be encapsulated into modules. Modules can have inputs, outputs, and internal states. <<verilog-blinker-listing>> shows a simple module that implements a blinker. The blinker has a clock input and a blinker output. At every rising edge of the clock, an internal counter is incremented by one. Once the counter reaches a certain value, the blinker output is toggled. The counter is reset to zero when the blinker output is toggled.

.Simple blinker module in SystemVerilog
[source.linenums#verilog-blinker-listing,verilog]
----
include::systemverilog-blinker/blinker.sv[]
----

Verification in SystemVerilog can be achieved by using test benches that instantiate the module under test and exercise its inputs. <<blinker-testbench-listing>> shows a testbench for the blinker module. The testbench instantiates the blinker module and connects it to a clock signal. It also asserts that the blinker output toggles every ten clock cycles.

.Testbench for the blinker module in SystemVerilog
[source.linenums#blinker-testbench-listing,verilog]
----
include::systemverilog-blinker/blinker_tb.sv[]
----

// TODO: citations? 
// TODO: More detail?

=== Design using alternative Hardware description languages

// TODO: This is a stub
There are multiple modern HDLs that try to improve on the shortcomings of Verilog and VHDL. Most of them try to bring some features from modern programming languages to hardware design. (linting, formatting, dependency management, namespaces/scoping, better support for multifile projects, etc.). These languages are usually transpiled to Verilog and then synthesized to netlists.

// TODO: List of alternative HDLs

// TODO: Mention timing somewhere

==== rust-hdl

rust-hdl is a Rust crate that allows describing RTL logic in Rust. The logic can then be transpiled to Verilog. rust-hdl also includes tools for simulation and verification. Because a rust-hdl-based design is just a Rust program, it can use most of the Rust ecosystem features. This includes the Rust testing framework for verification and simulation. It also makes it possible to reuse and share designs using the Rust package manager cargo. <<rusthdl>>

.rust-hdl struct for a simple blinker module.
[source.linenums#rusthdl-blinker-struct-listing,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-struct]
----

In rust-hdl logic, blocks (equivalent to Verilog modules) are defined as structs. The field of these logic blocks corresponds to the external and internal ports of the module. External ports are defined as `Signal` with a direction and a data type. If the logic block contains another logic block, it is also defined here. rust-hdl provides some basic logic blocks for common functions like D flip-flops (DFF) or constants. <<rusthdl>> An example of a logic block definition is shown above in <<rusthdl-blinker-struct-listing>>.

The combinational logic that connects these signals is described in the update function. Every rust-hdl signal has a .next field that determines the value of the signal after the update function. The update function must assign a value to all the .next fields. If there are multiple assignments to the .next fields, the last one is valid, like in normal Rust. As Rust can express much more than just combinational logic, rust-hdl only allows a limited synthesizable subset of Rust in the update function. <<rusthdl>> 

.The restrictions of the synthesizable subset include:
- No local variables
- Assignments are only allowed to .next fields of signals
- Function and method calls are limited to a small subset of library functions
- Only range based for loops are allowed

<<rusthdl>> 

.rust-hdl update function for the blinker.
[source.linenums#rusthdl-blinker-update-listing,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-logic]
----

rust-hdl also enforces an additional set of semantic rules about what the update function can contain. For example, it does not allow undriven nets, so there must be at least one assignment to every field's `.next` value. If there are multiple assignments to the next value of a signal, the last one is valid, like in normal Rust. rust-hdl also forbids to use of the `.next` value on the right side of an expression. rust-hdl enforces these rules at compile-time and generates somewhat helpful error messages. <<rusthdl>> An example for a update function is shown above in <<rusthdl-blinker-update-listing>>.

.Simulating and verifying the blinker
[source.linenums#rusthdl-blinker-simulate-listing,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-test]
----

The Rust unit testing framework can be used to simulate and verify rust-hdl modules. <<rusthdl-blinker-simulate-listing>> shows a test case that simulates and verifies that the blinker module toggles its output with the specified frequency (10 cycles in that case). The test case can be run with `cargo test` like any other Rust unit test.

.Generating verilog from the rust-hdl blinker
[source.linenums#rusthdl-blinker-generate-listing,rust]
----
include::rusthdl-blinker/src/main.rs[tag=generate-verilog]
----

rust-hdl can then be used to generate a Verilog description of the design as shown in <<rusthdl-blinker-generate-listing>>. The generated Verilog code can then be used with any Verilog toolchain to deploy the design to an FPGA. <<rusthdl>>

// Justify why rust-hdl is a good choice even though it is more verbose
// TODO: Compare LoC
rust-hdl code is more verbose than Verilog code. This is mostly because of the additional type annotations and the need to explicitly assign the .next value of every signal. 


=== Design using high-level synthesis
// What is HLS
High-level synthesis (HLS) is a process that can generate an RTL specification of a circuit from a description in a high-level programming language. This is a more productive approach than writing RTL code directly, but the resulting designs are usually less efficient. The generated RTL code can then be used in the same way as manually written RTL code. <<Mil20>> <<Nan16>> <<Lah19>> <<Cou09>>


// Performance of HLS compared to RTL
// TODO: Weirdly written (which experiments?)
The main advantages of HLS are reduced design time and lower development cost. On average, the development time of HLS designs is only a third of that of equivalent RTL designs. The tradeoff for the lower development time seems to be that HLS designs, on average, take up around 40% more basic FPGA resources than RTL designs. The performance of HLS designs is around two-thirds of that of an RTL design. These metrics generalize over many different HLS tools and input languages, and there is a big variance between different experiments. In about 40% of the cases, the HLS design was more efficient or performant than the RTL design. These metrics for performance and resource usage show a lot of variance between experiments. In about 40% of designs, the HLS design was actually more performant than the RTL design. In about 30%, it was more resource efficient. However, in terms of development time, there is little variance; in 90% of the experiments, the HLS design was faster than the RTL design. <<Lah19>>

==== How high-level synthesis works
// Intro
// TODO: Remove?
High-level synthesis tools generate a timed RTL implementation of a circuit from a functional specification. A functional specification is an untimed description of the desired behavior of the circuit. <<Cou09>>

// What are the tasks that HLS performs
Usually, HLS tools separate the HLS process into seven tasks. <<Cou09>> They can be separated into three stages.

// Compilation of the functional specification
First, the functional specification is compiled into a formal model. The formal model is usually a control and data flow graph (CDFG). In the CDFG, every node (called a basic block) represents a static sequence of statements. The edges between the nodes represent conditional data flow. Opposed to a normal data flow graph (DFG), the edges can be conditional. <<Cou09>> In this step, transformations can be applied to the functional specification. These transformations can include loop unrolling, inlining, dead code elimination, and other common software compilation optimizations. <<Cou09>> 

// TODO: I don't have a citation, but say this: While these hardware resources can be real hardware resources, at this stage, they are more comparable to Verilog modules. They are not (necessarily) real FPGA hardware resources, but they can also be arbitrary soft logic. ???

// What are the hardware resources
The resulting formal model then has to be mapped onto hardware resources. The types of resources are functional units, storage elements, and connection units. Functional units perform the actual computation on the data. They can be multipliers, arithmetic logic units, or other custom functions. Storage elements are used to store values over multiple cycles. They include registers, memories, or other custom storage elements. Connection units represent the connections between the functional units and storage elements. They include resources like multiplexers and buses. HLS tools include RTL descriptions for all of their supported resource types. <<Cou09>>

// Scheduling, allocation, binding
// TODO:  This binding process does not determine the placement of the resources on the FPGA. (maybe it takes it into account, but not necessary.)
Allocation determines the kinds and number of resources that are needed to satisfy the design constraints. Scheduling then schedules the operations into cycles based on the available resource types. Operations that do not have data dependencies between them can be scheduled in parallel. The next step after scheduling is called binding. During binding operations, get assigned to specific functional units. Variables that carry a value over multiple cycles must be bound to storage elements. Finally, it is assigned which connection units connect which functional units and storage elements. Scheduling information can be used to bind multiple variables with non-overlapping lifetimes to the same storage elements. <<Cou09>> 

// How does the resulting RTL look
The final step of HLS is to generate the RTL architecture for the design. Classically the architecture consists of a controller and a datapath. The datapath contains all the storage elements, functional units, and connection units described earlier. The controller is responsible for driving the datapath. <<Cou09>> 

// Datapath
The datapath contains all the storage elements, functional units, and connection units described earlier. These components can be connected arbitrarily by buses. The datapath inputs are data inputs from external sources and control inputs from the controller. The outputs of the datapath can be data outputs or control outputs. The datapath also receives the control signals from the controller. The control signals determine how the components are connected. They orchestrate the data flow through the datapath. <<Cou09>> 

// Controller
The controller usually consists of three parts. The state register contains the id of the current state. The output logic generates the control signals that drive the datapath based on the current state. The output logic also generates control outputs that can act as inputs for the datapath. The next state logic computes the next state of the FSM based on the inputs and the current state. <<Cou09>> 

// Disclaimer
While this is the typical architecture, different HLS tools can also do it differently. An alternative approach is to generate a processor with a hardcoded program and instruction set optimized for the task at hand. <<citation-needed>>

// TODO: Diagram for typical HLS architecture from Cou09

// TODO: IP Core


==== High-level synthesis tools
// Available HLS tools
HLS tools can be distinguished into two major categories. Those that accept a general-purpose language and those that accept a domain-specific language (DSL) as input. Using a DSL as input can lead to better-performing designs, but it also raises challenges for adoption. A general-purpose language makes it easier for the algorithm designer, who is usually a software developer, to write code. <<Nan16>> The most common input languages for HLS are C based, including C, {cpp}, and SystemC. <<Lah19>> 

// What HLS tools are there
// TODO: Add citation for usually.
// TODO: Add citation for Bambu is the biggest. 
HLS tools are usually available as a part of the IDE provided by the FPGA vendors. For example, AMD Xilinx provides the Vivado HLS tool as part of its IDE, and Intel Altera includes the Intel HLS Compiler in its Quartus Prime IDE. <<intel-hls>> The most popular HLS tool in academia is Vivado HLS <<Lah19>>. There are also a few open-source HLS tools available. The most relevant open-source HLS tool that is currently actively maintained is PandA Bambu. <<Nan16>> 

Bambu provides a research environment to experiment with new ideas in HLS. It can take C/{cpp} and LLVM IR as input. <<Fer21>>

This paper will use Bambu as an HLS tool because it is open-source and supports the latest LLVM version. 

// What are the problems with those languages

CAUTION: Bullet points

// TODO: This section does probably not belong in this part
Old programming languages, we have better alternatives nowadays
More steps in the design flow
More explicit. The register can be anything


==== Bambu HLS
// Bambu intro
Bambu is an open-source academic HLS tool. <<Fer21>> <<Nan16>> Its architecture is designed to make it easy to experiment with new ideas across high-level synthesis and related topics. It supports input specifications in standard C/{cpp} or the intermediate representations of GCC or Clang/LLVM. <<Fer21>>

// Bambu compared to commercial offerings
// TODO: Nan16 has benchmarks

// Bambu architecture overview
Bambu is based on a three-stage design. The front end is used to convert input specifications in various formats into a static single assignment (SSA) IR. The middle end performs various transformations and analyses on the SSA IR. The backend performs the actual architectural synthesis. <<Fer21>>

// Frontend
The front end utilizes a custom GCC or Clang plugin to process an input specification in any of the formats supported by these two compilers. Notably, this includes support for LLVM IR through Clang. Bambu then generates its own SSA IR from the control from the call graph and control flow information provided by GCC or clang. <<Fer21>>


// Middleend
// TODO: The last sentence is shit
The middle end applies a set of analyses and transformations to the specification. This includes common software compilation optimizations such as loop optimizations and dead code elimination. It also includes more HLS-specific optimizations. For example, multiplications and division with constants are replaced with shift and add operations because real multiplication is expensive in hardware. Bitwidth and range analysis optimizations are also performed because it does not need to target a datapath with a fixed width (i.e., 32bit or 64bit). <<Fer21>>

// Backend
The backend performs the actual architectural synthesis. This is mostly done as described in <<_how_high_level_synthesis_works>>. The most significant difference in Bambu is that the synthesis process acts on every function individually. It generates a controller and datapath for every function. If a function calls into another function, the generated logic for the other function is instantiated as a submodule. Bambu optimizes submodules that are shared between multiple modules. Bambu also provides hardcoded optimized modules for functions from common libraries such as `libc` or `libm`. The resulting architecture can be translated to VHDL or Verilog. <<Fer21>>

==== Languages used for High-level synthesis

// TODO Explain systems programming language

=== LLVM

// What is LLVM
// TODO: Find the source and describe how LLVM is used today
LLVM is a modular compiler framework that can be used to build compilers for many different programming languages. It defines a low-level code representation called LLVM intermediate representation (LLVM IR). <<Lat04>>

// What is LLVM IR
LLVM IR is a strongly typed, static single assignment (SSA) based intermediate representation. LLVM IR is designed to be easy to compile to machine code and to optimize. LLVM IR has a low-level, language-independent type system. The type system captures enough type information to safely perform a number of aggressive transformations that would traditionally be attempted only on type-safe languages in source-level compilers. LLVM IR is not intended to be a universal compiler IR, so it does not capture all of the language-specific type information. Some concepts not represented in LLVM IR are classes, inheritance, or exception-handling semantics. <<Lat04>>

// How is LLVM used in compilers
Because of this, compilers based on LLVM provide a front end that processes the program code in the source language. The front end is able to perform language-specific optimizations. The compiler frontend emits LLVM IR, which is passed to the LLVM backend, which performs a variety of transformations and optimizations. The processed LLVM IR is then passed to a code generator backend to translate it into native code for a given target. <<Lat04>> The LLVM project includes code generator backends for many targets. <<Lat04>> <<rustc-guide>>


=== The Rust programming language

// What is Rust
Rust is a modern systems programming language aiming to replace C and {cpp} as the industry standard systems programming language. It offers zero-cost memory safety, a strong type system, and a modern toolchain. Rust surpasses all other common memory-safe languages in terms of performance while offering many modern features that more established systems programming languages tend to lack. For these reasons, it has been voted the most admired/"loved" programming language every year since 2016. <<Bug22>> <<Cos19>> <<Kla23>>

// TODO: Insert rust performance diagram

// What is a systems programming language
Systems programming languages are programming languages that can deal with low-level details of memory management, data representation, and concurrency.
They are often designed for use in resource-constrained, performance-critical, or close-to-hardware programs. They are used to implement operating systems, embedded systems, device drivers, and other software that interacts with hardware. Common systems programming languages include C, {cpp}, and Rust. <<Kla23>> <<Str12>>

// TODO: Explain how the rust compiler uses LLVM
// Details for this are already summarized at <<rustc-guide>>

// Explain the borrow checker and memory safety
// TODO: Ask about the first sentence being an exact quote
// TODO: Make the first quote shorter
Rust is the first industry-supported computer programming language to overcome the longstanding trade-off between the control over resource management provided by lower-level languages for systems programming and the safety guarantees of higher-level languages. <<Bug22>> <<Jun17>> Rust achieves this by enforcing that every variable is always owned by exactly one scope. When that scope ends, the variable is destroyed. The time between creation and destruction of a variable is called its lifetime. Rust provides semantics for moving ownership between scopes. <<Kla23>>
This model of scope-based resource management is called RAII. It is used by many systems programming languages, including {cpp}. While {cpp} enables the programmer to break out of that system by using pointers to variables that are not owned by the current scope or its parents, Rust does not allow this. <<Str12>> Rust instead uses a type of reference with attached lifetime information called a borrow. The borrow checker statically guarantees at compile-time that every borrow always points to a valid thing in memory. It does this by ensuring that the lifetime of every borrow ends at or after the lifetime of the current scope. The compiler also makes sure that there can only be either one mutable borrow or multiple immutable borrows to a value at the same time. This ensures that the values of borrows do not change unexpectedly. The programmer can opt out of the borrow checker by annotation code as `unsafe`. This allows the programmer to use raw pointers and other unsafe constructs. Unsafe code is sometimes necessary to implement low-level data structures, such as Heap memory (`Box` or `Vec`) or types with internal mutability (`Ref`). The standard library provides most of these constructs, so most Rust programs do not need to use unsafe code. <<Kla23>> <<Bug22>> There is ongoing work on verifying that the unsafe code in the standard library is safely encapsulated by its types. <<Jun17>>

// Overview of how the compiler processes Rust code
// TODO: Explain desugaring (the word)
Rust is a compiled language. The Rust compiler (rustc) can compile Rust code to native code for many different platforms. It accomplishes this by compiling Rust to LLVM IR and then using LLVM for code generation. This allows Rust to support many different platforms without having to implement a backend for every platform. It also enables Rust to utilize the large suite of advanced optimizations collected by the LLVM project. The compiler does not generate LLVM IR directly from the input Rust code. Instead, the input code gets passed through multiple IRs, HIR, THIR, and MIR. HIR and THIR still resemble Rust code, but some constructs get desugared. The rust compiler uses these stages to perform type checking and verification. Verified THIR is converted into MIR, which is a CFG-based representation of the code. rustc performs flow-sensitive safety checks like borrow-checking on this level. The MIR is also used to apply various Rust-specific optimizations to the code. The Rust compiler can be configured to output the various intermediate representations instead of generating machine code. <<rustc-guide>>

// Overview of the tooling-related features of Rust
An important part of what makes the Rust ecosystem so productive is that Rust offers standardized tooling. Every Rust project (also called crate) contains a `Cargo.toml` file specifying the project metadata and dependencies. The `cargo` tool can then perform all standard tasks like building, executing, unit testing, integration testing, formatting the code, generating documentation, downloading dependencies, building dependencies, managing dependencies, publishing the project, benchmarking, setting up projects, and more. The official community crate registry `crates.io` can be used to easily find dependencies. It also links to the automatically generated documentation for every crate. The tooling alone makes Rust a much better development experience than most systems languages. <<Bug22>>

// 

=== Design using accelerator design languages

Accelerator design languages (ADL) are a family of programming languages that are specifically designed to be synthesized to HDL. They are mostly similar to programming languages but offer many features that are usually only found in HDLs, like more fine-grained control over timing and memories memory access. <<hdl-to-adl>>



== Concept, implementation, and architecture

// Short overview of the solution and this section
The goal of this paper is to use Rust as a source language for HLS using an existing HLS tool. First, a suitable toolchain has to be designed. Then a concept for integrating the toolchain with rust-hdl has to be developed. Finally, a proof-of-concept implementation will be done to show that the process works and enable evaluation of the resulting solution.

// TODO: Maybe define some criteria for our solution
// * Can synthesize a simple Rust program
// * Can synthesize our md5 implementation
// * Can synthesize most stateless Rust function
// * The synthesized function can use Rust crates from crates.io
// * The existing Rust tooling (linter, formatter, etc.) works with our function

=== Basic toolchain for synthesizing Rust

As no tool currently can synthesize Rust directly, the first step in the toolchain needs to convert the Rust code into a language that can be used by an existing HLS tool. There are multiple HLS tools that support C or {cpp} by using LLVM compiler infrastructure. As a result of using LLVM, some of these tools can also use LLVMs intermediate representation (LLVM IR) directly as an input language. <<Nan16>> The Rust compiler can compile Rust code to LLVM IR. This will be the starting point of the toolchain

The Rust compiler is frequently updated, and the generated LLVM IR usually uses the latest LLVM version. <<citation-needed>> This means that a suited HLS tool needs to be actively maintained to support the generated LLVM IR. The only HLS tool that fulfills these requirements seems to be PandA Bambu. <<Fer21>> SmartHLS and Vivado HLS may also be capable of operating on LLVM IR, but they are not open source and only available as part of an IDE and not as standalone programs, which makes them unsuitable for our use case.

.The toolchain
[pikchr]
....
   arrow right 150% "Rust" "function"
   box rad 10px "Rust Compiler" fit
   arrow right 190% "LLVM IR" "function"
   box rad 10px "PandA Bambu" fit
   arrow right 130% "Verilog" "RTL"
....

The basic toolchain is relatively straightforward. In the first step, the Rust compiler is used to convert a Rust function to an LLVM IR function. The second step passes the generated LLVM IR function to Bambu, which converts it to Verilog. The resulting Verilog can then be used in a larger HDL design.

==== Toolchain proof of concept

CAUTION: Format keccak

// Intro
// TODO: Add a short section on keccak
// TODO: Format keccak name
This toolchain can be used to synthesize Rust functions. The following section will show how an example Rust function can be synthesized. 

.`minmax` function in Rust
[source#minmax-rust-listing.linenums,rust]
----
include::../rust-minmax/src/minmax.rs[tag=function]
----

CAUTION: Unify minmax spelling

The `minmax` function shown in <<minmax-rust-listing>> finds the minimum and maximum in an array of integers. The implementation of the function is kept in a C-like style so that it can be compared to an equivalent C implementation later. The function takes a pointer to an array of integers (`numbers`), the number of elements in the array (`numbers_length`), and two pointers to integers (`out_max` and `out_min`). The function finds the smallest and largest value of the array and writes them to the memory locations pointed to by `out_max` and `out_min`.

// TODO: Explain that a Rust slice is just a pointer to an array with a length
// TODO: Specify: Bambu does not support the LLVM that is generated by slices or smth like that
The function takes a pointer to an array and its size instead of a rust slice because Bambu does not support slices in the interface. This is a limitation of the current implementation of Bambu and could be fixed in the future. The function also needs to be annotated with `#[no_mangle]` to instruct the Rust compiler to preserve the function name in LLVM IR. This is required because Bambu uses the function name to generate the name of the Verilog module. The function is also `extern "C"` to instruct the Rust compiler to generate LLVM IR that is compatible with the standard C ABI. Finally, the interface of the function is marked as `unsafe` because it uses raw pointers.

Inside the function, the pointer and size are converted to a slice (`input`). Local variables are defined for the minimum and maximum values (`local_max` and `local_min`). The function then iterates over the slice and updates the local minimum and maximum values if a smaller or larger value is found. Finally, the local minimum and maximum values are written to the memory locations pointed to by `out_max` and `out_min`. 

.Compile Rust to LLVM IR
[source#rust-to-llvmir-listing,shell]
----
rustc \
  src/minmax.rs -o minmax.ll --crate-type=lib \
  --emit=llvm-ir # Emit LLVM IR instead of machine code
  -C llvm-args=--opaque-pointers=false # Disable opaque pointers
  -C no-vectorize-loops \
  -C panic=abort \
  -C overflow-checks=off \
  -C opt-level=3 \
  -C linker-plugin-lto=on \
  -C embed-bitcode=on \
  -C lto=fat
----

The first step is to compile the function to LLVM IR using the Rust compiler. The command shown in <<rust-to-llvmir-listing>> contains most of the options that are required or recommended to generate LLVM IR that can be used by Bambu. `src/keccak.rs -o keccak.ll --crate-type=lib` specifies the filenames and that a library and not an executable will be built. The most important option is `--emit=llvm-ir`, which tells the compiler to emit LLVM IR instead of machine code.

Recent versions of the Rust compiler will generate LLVM IR with https://llvm.org/docs/OpaquePointers.html[opaque pointers](`ptr` instead of `i32*`) by default. This is not supported by Bambu. Opaque pointers can be disabled by passing `-C llvm-args="--opaque-pointers=false"` to rustc.
// TODO: Explain LLVM bitcode

// TODO: Does Bambu support cpp exceptions?
// TODO: Does Bambu support c exit?
// TODO: Describe Rust panic
By default, the Rust compiler generates code that unwinds the stack on panic. The generated LLVM IR will also have an exception-handling personality function added to every function in LLVM IR. This is not synthesizable by Bambu. The `-C panic=abort` instructs rustc to instead terminate the program on panic. This is also not synthesizable, but Bambu seems to compile the code if the panics can never be reached. It may be possible to adjust Bambu to provide a custom implementation for panic.

The `-C no-vectorize-loops` option disables loop vectorization. This is required because Bambu does not support vector instructions.

Disabling overflow checks for arithmetic operations reduces the number of places where a function can cause a panic significantly. The `-C overflow-checks=off` option disables overflow checks. The Rust compiler can also generate extra debug assertions, which can make finding and fixing some bugs easier. These checks usually manifest in panic at runtime. To avoid panic, the `-C debug-assertions=off` option can be used to disable debug assertions. On every optimization level other than `0`, they are automatically disabled, so this option is not required in this example.

The final set of parameters enables various Rust compiler optimizations. These serve three purposes. The Rust compiler can perform more optimizations than Bambu because it has more information about the code. The second purpose is to reduce the amount of code that Bambu has to synthesize. The third and probably most important purpose is to eliminate dead code that could potentially cause panic, as Bambu does not support that. For this reason, it is basically always required to set the optimization level to any level higher than `1`.

.High-level synthesize LLVM IR into Verilog
[source#llvmir-to-verilog-listing,shell]
----
bambu minmax.ll \
  --compiler=I386_CLANG16 \
  --top-fname=minmax \
  -O3
----

The LLVM IR can then be used with Bambu to generate Verilog. The command shown in <<llvmir-to-verilog-listing>> instructs Bambu to generate a Verilog module named `minmax` from the LLVM IR file `minmax.ll`. The `--compiler=I386_CLANG16` option instructs Bambu to use Clang 16 as the front end for processing input. Bambu supports multiple versions of Clang and GCC. The `--top-fname=minmax` option specifies the name of the top-level function. The top-level function will be used as the entry point in the hardware design, comparable to the `main` function in a C program. This will also be the name of the generated Verilog module. The `-O3` option instructs Bambu to also perform optimizations in its front and middle end.
==== Interface of the generated Verilog module


.Interface of the generated module
[symbolator#minmax-generated-interface]
....
module minmax(
  //# {{control|Control}}
  input clk;
  input reset;
  input start_port;
  output done_port;
  //# {{data|Parameters}}
  input [31:0] Pd5;
  input [31:0] Pd6;
  input [31:0] Pd7;
  input [31:0] Pd8;
  //# {{power|Memory}}
  input [63:0] M_Rdata_ram;
  input [1:0] M_DataRdy;
  output [1:0] Mout_oe_ram;
  output [1:0] Mout_we_ram;
  output [63:0] Mout_addr_ram;
  output [63:0] Mout_Wdata_ram;
  output [11:0] Mout_data_ram_size;
);

endmodule
....

.Descriptions of the control signals
[#control-signals-table,cols="1,4"]
|===
|Port |Description

|`clock`
|The clock signal is used to clock the module

|`reset`
|Resets the module if set to low.

|`start_port`
|The module will start executing if this is high and the function is finished. Pinning this to high will cause the function to repeat.

|`done_port`
|Pulses high for one cycle when the module is done.

|`return_port`
|Contains the return value while `done_port` is high. It can contain random values during function execution. Does only exist if the function has a return value.

|===

The interface of the component is shown in <<minmax-generated-interface>> and can be split into four categories. The first section contains clocking and control signals, including `clock`, `reset`, `start_port`, and `done_port`. These signals' exact meanings are described in <<control-signals-table>>.

The second section contains the function parameters. When LLVM IR is used as input for Bambu, the real names of the function parameters get lost, and they are replaced with numbered names like `Pd5`. The order of the signals stays the same as the order of the inputs to the original function. Bambu always numbers the parameters in order of appearance in the source. The original names can be mapped to the numbered names based on their order. In this case, `Pd5` is `numbers_length`, `Pd6` is `numbers`, `Pd7` is `out_max`, and `Pd8` is `out_min`. Memory pointers got converted to 32-bit numbers, but Bambu can also be configured to generate other address sizes.

// TODO: Ask if this kind of memory interface has a name
The original function takes a pointer to memory as input, so the generated module needs to be able to access that memory. Bambu generates a memory interface for the component which needs to be connected to memory. During function execution, the component will use this memory interface to retrieve and modify the input values. By default, Bambu generates a memory interface for memory with 32-bit data and 32-bit addresses with two direct parallel connections. Most of the signals are double the size they would need to be for a single connection. For these signals, the first half controls the first channel, and the second half contains the second channel. <<memory-interface-table>> describes the memory interface in more detail.

CAUTION: Use STEM in this table

// TODO: Make tables wider than text if necessary
.Descriptions of the memory interface
[#memory-interface-table,cols="1,1,3"]
|===
|Port |Size per channel (bit) |Description

|`Mout_oe_ram`
|1
|Set to 1 to read from the channel.

|`Mout_we_ram`
|1
|Set to 1 to write to the channel.

|`Mout_data_ram_size`
|log2(dataWidth) + 1
|Set the width of bits that should be written to the memory. It can be a value between 0 and the width of your data.

|`Mout_addr_ram`
|addressWidth
|Select the address this channel should operate on.

|`M_Wdata_ram`
|dataWidth
|Contains the data that will be written to memory if `Mout_we_ram` is set.

|`M_Rdata_ram`
|dataWidth
|Contains the data that was read from memory if `Mout_oe_ram` was set in the last cycle.

|`M_DataRdy`
|1
|Nonzero if the memory is not ready

|===


=== Integrating the toolchain with rust-hdl

//TODO: Verilator background section
// With rust macros, it is possible to execute arbitrary code transformations at compile-time. This can be used 
Using the toolchain, as shown in the previous section, requires a few manual steps. Especially the changing parameter names are difficult to work with as they need to be adjusted after every modification. A library for integration with rust-hdl was created to make the toolchain easier to use. The library makes it possible to create a single Rust crate containing both RTL and HLS code in the same project. The library contains steps for synthesizing specific functions from a bigger Rust project, wrapping synthesized Verilog in a rust-hdl struct, highlighting errors in the source code before synthesis, and linking simulations generated with Verilator.

// TODO: Move to the background
Cargo provides a way to hook into the build process using build scripts. These are small Rust programs that can modify the Source code before it is passed to the Rust compiler. The Rust compiler also allows custom code transformations at compile-time using procedural macros. Procedural macros backed by Rust functions that are executed at compile-time and can modify the code they are applied to.

The Rust library provides a `+#[hls]+` macro that can be used to mark Rust modules for HLS. The macro is evaluated two times. Before compilation, a build script finds all `+#[hls]+` macros. It then extracts every marked module into a separate temporary crate. This crate is then synthesized to Verilog using the toolchain mentioned above. A Rust module containing a rust-hdl struct that wraps the synthesized Verilog is then generated. That Rust module is then placed alongside the original source code. During compilation, the `+#[hls]+` macro is evaluated by the Rust compiler. The macro then emits import directives for the module that were synthesized during the build script. It also evaluates if the macro is used in a valid context and emits useful error messages if it is not. <<macro-evaluation-overview>> shows an overview of the macro evaluation process.

.Block diagram of the macro evaluation before and during compilation
[nomnoml.completly-oversized-content#macro-evaluation-overview,opts=inline,width=20cm]
....
#gutter: 10
#fontSize: 18
#leading: 1.25
#lineWidth: 2
#padding: 14
#spacing: 40
#stroke: #000000
#font: Spectral
#direction: right
#fill: #f7f8f7; #ffffff; #f7f8f7; #ffffff; #f7f8f7; #ffffff
#.downwards: direction=down
[cargo build|
[<downwards> Buildscript|
  [parse rust crate]->[find marked modules]
  [find marked modules]->[extract crate]
  [extract crate]->[compile to LLVM IR]
  [compile to LLVM IR]->[synthesize with Bambu]
  [synthesize with Bambu]->[parse result]
  [parse result]->[fix parameter names]
  [fix parameter names]->[generate rust_hdl interface]
  [generate rust_hdl interface]->[create rust_hdl module]
  [parse result]->[create {cpp} library]
  [create {cpp} library]->[create rust_hdl module]
  [create rust_hdl module]->[place new rust file in original crate]
  [place new rust file in original crate]->[emit directives to link {cpp} library]
]

[<downwards>Rust compiler|
[<downwards> HLS macro evaluation|
  [parse rust macro]->[check for errors]
  [check for errors]->[emit import directives for synthesized module]
]
[<downwards> Compilation]
[HLS macro evaluation]->[Compilation]
]

[Buildscript]->[Rust compiler]
]
....

Rust-hdl is not able to simulate embedded Verilog. Verilator can create a {cpp} library from Verilog that simulates a single component. Such a library is created for every synthesized module. The generated rust-hdl structs contain the glue code necessary to call their library. The build script emits the directives necessary to link the generated libraries. Then Cargo will link the libraries into the final binary after compilation.

Using this approach makes it possible to use the toolchain in a rust-hdl project. The modules can be used, simulated, and tested like regular rust-HDL modules. This also allows us to perform behavioral verification directly in rust unit tests. This workflow makes it possible to use HLS from Rust in a fully automated way.

=== Example of using the integration

// TODO: Write section
Lorem ipsum dolor sit amet.

== Experiments and results

CAUTION: Write some text

Hypothesis: HLS from Rust is as performant as HLS from {cpp}
Hypothesis: Using packages makes development faster
Hypothesis: With rust, we can easily use packages

=== Experimental setup
// Algorithm -(implementation)> implementation -(synthesis)> design

// TODO: I think there is probably a better word for `equivalent Rust implementation`. Maybe `algorithmically identical Rust implementation`?
A set of algorithms will be compared to evaluate how HLS from Rust compares to HLS from {cpp}. Every algorithm will have a {cpp} and equivalent Rust implementation. The equivalent Rust implementation is algorithmically as close to the {cpp} implementation as possible. When possible, a more Rust-like implementation, using more Rust features like slices or iterators, is also compared. It will be referred to as the _idiomatic_ implementation. The goal of the equivalent Rust implementation is to compare the performance of the toolchain and compiler. Comparing the idiomatic Rust implementation can show whether the toolchain is able to synthesize idiomatic Rust code. If an implementation for a specific algorithm is available on crates.io, that implementation will also be added to the comparison. This shows whether we can use the Rust ecosystem to speed up development.

// TODO: Criticize that the GCC version is outdated af
The {cpp} implementations will be synthesized using Bambu with both the GCC (Version 8) and clang (Version 16) front end. Rust implementations will be synthesized using the toolchain described in the previous section, which is also based around Bambu. All implementations will be synthesized once with optimizations for size (`-Os`) and once with optimizations for speed (`-O3`).

// TODO area is also RAM and DSPs

Different designs will be evaluated for the area of an FPGA they take up, their clock maximum frequency, and the number of clock cycles they take for a single operation. In case a design has varying clock cycles, the average of all test cases will be used. The area and maximum frequency of a design will be taken from the placement report of nextpnr. nextpnr will be configured to target a Lattice ECP5 FPGA with 44k LUT and a speed grade of 6 (LFE5U-45F-6BG381C) for all tests. Memory will be configured to use only a single channel instead of the default of two channels. This makes it easier to compare the traces of different designs. The area of a design is measured as the sum of the number of LUTs, flip-flops, BRAM blocks, and DSP blocks. The number of clock cycles will be measured using the rust_hdl simulator for Rust designs and the Bambus builtin testbench generator {cpp} designs. As both cases use Verilator to generate the actual simulation for the designs, the results are comparable.
 
This evaluation focuses only on comparing the behavior and not on the exact implementation of the generated hardware designs. Because of this, the synthesized Verilog files will be mostly treated as black boxes.

=== Tested algorithms
//TODO: Stop using algorithms and function interchangeably

Three functions will be compared.

The minmax function finds the minimum and maximum values in an array of signed 32-bit integers. This algorithm is chosen because it is simple and can be implemented in a single function. This makes it easy to compare the performance of the toolchain and compiler. It is also one of the examples included in the Bambu repository.

// TODO: A background section on keccak
// TODO: Stylize keccak-f[1600] with latex math
The keccak-f[1600] function is a cryptographic function often used as a hash function. It was chosen because it is an algorithm that is commonly used and reasonably complex. This makes it a good example of how well the toolchain can optimize complex algorithms. It takes a pointer to 25 64-bit numbers and permutates them in place.

// TODO: This is not good. Rewrite
// The md5 hash function is another hash function. It was also chosen because it is a common algorithm. While md5 is no longer considered secure, it is also reasonably easy to implement.

// TODO: Find better functions. At least not two hash functions

// TODO: If there is time, add an FIR filter: https://www.hackster.io/michi_michi/fpga-fir-filter-hls-kria-kv260-pynq-2eec35

// TODO: === Show limitations of the synthesizable subset of Rust
// Show the limitations of the toolchain. Limitations of the synthesizable subset of Rust.
// Somewhere but not here

Each {cpp} implementation was tested in four configurations. Either compiled with Bambus GCC or Clang frontend, once with optimization profiles for speed (`-O5`) and once for size (`-Os`). The equivalent Rust implementations were also tested once with optimization profiles for speed and size. When optimizing a Rust implementation for speed, rustc is set to `-O3` and Bambu to `-O5`. When optimizing for size, rustc is set to `-Oz` and Bambu to `-Os`.

// TODO:  === Show how the Rust ecosystem can be used
// not here
// TODO:  === Generated modules
// not here

=== Testing the minmax functions

Three implementations of a minmax function will be compared. The Rust implementation was already shown above in <<minmax-rust-listing>>. It is based on the {cpp} implementation shown in <<minmax-cpp-listing>>. The third implementation is shown in <<minmax-idiom-listing>>. It is a more Rust-like implementation that is using iterators and a return value. It will be referred to as the idiomatic Rust implementation.

.Idiomatic `minmax` implementation in Rust
[source#minmax-idiom-listing.linenums,rust]
----
include::../rust-minmax/src/minmax_idiomatic.rs[tag=function]
----

The Rust compiler should be able to compile the idiomatic Rust implementation to mostly the same code as the equivalent Rust implementation. The biggest difference is that the idiomatic function returns a struct instead of writing its result to memory. The expectation is that the idiomatic Rust design always performs a bit better than the equivalent Rust design, as it does not have to access memory for writing the result. It has a specific `return_port` that contains the result.


// TODO: Replace this with the blog post's text, as that is better.
The Rust, idiomatic Rust, and Clang designs optimized for speed all take up around 3x as much space as the designs optimized for size. With the same optimization settings, all designs that are based on LLVM toolchains need roughly the same space. The GCC designs do not show this result. The design optimized for speed is only a little 1.2x bigger than the one optimized for size. That one is also the smallest design overall. Notably, even the GCC design optimized for speed is the third smallest design. Both designs based on the idiomatic Rust implementation take up a little less space than the respective design based on the C-like Rust implementation.

It could be that LLVM performs some optimization that significantly increases the required area when optimizing for speed. GCC does not seem to perform that kind of optimization.

.Area of the different minmax designs
:chart-id:
:vega-lite-filename: processed-charts/minmax_overview_area.vl.json
include::vega-chart.adoc[]

Each design was tested with test cases ranging from 0 to 50 elements. <<minmax-average-cycles>> show the average number of cycles for every design. 

.Average clock cycles for each minmax design
:chart-id: id=minmax-average-cycles
:vega-lite-filename: processed-charts/minmax_average_cycles.vl.json
include::vega-chart.adoc[]

The measurements show that the designs optimized for speed are all faster than the designs optimized for size. The six designs that were optimized by an LLVM toolchain show a similar performance with the same optimization goal.

The GCC designs are significantly both significantly slower. The GCC design optimized for speed has comparable performance to the LLVM designs optimized for size. The GCC design optimized for size takes the most cycles. It needs 30% more cycles than the next slowest designs.

There seem to be three performance classes, the fastest one comprised of the LLVM designs optimized for speed, the second fastest one comprised of the LLVM designs optimized for size and the GCC design optimized for speed, and the slowest one comprised of the GCC design optimized for size. 

The respective idiomatic Rust design is the fastest in the two fastest classes. The equivalent Rust design is the second fastest one in both classes, then come the {cpp} designs.

As expected, the fastest class is comprised of the LLVM designs optimized for speed, with the designs optimized for size being in the second fastest class. The GCC designs are all one class slower than the LLVM designs with the same optimization goal.

The fastest class contains all the big designs, while the small designs are all in the slower two classes. It seems like the LLVM optimizations for speed actually increased the performance at the cost of the area. This was to be expected.

// TODO ^ Remove some sentences

<<minmax-detailed-cycles>> shows the exact number of cycles for each test case. The three performance classes are clearly visible.

.Clock cycles in relation to the input length
:chart-id: id=minmax-detailed-cycles
:vega-lite-filename: processed-charts/minmax_detailed_cycles.vl.json
include::vega-chart.adoc[]

The slowest class takes stem:[3] cycles for every additional input. The second fastest class takes stem:[2] cycles for every additional input.

Interestingly the fastest designs require three extra cycles per input for three additional inputs, but for every fourth input, they actually require three fewer cycles. This averages out to stem:[1.5] cycles for every additional input. It should be noted again that these are also the biggest designs.

The LLVM designs optimized for speed have unrolled the loop by a factor of four. This can be seen in the control-flow graph (CFG) of the LLVM IR shown in <<minmax-speed-cfg>>. The function processes a batch of 4 elements until less than four elements are left. Then it processes the remaining elements one by one. Bambu seems to preserve this structure in the generated designs. This explains why the big designs are also the fastest ones, as the additional size contains the 4x operation. The small designs save area by only having the 1x operation.

The idiomatic Rust design is consistently the fastest design taking one cycle less than the next fastest design. This constant advantage probably appears because it does not have to write its results to memory after it has processed all elements. The C-like Rust designs are two cycles faster than the equivalent {cpp} designs compiled with Clang. 

The GCC designs are all slower than the LLVM designs. This could be the trade-off for the smaller design size.

The other relevant factor for estimating the performance of a design is the maximum frequency it can run at. <<minmax-max-frequency>> gives an overview of the maximum frequency for each design.

.Maximum frequency for each minmax design
:chart-id: id=minmax-max-frequency
:vega-lite-filename: processed-charts/minmax_max_frequency.vl.json
include::vega-chart.adoc[]

Most notably, the LLVM designs optimized for speed are the ones that clock the slowest. The two Rust designs optimized for speed can clock slightly higher than the {cpp}/Clang design optimized for speed. This is to be expected as they are also the designs that take up the most space, which most likely results in longer critical paths.

The five smaller designs all clock around 2x faster than the bigger designs. Their maximum frequency seems to be roughly ordered by the size of the design.

While the small designs can run at much higher frequencies, they also require more clock cycles. The relevant metric for comparing the performance is the execution time. It is calculated by multiplying the number of cycles with the inverse of the maximum frequency. <<minmax-execution-time>> shows the average execution time for each design.

.Average execution time per test
:chart-id: id=minmax-execution-time
:vega-lite-filename: processed-charts/minmax_performance.vl.json
include::vega-chart.adoc[]

The LLVM designs optimized for speed perform between 25-30% better than the designs optimized for size. So they still seem to split into two groups, although the divide is not as big as with the other measurements.

As expected, the significantly slower frequency of the big designs is more than enough to negate the advantage they gained by requiring fewer cycles. 

The GCC designs clocked at nearly the same frequency. The lower cycle count of the design optimized for speed makes it perform better than the one optimized for size. It performs comparably to the LLVM-based designs optimized for size, while the one optimized for size performs comparably to the LLVM-based designs optimized for speed. 

// TODO: Look into specifying how big things are getting parallelized



.Minmax Area to performance
:chart-id: id=minmax-area-performance
:vega-lite-filename: processed-charts/minmax_area_performance.vl.json
include::vega-chart.adoc[]

The big designs perform worse and take up more area than the small designs. The idiomatic Rust design (size) is better in terms of size and performance than nearly every other design. The only exception is the GCC design optimized for size, which is slightly smaller but performs significantly worse. These results can be seen in <<minmax-area-performance>>.

The task of computing the minimum and maximum can be easily parallelized. The input can be split over multiple instances of the function, and the results of each can be processed by another minmax function afterward. <<minmax-space-efficiency>> compares the designs by their performance per area.

.Minmax Space efficiency
:chart-id: id=minmax-space-efficiency
:vega-lite-filename: processed-charts/minmax_space_efficiency.vl.json
include::vega-chart.adoc[]

The idiomatic Rust design (size) is the most space-efficient design. The two GCC designs are second and third. The GCC design optimized for speed is a bit faster but also a bit bigger than the one optimized for size. This trade-off equalizes again in this metric, as their performance per area is nearly identical.

The big designs perform really poorly in this metric because they perform worse and take up more area than the small designs.

// TODO: Change idiomatic Rust design to use return via memory

// Experiment conclusion

// TODO: Convert bullet points to text

The designs generated by LLVM-based toolchains with the same optimization goal perform quite similarly.

The Rust designs perform slightly better, but the difference is not significant.

The idiomatic Rust design performs a bit better than the equivalent Rust design, but that could be because it has a slightly different interface, so the results may not be directly comparable.

The small designs performed better on all metrics except for cycle count.

For GCC, the optimization goal seems to align with the actual benefits of the design. The design optimized for Code size also resulted in a smaller design, while the design optimized for speed resulted in a faster design.

For LLVM, that is only partially true. While the design optimized for size resulted in a smaller design, it was also faster than the design optimized for speed.

LLVM seems to perform more aggressive loop unrolling than GCC.

Bambu seems to preserve unrolled loops in LLVM IR.

=== Testing the keccak functions

==== Tested implementations

// TODO: Provide source to the keccak implementation
The reference {cpp} implementation is shown in <<keccak-cpp-listing>>. It is based on a reference implementation provided by the Keccak team. It takes a pointer to 25 64-bit elements and mutates them in 24 rounds. Each round consists of 5 functions that transform the elements in some way. The values after 24 rounds are the result of the function.

.Interface of the keccak function
[source#keccak-rust-interface.linenums,rust]
----
include::../rust-keccak/src/keccak.rs[tag=main-function]
----

The Rust implementation is shown in <<keccak-rust-listing>>. It is kept as close to the reference implementation as possible. All functions of this direct port need to be marked as unsafe because they uses raw pointers. It is not possible to safely access raw pointers as they could point to invalid memory.

The idiomatic Rust implementation is nearly identical, except that it casts the pointer to an array of size 25. This allows the compiler to verify that the array is never accessed out of bounds. It also could allow the compiler to optimize the code better, as it can assume that the array is never accessed out of bounds. The idiomatic Rust implementation can be seen in <<keccak-idiom-listing>>.

The fourth implementation uses the keccak crate from crates.io <<keccak-crate>>. It provides optimized implementations of the keccak sponge functions in different sizes. The tested function, shown in <<keccak-crate-listing>>, just calls the correct implementation from the crate. It is added to the comparison to determine whether using crates from the Rust ecosystem for HLS is practical.

==== Results

<<keccak-overview-area>> shows the results of the area measurements. The Rust and idiomatic Rust designs seem to be nearly the same size on both optimizations.

.Area of the different keccak designs
:chart-id: id=keccak-overview-area
:vega-lite-filename: processed-charts/keccak_overview_area.vl.json
include::vega-chart.adoc[]

The three LLVM designs that are based on the reference implementation and optimized for speed are also similar in size. This could suggest that LLVM was able to optimize the code in a similar fashion. When optimized for size, the Rust-based designs are significantly bigger.

The Rust design from <<keccak-crate>> generates two nearly identically sized designs. They have roughly the same size as the other Rust designs optimized for size.

The GCC design optimized for speed is the biggest. The one optimized for size is the second smallest. This is what we would expect from the optimization goals.

// TODO: Document test cases
// Performance
The performance of each design was measured for three test cases. Every test case is a 25-element array of 64-bit integers. The input in the first test case was all zeros, and for the second, all 25s.T The third test case received the result of the first test case as its input. The number of cycles was consistent among the test cases for all designs. <<keccak-cycles>> shows the results of the cycle count measurements.

The 25 inputs need to be read at least once and written at least once, as they double as outputs. Accessing memory takes one cycle and has a latency of two cycles, so the theoretical minimum for the number of cycles is 51 if it is not performed in place. If the algorithm is performed directly in memory, the number of memory accesses will be significantly larger.

.Average clock cycles per test of the different keccak designs
:chart-id: id=keccak-cycles
:vega-lite-filename: processed-charts/keccak_average_cycles.vl.json
include::vega-chart.adoc[]

The LLVM designs optimized for speed all perform quite well, with the Clang design being the fastest. Interestingly the keccak-crate design optimized for size also performs quite well. Looking at the traces for these five designs reveals that they all perform only the minimum required amount of 50 memory accesses. The trace for the Clang design is shown in <<keccak-clang-speed-trace>>. The traces for the fast Rust designs are similar, but they take 48 cycles to do the calculation between the memory accesses. It seems like they require two cycles per round (24 * 2 = 48), while the Clang design only needs one cycle per round but has an additional cycle of overhead (24 + 1).

//TODO: Mention that the crate designs also perform random nonsensical memory accesses.



.Trace of the Clang design optimized for speed
[wavedrom.really-slightly-oversized-content,id="keccak-clang-speed-trace"]
....
include::samples/keccak_clang_speed.wavejson.json[]
....


The generated designs generated from the implementation from crates.io also perform memory reads during the calculation. These memory accesses are not part of the algorithm and appear random. The addresses they try to reach are out of bounds, and random values are returned during the tests. This seems not to influence the correctness of the result or the performance of the calculation. The random reads can be clearly seen in <<keccak-crates-speed-trace>>.

.Trace of the crates.io design optimized for size
[wavedrom.oversized-content,id="keccak-crates-speed-trace"]
....
include::samples/keccak_crates_speed.wavejson.json[]
....

// TODO: Attach snippet from the traces
The other designs all need significantly more cycles. They will be called slow designs. The difference between the fastest slow design and the slowest slow design is roughly factor 10. But even the fastest slow design is 50x slower than the slowest fast design. The main reason for their high cycle count is that they work directly on memory. The trace for the GCC design optimized for speed (<<keccak-gcc-speed-trace>>) shows that it constantly performs memory accesses. The traces for the other slow designs are similar. How much of the calculations are performed on external memory seems to dictate the exact cycle count.

.Excerpt from the trace of the GCC design optimized for speed
[wavedrom.slightly-oversized-content,id="keccak-gcc-speed-trace"]
....
include::samples/keccak_gcc_speed.wavejson.json[]
....

// Speculation
It could be that the designs optimized for size split up the work into multiple small functions that each load the values from memory and store them again.

The maximum frequency for each keccak design is shown in <<keccak-frequency>>. The fastest designs are the Rust designs optimized for speed, except for the crates.io design. It seems like the tradeoff of performing each round of keccak in two cycles instead of one is worth it, as they are able to run at a higher frequency compared to the LLVM design. Notably, both crate designs are running at the slowest frequency of all designs. This probably has something to do with the unnecessary memory accesses they perform.
// TODO: Future: Investigate why the crates designs are so slow



.Maximum frequency of the different keccak designs
:chart-id: id=keccak-frequency
:vega-lite-filename: processed-charts/keccak_max_frequency.vl.json
include::vega-chart.adoc[]

The other Rust designs optimized for size run at a lower frequency than the other designs optimized for size. The size-optimized {cpp} designs run at the third and fourth highest frequency.

.Execution time per design
:chart-id: id=keccak-performance
:vega-lite-filename: processed-charts/keccak_performance.vl.json
include::vega-chart.adoc[]

The execution time of the designs optimized for speed is roughly the same. Of them, the Rust designs are a bit faster than the LLVM designs. Even though these designs take more cycles, their higher clock speed is able to compensate for that. The crates.io design is the slowest of the fast designs.

The designs optimized for size are all significantly slower. The GCC design optimized for speed is the fastest of the designs that access memory. It is still more than 20 times slower than the next fastest design.

.Keccak area to performance
:vega-lite-filename: processed-charts/keccak_area_performance.vl.json
include::vega-chart.adoc[]

.Keccak space efficiency
:vega-lite-filename: processed-charts/keccak_space_efficiency.vl.json
include::vega-chart.adoc[]

The fast designs offer way more performance per area than the slow designs. While the size-optimized designs generated from {cpp} are the smallest designs overall, their abysmal performance makes them not very space efficient. The Rust designs optimized for size are the worst, as they are both slow and large. The Rust designs optimized for speed are the most space-efficient designs. The crates.io design is in the middle, as its performance is slightly slower than the other fast designs, and they are as big as the large designs.

// Notes on crates.io design

In the case of the crates.io design, it did not make any difference whether it was optimized for speed or for size. <<keccak-crates-implementation-listing>> shows that their implementation explicitly inlines and unrolls steps of a round. Besides the explicit unrolling, the other big difference of that implementation is that the number of rounds is variable. The Rust compiler seems to respect these optimizations. It generates basically the same LLVM IR for both optimizations. The biggest difference is that the version optimized for speed inlines the control of the main loop into the loop body.

// TODO: Insert CFG for both LLVM IR files. 
// experiments/rust-hls-experiments/keccak_crates_speed/rust_hls/keccak_hls_synthesized/keccak.ll
// Strip all implementation except for differences from both CFGs.


// TODO:
The LLVM IR generated from the other Rust implementations when optimizing for speed is nearly identical to the LLVM IR generated from the crates.io design. It is peculiar that the crates.io designs are bigger and run at a much slower speed, as they are both based on such similar LLVM IR. The most notable differentiation between them is in accessing the `KECCAK_ROUND_CONSTANTS` array. In each of the 24 rounds, a different constant from that array is needed. The array accesses use the same instructions in both cases, albeit in slightly different places. The crates.io implementation fetches the round constant in the middle of the round and the other LLVM IR at the end. This should make no difference as they have the same data dependencies and can be reordered. The crates.io design uses a pointer to an integer `i64*` to refer to the array, and the other implementation uses a pointer to an array of known size `[24 x i64]*`.

It can be assumed that this causes Bambu to perform poorly for some reason. It could be that the generated design tries to access the array at an outside memory location. This would also explain the 24 memory accesses to out-of-bounds memory locations that are performed during the calculations. These values from external memory are apparently not used during calculation. It seems to be a bug in Bambu where the generated design tries to access external memory when the actual values are in internal memory. It can be assumed that Bambu cannot prove that the instruction only accesses the internal memory. However, the bug is that it tries to access external memory in addition to internal memory. It may be that Clang does not generate instructions like this, so the bug did never occur until now.

The crates.io design also performs a check to verify that the number of rounds is not greater than 24 and panics if it is. It is safe to assume that Bambu detects that this will never happen in our case and optimizes it away. If Bambu did not do that, it would not be able to synthesize the design. 
// While the designs are mostly treated as block-boxes, the generated Verilog reveals that the ((((not sure controller or datapath)))) of the crates.io design contains a connection to memory, while the others do not.


// TODO: Add md5 back in if there is time left

// === Testing the md5 functions

// Three implementations will be tested for the md5 function. 

// The {cpp} reference implementation is based on a 

// .Area of the different md5 designs
// :vega-lite-filename: processed-charts/md5_overview_area.vl.json
// include::vega-chart.adoc[]

// .Execution time per design
// :vega-lite-filename: processed-charts/md5_performance.vl.json
// include::vega-chart.adoc[]

// .The maximum frequency of the different md5 designs
// :vega-lite-filename: processed-charts/md5_max_frequency.vl.json
// include::vega-chart.adoc[]

// .Average clock cycles per test of the different md5 designs
// :vega-lite-filename: processed-charts/md5_average_cycles.vl.json
// include::vega-chart.adoc[]

// .MD5 Area to performance
// :vega-lite-filename: processed-charts/md5_area_performance.vl.json
// include::vega-chart.adoc[]

// .MD5 Space efficiency
// :vega-lite-filename: processed-charts/md5_space_efficiency.vl.json
// include::vega-chart.adoc[]


== Evaluation

=== Supported features / Limitations

Extern c interface

no panic, exit, etc

not exhaustive

We have shown that

Discuss limitations

Discuss the results of the performance experiments

Discuss compiler flags. 

// Why is this a solution
// Discuss the results of the experiments

== Conclusion



// Repeat the problem
As mentioned previously, current HLS tools focus on the common systems-programming languages C or {cpp} as their input language. Rust could provide a more modern alternative if it was supported by HLS tools.

// Really short summary
It was shown that Bambu can be used to perform HLS from Rust. The Rust compiler can output LLVM IR, which can be used as an input for Bambu. When the Rust compiler is optimizing for speed, the generated designs are similar in terms of performance and size to the designs generated from {cpp}/Clang. When optimizing for size, the results were mixed; some designs were smaller and faster, some were larger and bigger, and some were similar. They showed that the Rust-based designs usually perform similarly to the {cpp}/Clang-based designs. It was also demonstrated that it does not make a difference whether the Rust specifications are C-like or more Rust-like. A specification using an implementation from crates.io was able to be synthesized with promising results. In that specific case, the resulting design was larger than necessary, probably caused by a bug in Bambu. As expected, the tooling is not as mature as for {cpp}. 

// Gotchas when using Rust
There are multiple challenges when using the Rust compiler to generate LLVM IR that is compatible with Bambu. Bambu does not support all instructions, so the Rust compiler needs to be configured only to generate compatible IR. Bambu's LLVM support is focused on LLVM IR generated by Clang. As a result, Bambu is not well-tested on LLVM IR generated by other tools. While Bambu is able to process and compile them, the resulting designs might not be as optimized as they are with Clang. This can lead to bugs like the one described in <<insert-id>>. Rust also performs bounds checks on all memory accesses that could overflow. If these bound-checks cannot be removed during optimization, the resulting LLVM IR contains calls to panic which cannot be synthesized by Bambu. By default, Rust also performs overflow checks on all integer arithmetics, which also can result in panics. They can be disabled. Another limitation is that only Rust functions with a C-compatible interface can be synthesized.

// Toolchain
// TODO: The second sentence is shit.
A framework for integrating the toolchain with the Rust-based HDL rust_hdl was developed. It is available as a rust crate named rust_hls. The framework makes it possible to write HLS specifications and the HDL designs that used the HLS designs generated from the specifications in the same project. It also enables to write tests for the specification and the generated design side-by-side as Rust unit tests. It achieves this by searching the project for modules marked as HLS specifications, running the toolchain for the found specifications, and embedding the generated modules into the original project. If a function marked as HLS specification shows obvious problems, an error will be emitted before synthesis is performed. It also allows individual configuration of the toolchain for each specification. 

== Future work


// How can the Rust compiler be made to generate better LLVM IR for HLS?

// Measure more test cases

// Test out the limitations of the toolchain

// 



// Can it be better than CPP when the tools are adjusted for Rust LLVM?
Future work on HLS from Rust should focus on improving support for Rust-generated LLVM IR in HLS tools. While Bambu is currently able to synthesize the LLVM IR generated by Rust, in some cases, it seems possible to improve the generated designs. It should be possible to find these cases and improve Bambu to handle them better.

It would also be interesting to see if the LLVM IR generated by Rust can be improved to generate better designs. Our toolchain builds every dependency of the project separately and then links them together. This is not ideal, as it does not allow the compiler to perform cross-crate link-time optimizations. This would probably require adjusting cargo or the Rust compiler to allow link-time optimizations when compiling to LLVM IR. The toolchain only configures the Rust compiler with the settings that are absolutely necessary for generating LLVM IR that Bambu can synthesize. Besides that, it uses rustc's default optimization profiles for speed and size. There are probably significant improvements possible by evaluating which optimizations are useful for HLS and which are not.


[glossary]
== List of abbreviations
// Abbreviations from here will automatically be linked to the document

// Abbreviations in random order and links to read more about them
[glossary]
[[FPGA]]FPGA:: Field-Programmable Gate Array link:pass:[https://en.wikipedia.org/wiki/Field-programmable_gate_array][🔗^]
[[HLS]]HLS:: High-Level Synthesis link:pass:[https://en.wikipedia.org/wiki/High-level_synthesis][🔗^]
[[HDL]]HDL:: Hardware Description Language link:pass:[https://en.wikipedia.org/wiki/Hardware_description_language][🔗^]
[[ADL]]ADL:: Accelerator Design Language link:pass:[https://www.sigarch.org/hdl-to-adl/][🔗^]
[[GPU]]GPU:: Graphics Processing Unit link:pass:[https://en.wikipedia.org/wiki/Graphics_processing_unit][🔗^]
[[LLVM_IR]]LLVM IR:: LLVM Intermediate Representation link:pass:[https://en.wikipedia.org/wiki/LLVM#Intermediate_representation][🔗^]
[[RTL]]RTL:: Register-Transfer Level link:pass:[https://en.wikipedia.org/wiki/Register-transfer_level][🔗^]
[[DUT]]DUT:: Design/Device Under Test link:pass:[https://en.wikipedia.org/wiki/Test_bench][🔗^]
[[ASIC]]ASIC:: Application Specific Integrated Circuit link:pass:[https://en.wikipedia.org/wiki/Application-specific_integrated_circuit][🔗^]
[[QoR]]QoR:: Quality of Results link:pass:[https://en.wikipedia.org/wiki/Quality_of_results][🔗^]
[[CPU]]CPU:: Central Processing Unit link:pass:[https://en.wikipedia.org/wiki/Central_processing_unit#Structure_and_implementation][🔗^]
[[LUT]]LUT:: Look-Up Table link:pass:[https://en.wikipedia.org/wiki/Lookup_table][🔗^]
[[FF]]FF:: Flip-Flop link:pass:[https://en.wikipedia.org/wiki/Flip-flop_(electronics)][🔗^]
[[DFF]]DFF:: D Flip-Flop link:pass:[https://en.wikipedia.org/wiki/Flip-flop_(electronics)#D_flip-flop][🔗^]
[[BRAM]]BRAM:: Block RAM link:pass:[https://nandland.com/lesson-15-what-is-a-block-ram-bram/][🔗^]
[[DSP]]DSP:: Digital Signal Processor link:pass:[https://www.fpgakey.com/tutorial/section613][🔗^]
[[CLB]]CLB:: Configurable Logic Block link:pass:[https://www.fpgakey.com/wiki/details/51][🔗^]
[[LB]]LB:: Logic Block link:pass:[https://www.fpgakey.com/wiki/details/51][🔗^]
[[LE]]LE:: Logic Element link:pass:[https://www.fpgakey.com/wiki/details/342][🔗^]
[[RAII]]RAII:: Resource Acquisition Is Initialization / Scope-Bound Resource Management link:pass:[https://en.wikipedia.org/wiki/Resource_acquisition_is_initialization][🔗^]
[[HIR]]HIR:: High-level Intermediate Representation link:pass:[https://rustc-dev-guide.rust-lang.org/hir.html][🔗^]
[[THIR]]THIR:: Typed HIR link:pass:[https://rustc-dev-guide.rust-lang.org/thir.html][🔗^]
[[MIR]]MIR:: Mid-level Intermediate Representation link:pass:[https://rustc-dev-guide.rust-lang.org/mir/index.html][🔗^]
[[PAL]]PAL:: Programmable Array Logic link:pass:[https://en.wikipedia.org/wiki/Programmable_Array_Logic][🔗^]
[[CFG]]CFG:: Control-Flow Graph link:pass:[https://en.wikipedia.org/wiki/Control-flow_graph][🔗^]


[bibliography]
== References

// Claims to have a transpiler from a subset of Rust (RAR) to restricted algorithmic C (RAC) that can be synthesized to FPGA. No source.
// The first paper to mention HLS from Rust. 
* [[[Har22]]]
Hardin, David,
_Hardware/Software Co-Assurance using the Rust Programming Language and ACL2_,
arXiv preprint arXiv:2205.11709,
2022.
link:pass:[https://arxiv.org/abs/2205.11709v1][🔗^]

* [[[Rog20]]]
Rogers, Samuel and Slycord, Joshua and Baharani, Mohammadreza and Tabkhi, Hamed,
_gem5-SALAM: A System Architecture for LLVM-based Accelerator Modeling_,
2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 471-482,
2020.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9251937][🔗^]

* [[[Li21]]]
Li, Rui and Berkley, Lincoln and Yang, Yihang and Manohar, Rajit,
_Fluid: An Asynchronous High-level Synthesis Tool for Complex Program Structures_,
2021 27th IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC), 1-8,
2020.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9565447][🔗^]

* [[[Lia23]]]
Liang, Geng-Ming and Yuan, Chuan-Yue and Yuan, Meng-Shiun and Chen, Tai-Liang and Chen, Kuan-Hsun and Lee, Jenq-Kuen,
_The Support of MLIR HLS Adaptor for LLVM IR_,
Workshop Proceedings of the 51st International Conference on Parallel Processing, 1-8,
2020.
link:pass:[https://doi.org/10.1145/3547276.3548515][🔗^]

// Bambu provides a research environment to experiment with new ideas across HLS, high-level verification, and debugging.
// Bambu input: standard C/{cpp} specifications, LLVM IR, IRs from GCC
// Includes many optimizations
// Makes it easy to integrate new transformations and optimizations
// Is open-source
// Bambu is a command line tool
// Supports most C/{cpp} constructs
// Bambu has three phases. frontend, middleend and backend
// Frontend: Uses Clang or gcc
// Uses a compiler plugin for both extracting the call graph and control flow information
// Builds its own static single assignment IR
// This decouples the compiler front end from the rest of the HLS process.
// Vivado HLS has a frontend based on clang
// Middle end:
// Bambu rebuilds the call graph and control data flow graph and adds its own data structures.
// Applies a set of analyses and transformations.
// Including common software compilation optimizations
// Including target-specific transformations. Such as replacing multiplication and divisions with constants with shift and add operations.
// Can exploit custom-sized operators
// TODO: Explore if Bambu can use the rust crate for more integer sizes like u21
// Bambu performs bitwidth and range analysis to minimize bit width
// Backend: Bambu performs the actual architectural synthesis here
// The synthesis process acts on every function individually
// Every function has at least two parts: control logic and datapath
// Control logic is an FSM
// Control logic handles the routing of data values and temporal execution of ops
// Bambu steps:
// * Function allocation
// Bambu has a technology library for standard system libraries such as libm or libs
// This step associates High-level functions with hardware resources
// Bambu supports sharing functions across module boundaries
// * Memory allocation
// Defines memories to store variables.
// Defines how dynamic memory is implemented
// Memories in Bambu can be classified as read-only, local, with aligned or unaligned memory access
// Bambu supports accessing protocol-based memories
// * Resource allocation
// Maps operations (not mapped onto functions) onto resource units
// Resource units are available in the Bambus resource library
// Floating point operations are supported by generating soft floating point cores
// Rich resource library with multiple implementations for the same operation
// Resource library annotated with latency and resource occupation.
// * Scheduling
// Bambu uses List scheduling
// Every operation has a priority
// An operation is ready when its dependencies have been satisfied
// Ready operations can be scheduled if the resources are available
// Multiple competing for a resource: higher priority
// Also has a speculative scheduling algo
// * binding
// Pretty much standard
// Bambu considers how profitable it is for two operations to share the same resource
// Resources that occupy a big area are more likely to be shared
// * netlist generation
// Translates the architecture in an RTL description
// In Verilog or VHDL
//
// Research topics for Bambu: "They range from parallelized hardware accelerator design, dynamic scheduling, verification, and debugging, design exploration of the compilation flow, machine learning accelerator design, IR development, and integration with logic synthesis tools", MLIR
// MLIR dialects that can be translated to LLVM IR
* [[[Fer21]]]
+F. Ferrandi et al.+,
_Invited: Bambu: an Open-Source Research Framework for the High-Level Synthesis of Complex Applications_,
2021 58th ACM/IEEE Design Automation Conference (DAC), 1327-1330,
2021.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9586110][🔗^]
link:pass:[https://re.public.polimi.it/retrieve/668507/dac21_bambu.pdf][📁^]

* [[[Rot10]]]
+Nadav Rotem,+
_C-to-Verilog. com: High-Level Synthesis Using LLVM_,
University of Haifa,
2010.
link:pass:[https://llvm.org/devmtg/2010-11/Rotem-CToVerilog.pdf][🔗^]

* [[[Sch20]]]
Fabian Schuiki, Andreas Kurth, Tobias Grosser, and Luca Benini,
_LLHD: a multi-level intermediate representation for hardware description languages_,
In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2020), 258-271,
2020.
link:pass:[https://doi.org/10.1145/3385412.3386024][🔗^]

// Multiple HLS tools use LLVM
// C/Cpp are the most popular languages for HLS
// NOTE: I focused on FPGA descriptions
// Clock frequency scaling in CPU stalled around 2005
// A alternative approach for high-throughput and energy-efficient processing is to use specific accelerators
// Specialized accelerators are hard to design and program
// RTL requires advanced hardware expertise
// RTL specifies cycle-by-cycle behavior explicitly
// RTL is a low-level abstraction
// RTL leads to longer development times
// FPGAs with HLS can reduce that.
// FPGAs are configurable integrated circuits
// Most FPGAs are reconfigurable
// FPGA vendors provide toolchains to synthesize HTL to bitstream
// bitstream gets programmed to the FPGA
// HLS tools start from an HLL and automatically produce a circuit specification in RTL
// HLS offers to enable software engineers to benefit from the performance and energy efficiency of hardware without having hardware expertise
// HLS tools enable hardware engineers to design systems faster
// HLS tools enable hardware engineers to explore the design space rapidly
// Microsoft uses FPGAs to accelerate Bing search
//
// 
* [[[Nan16]]]
+R. Nane et al.+,
_A Survey and Evaluation of FPGA High-Level Synthesis Tools_,
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 1591-1604,
2016.
link:pass:[https://ieeexplore.ieee.org/abstract/document/7368920][🔗^]
link:pass:[https://sci-hub.st/10.1109/tcad.2015.2513673][📁^]

* [[[Nor18]]]
+D. H. Noronha, B. Salehpour and S. J. E. Wilton+,
_LeFlow: Enabling Flexible FPGA High-Level Synthesis of Tensorflow Deep Neural Networks_,
Fifth International Workshop on FPGAs for Software Programmers, 1-8,
2018.
link:pass:[https://ieeexplore.ieee.org/abstract/document/8470462][🔗^]

* [[[Soz22]]]
Sozzo, Emanuele Del, et al.,
_Pushing the level of abstraction of digital system design: A survey on how to program FPGAs_,
ACM Computing Surveys, 1-48,
2022.
link:pass:[https://dl.acm.org/doi/abs/10.1145/3532989][🔗^]

* [[[XLS]]]
_XLS project page_
link:pass:[https://google.github.io/xls/][🔗^]

* [[[DSLX]]]
_DSLX Reference_
link:pass:[https://google.github.io/xls/dslx_reference/][🔗^]

* [[[rusthdl]]]
_rust-hdl project overview_
link:pass:[https://github.com/samitbasu/rust-hdl][🔗^]


* [[[Zen12]]]
_Identifying Barriers to Adoption for Rust through Online Discourse_
link:pass:[https://arxiv.org/pdf/1901.01001.pdf][🔗^]

* [[[so-trends]]]
_Stack Overflow Trends_
https://insights.stackoverflow.com/trends?tags=rust%2Cc%2B%2B

// Rust has an ecosystem that greatly simplifies any software project
// Rust is great
// Rust has been the "most loved" language since 2016
// Rust is meant to supersede C/{cpp}
// Rust's focus is on safety and performance
// For any need, you may have libraries exist 
// Dependencies can be installed using the official cargo tool
// Rust is the first industry-supported computer programming language to overcome the longstanding trade-off between the control over resource management provided by lower-level languages for systems programming, and the safety guarantees of higher-level languages
// Rust enables many common systems programming pitfalls to be detected at compile-time
// Rust surpasses all other common memory-safe languages in terms of performance
// Rust has data-race prevention
// Considering performance Rust is one of the best languages
// Considering safety, Rust is the best language
// Rust offers many modern features that the more established systems-programming languages tend to lack.
// Cargo is the package manager for Rust
// Cargo is the build system for Rust
// Cargo facilitates downloading and building dependencies
// Cargo facilitates unit testing and integration testing
// Cargo facilitates benchmarking
// Cargo facilitates build management with different profiles
// Cargo facilitates documentation generation from comments
// Rustfmt facilitates code formatting
// Dependency management is handled with a configuration file
// Dependencies are automatically installed during compilation
// Dependencies can be easily found on the official community crates registry
// Cargo allows viewing unified documentation for all dependencies
// Unit tests are written in the same file as the code they test
// Benchmarking is done in a similar fashion to unit testing
// The tooling alone makes Rust a much better development experience than most systems languages
// The tooling is most likely a considerable contributor to its rise.
// Rust is approaching the status of a mainstream language in health informatics applications
// The criticisms of Rust tend to originate from its lack of maturity
// C and {cpp} are well-adopted and much more established in the industry than Rust
// lack of demand for Rust developers in the market
* [[[Bug22]]]
+William Bugden, Ayman Alahmar+,
_Rust: The Programming Language for Safety and Performance_,
asXiv,
2022.
link:pass:[https://arxiv.org/pdf/2206.05503.pdf][🔗^]

// Rust can be used for GPU programming
* [[[Byc22]]]
+Andrey Bychkov, Vsevolod Nikolskiy+,
_Rust Language for GPU Programming_,
In: Voevodin, V., Sobolev, S., Yakobovskiy, M., Shagaliev, R. (eds) Supercomputing. RuSCDays 2022. Lecture Notes in Computer Science, vol 13708. Springer, Cham, 2022, pp. 522-32,
2022
link:pass:[https://doi.org/10.1007/978-3-031-22941-1_38][🔗^]

// Rust can be used for web programming
* [[[Kyr22]]]
+Kyriakou K-ID, Tselikas ND+,
_Complementing JavaScript in High-Performance Node.js and Web Applications with Rust and WebAssembly._,
Electronics 11, no. 19: 3217,
2022
link:pass:[https://doi.org/10.3390/electronics11193217][🔗^]

// Probably one of the greatest features of the language is the package manager, called cargo.
// Rust is a high-level language
// Rust is very efficient in terms of performance
// Rust is based on the principle of zero-cost abstractions
// Rust provides a memory safety mechanism without using a garbage collector called the borrow checker
// Rust is a strongly typed language
// Rust provides an out-of-the-box package manager used for importing dependencies, building, and distributing a project.
// If a variable is declared in a specific context, it will be freed when the context is over.
// The ownership of a variable can be passed to another context
// More basic description of Rust ownership stuff will skip that for now
// Development in {cpp} on a production level requires the use of additional tools such as CMake, Make, etc. This adds a layer of complexity.
// Rust has mandatory tooling for building, distributing, and depending on a project
// In Rust, only a manifest file is needed to configure the project for any scenario possible
// Rust can be compiled into web assembly
// Rust is more energy efficient than any other language except C for IoT applications
// Rust is faster than any other language except C for IoT applications
// Rust can easily integrate with C or {cpp} code
// Rust solves the problem of memory safety without using a garbage collector
// Microsoft states that 70% of security flaws discovered in their systems are related to memory safety
* [[[Cos19]]]
+Cosmin Cartas+,
_Rust - The Programming Language for Every Industry_,
ECONOMY INFORMATICS JOURNAL, 19, 45-51,
2019
link:pass:[https://doi.org/10.12948/ei2019.01.05][🔗^]

// state-of-art bottom-up logic programming within the Rust ecosystem
* [[[Sah22]]]
+Arash Sahebolamri, Thomas Gilray, Kristopher Micinski+,
_Seamless Deductive Inference via Macros_,
Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction, 77-88,
2022
link:pass:[https://doi.org/10.1145/3497776.3517779][🔗^]

// Productivity in HLS is better than HDL
// HLS offers easier design and testing
// HDL implementation is better than HLS
* [[[Mil20]]]
+Roberto Millón, Emmanuel Frati, Enzo Rucci+,
_A Comparative Study between HLS and HDL on SoC for Image Processing Applications_,
Revista elektron, Vol. 4, No. 2, 100-106,
2020
link:pass:[https://doi.org/10.37537/rev.elektron.4.2.117.2020][🔗^]
http://elektron.fi.uba.ar/index.php/elektron/article/view/117/219[📁^]

// Describing the traditional HDL design flow (in 1996)
// TODO: Find a newer source
* [[[Smi96]]]
+Douglas J. Smith+,
_VHDL & Verilog compared & contrasted—plus modeled example written in VHDL, Verilog and C._,
In Proceedings of the 33rd annual Design Automation Conference, pp. 771-776,
1996
link:pass:[https://dl.acm.org/doi/pdf/10.1145/240518.240664][🔗^]

// 
* [[[Fla20]]]
+Peter Flake, Phil Moorby, Steve Golson, Arturo Salz, and Simon J. Davidmann+,
_Verilog HDL and its ancestors and descendants._,
Proc. ACM Program. Lang. 4, no. HOPL (2020): 87-1,
2020
link:pass:[https://www.researchgate.net/profile/Arturo-Salz-2/publication/342137214_Verilog_HDL_and_its_ancestors_and_descendants/links/613fc7b45d9d0e131b427dbb/Verilog-HDL-and-its-ancestors-and-descendants.pdf][🔗^]

* [[[intel-hls]]]
_Intel® High Level Synthesis Compiler_
https://www.intel.de/content/www/de/de/software/programmable/quartus-prime/hls-compiler.html

* [[[hdl-to-adl]]]
_From Hardware Description Languages to Accelerator Design Languages_
https://www.sigarch.org/hdl-to-adl/


// Survey literature from 2010 to 2016
// Probably the best comparison of HLS and RTL
// Also, the newest
// Shows that the quality of results of RTL is better than that of HLS
// Shows that development time with HLS is a third of that of the RTL flow
// Shows that the productivity of a designer is over four times higher with HLS than with RTL
// Vivado HLS is the most common HLS tool. At least it is used significantly more than any other HLS tool in papers.
// Xilinx is the leading FPGA vendor
// FPGAs are made of configurable logic blocks (CLB, different vendors, different names).
// The CLBs are connected with programmable interconnects.
// The CLBs consist of a few logic cells, logic elements, or adaptive logic modules (ALM) (LC, LE, and ALM are the same. Different vendors use different names).
// Logic cells are made of a combination of programmable look-up tables (LUTs) and flip-flops (FFs).
// FPGAs can also have other resources, but these are vendor specific. Most commonly, DSP blocks and BRAM blocks.
// There are four performance metrics that are commonly used to compare HLS and RTL: performance, execution time, latency, maximum frequency
// For projects bigger than 250 lines of code HLS also needs fewer lines of code than RTL
// Reduction in development time for HLS seems independent of project size.
// On average, HLS uses 41% more basic FPGA resources than RTL
// The usage of advanced FPGA resources of HLS is similar to RTL
// C-based languages are the most common, then OpenCL-based, then high-level language based.
// CUDA/OpenCL-based HLS is especially resource-consuming and has the worst performance
// The performance of HLS designs is similar to the performance of RTL designs.
// The only example in academia where the development time of HLS was more than RTL was when the developer had to learn the HLS tool in the process.
// Only looks at small to medium designs, 50-500 lines of code
// It is easier to adopt HLS than RTL for people who have experience in software design
// HLS allows for efficient behavioral verification
// The HLS output must still be verified for non-behavioral aspects. This traditional verification is difficult because there is no direct relationship to the source code.
// HLS halves verification time in many cases
// HLS is a particularly good choice when the time to market is a dominant issue, and there is no compelling need to gain the ultimate performance or smallest resource usage for the product
// There is no standard example to compare HLS and RTL
* [[[Lah19]]]
+Sakari Lahti, Panu Sjövall, Jarno Vanne, Timo D. Hämäläinen+,
_Are We There Yet? A Study on the State of High-Level Synthesis_,
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 38, no. 5, pp. 898-911,
2019
link:pass:[https://doi.org/10.1109/TCAD.2018.2834439][🔗^]
link:pass:[https://sci-hub.st/10.1109/tcad.2018.2834439][📁^]


// Studied Rust’s ownership discipline in the presence of unsafe code.
// Shows that various important Rust libraries with unsafe implementations, many of them involving interior mutability, are safely encapsulated by their type
// NOTE: Did only read the abstract and conclusion
* [[[Jun17]]]
+Ralf Jung, Jacques-Henri Jourdan, Robbert Krebbers, Derek Dreyer+,
_RustBelt: Securing the Foundations of the Rust Programming Language_,
Proc. ACM Program. Lang. 2, POPL, Article 66 (January 2018), 34 pages.,
2017
link:pass:[https://doi.org/10.1145/3158154][🔗^]

// Cpp uses RAII
// "In particular, a programmer can choose to write a low-level-C style and/or violate every rule of good programming. That is not my topic here."
* [[[Str12]]]
+Bjarne Stroustrup+,
_Foundations of {cpp}_,
Programming Languages and Systems. ESOP. Springer, pp. 1-25,
2012
link:pass:[https://doi.org/10.1007/978-3-642-28869-2_1][🔗^]

* [[[Kla23]]]
+Steve Klabnik, Carol Nichols+,
_The Rust programming language_,
No Starch Press,
2023
link:pass:[https://doc.rust-lang.org/book/foreword.html][🔗^]

// Rust compiler has multiple intermediate representations (IRs)
// * MIR (Mid-level IR)
// * HIR (High-level IR)
// * THIR (Typed HIR)
// * LLVM IR
// Typechecking happens on HIR
// Optimization happens on MIR
// MIR is a typed SSA
// Borrowchecking happens at the MIR level
// Optimizations also happen in LLVM
// LLVM is used as the backend
// LLVM can generate machine code for many architectures
// LLVM is a collection of modular and reusable compiler and toolchain technologies
// LLVM contains a pluggable compiler backend used by rustc and Clang
// Clang is a C compiler
// LLVM takes LLVM IR
// Rust compiler uses LLVM because
// * They don't have to write their own backend. Reduces implementation and maintenance effort.
// * Benefit from the large suite of advanced optimizations that LLVM provides
// * Rust can be compiled into any of the platforms that LLVM supports.
// * Community benfits. Things like specter and meltdown only need to be fixed in LLVM, and many compilers benefit from that
// rustc groups LLVM IR into "modules" known as codegen units
// Rustc can use LLVM to codegen multiple of these modules in parallel utilizing multiple CPU cores
// The resulting object files are then linked together by the linker
* [[[rustc-guide]]]
_Rust Compiler Development Guide_,
2023
link:pass:[https://rustc-dev-guide.rust-lang.org/backend/codegen.html][🔗^]
link:pass:[https://rustc-dev-guide.rust-lang.org/mir/optimizations.html][🔗^]

// LLVM is a compiler framework
// Defines a low-level code representation in a single static assignment (SSA) form
// LLVM IR
// Describes a program using an abstract RISC-like instruction set with higher-level information
// LLVM IR contains type information
// LLVM IR contains explicit control flow graphs
// LLVM IR contains explicit dataflow representation (using SSA)
// Has a low-level, language-independent type system
// Has instructions for performing type conversions and low-level address arithmetic while preserving type information.
// Low-level exception handling instructions
// LLVM is not intended to be a universal compiler IR
// does not represent high-level language features directly
// LLVM has no notion of high-level constructs such as classes, inheritance, or exception-handling semantics
// LLVM does not specify a runtime system or particular object model
// "Type information captured by LLVM is enough to safely perform a number of aggressive transformations that would traditionally be attempted only on type-safe languages in source-level compilers."
// NOTE: I skipped section 2
// "The goal of the LLVM compiler framework is to enable sophisticated transformations at link-time, install-time, runtime, and idle-time, by operating on the LLVM representation of a program at all stages."
// Static compiler front-ends emit code in the LLVM representation
// combined by the LLVM linker
// Linker performs a variety of link time optimizations
// The resulting code is then translated to native code for a given target.
// Language-specific optimizations must be performed in the frontend
// External static LLVM compilers are known as front-ends
// Frontends translate source language programs into LLVM IR
// Can perform aggressive interprocedural optimizations across the entire program
// Some of the interprocedural optimizations are:  inlining, dead global elimination, dead argument elimination, dead type elimination, constant propagation, array bounds check elimination, simple structure field reordering, and Automatic Pool Allocation
// Uses code generator backends to translate LLVM IR into native code for a given target
* [[[Lat04]]]
+C. Lattner, V. Adve+,
_LLVM: a compilation framework for lifelong program analysis & transformation_,
International Symposium on Code Generation and Optimization, 2004. CGO 2004., San Jose, CA, USA, 2004, pp. 75-86
link:pass:[https://doi.org/10.1109/CGO.2004.1281665][🔗^]
link:pass:[https://sci-hub.st/10.1109/cgo.2004.1281665][📁^]
// Shows that HLS is twice as fast as HDL 
// M. Pelcat, C. Bourrasset, L. Maggiani and F. Berry, "Design productivity of a high-level synthesis compiler versus HDL," 2016 International Conference on Embedded Computer Systems: Architectures, Modeling, and Simulation (SAMOS), Agios Konstantinos, Greece, 2016, pp. 140-147, doi: 10.1109/SAMOS.2016.7818341.
// https://ieeexplore.ieee.org/abstract/document/7818341

// FPGA inception 30 years ago
// FPGAs bring faster design cycles than custom chips
// FPGAs lower dev cost than custom chips
// low-level hardware reconfigurability
// FPGA architecture offers many design choices
// FPGAs consist of different types of programmable blocks
// "FPGAs are reconfigurable computer chips that can be programmed to implement any digital circuit."
// prefabricated routing tracks with programmable switches
// Functionality of all FPGA blocks is controlled by SRAM cells
// Milloions of SRAM cells
// HDL is converted to bitstream
// Bitstream is used to program all configuration SRAM cells
// Lower NRE cost than ASICs
// Shorter time to market than ASICs
// off-the-shelf FPGA can be used to implement a design in a matter of weeks
// Skipping physical design, layout, fabrication, and verification
// Allow continuous hardware upgrades by loading new bitstreams in the field
// Considered a compelling solution for small and medium-sized designs 
// Exact hardware for every application
// Exact datapath width, pipeline stages, and parallel units as required
// Can achieve higher efficiency than CPU or GPU
// Can implement instruction-free streaming hardware
// Can implement a custom instruction set
// Adopted in many domains.
// "adoption of FPGAs in many application domains including wireless communications, embedded signal processing, networking, ASIC prototyping, high-frequency trading, and many more."
// Deployed on a large scale in data centers, packet processing, machine learning
// Lower efficiency than ASICs
// FPGA, on average 35 times larger than ASIC implementation
// FPGA, on average four times slower than ASIC implementation
// For designs that utilize other FPGA blocks, the gap is smaller, still nine times large
// FPGA architects seek to reduce the gap while maintaining programmability
// Early FPGAs were simple arrays of logic blocks
// Modern FPGAs are complex heterogeneous architectures that have more block types
// Modern FPGA have blocks like BRAM, DSP, processors, external interfaces
// FPGA architectures are evaluated based on the efficiency of implementing a wide variety of designs
// There are academic test suites for evaluating FPGA architectures
// VTR is a CAD system to layout designs on FPGAs
// CAD system applies a series of complex optimizations 
// CAD system converts RTL design to netlist.
// CAD system maps netlist to FPGA blocks
// CAD system places blocks on FPGA and routes the connections between them
// CAD system outputs bitstream implementation
// Total area is a key metric
// "Total area is the sum of the areas of the FPGA blocks used by the application, along with the programmable routing included with them."
// Timing analyzer finds the critical path through blocks and routing
// Critical path limits maximum clock frequency
// Power consumption is estimated based on resources used and signal toggle rate
// _hardened_ blocks are blocks that are implemented as ASICs
// What functionality to harden is a design choice
// What area of the FPGA to use for hardened blocks is a design choice
// Hardened blocks can still have some level of configurability
// How flexible the hardened blocks are is a design choice
// Hardened blocks are faster, smaller and more power efficient than programmable blocks
// Tradeoff between flexibility and efficiency
// Unused hardened blocks are wasted silicon
// Problems with slow routing to hardened blocks, if they are far away
// PAL first reconfigurable computing devices
// PAL does not scale well; area increases quadratically with IO size
// CPLD includes multiple PALs and programmable routing in a package
// 1984 Xilinx pioneers first LUT-based FPGA
// SRAM-based LUTs with interconnects between them
// Scales well
// Much higher area efficiency than and/or based designs
// LUTs form the fundamental logic element in all commercial FPGAs
// Alternative designs perform worse than LUTs
// K-LUT implements a K-input LUT
// K-LUT stores the truth table in SRAM cells,
// K input signals are used as multiplexers to select line
// truth table contains 2^K values
// A basic logic element (BLE) is a K-LUT with an output register
// A BLE can implement DFF or a K-LUT
// A BLE has K inputs and two outputs, one for routing and one for feedback inside the LE
// Logic blocks are composed of multiple (N) BLEs
// Logic blocks have a local interconnect
// The local interconnect connects the inputs of the LB and the feedback outputs of the BLEs to the inputs of the BLEs.
// The local interconnect is often arranged as a local full or partial crossbar.
// See Figure 4 in the paper.
// Over time, K and N have increased.
// More K means more functionality in a single LUT
// More K leads to less logic in the critical path
// More N means less demand for fast inter-LB routing
// The area of the LUT increases exponentially with K as more SRAM cells are needed (2^K)
// More K linearly degrades the speed of the LUT
// If the local interconnect is a crossbar, its size increases quadratically with N
// If the local interconnect is a crossbar, its size decreases linearly with N
// Empirically, the best size for K is 4-6, and for N, it is 3-10
// First LUT-based FPGA from Xilinx: N = 2, K = 3
// Around 2000: 4-LUTs common
// Study: 4-LUTs vs 6-LUTS: 6-LUTS:14% more perf, 17% bigger
// Fracturable LUTs can be broken down into smaller LUTs, but limitations like shared inputs
// FPGA architectures from Xilinx and Altera converge to relatively large LBs with 8 and 10 N
// Future designs even bigger LBs
// inter-LB wire delay scales poorly with a process shrink
// Larger LB sizes can lead to faster CAD tool runtimes
// Modern FPGAs have more than 1 FFs per BLE
// Even though there are optimization, the core ideas stayed similar
// 22% of logic elements in FPGAs are implementing arithmetic
// These operations can be implemented with LUTs but are inefficient
// A ripple carry adder requires 2 * the number of bits LUTs
// This leads to high logic utilization and long critical paths
// All modern FPGAs include hardened arithmetic circuitry in their logic blocks
// How the arithmetic is accelerated is a design choice
// It can be a dedicated adder between two LUTs
// It can be just a fast path for the carry bit
// At least 3x faster than LUT-based implementations
// NOTE: there is more detail on the different types of arithmetic optimizations in the paper
// Recently, deep learning has become a key workload
// Deep learning has multiply-accumulate operations at its core, which could benefit from hardened, bigger hardened arithmetic
// Programmable routing is over 50% of the area of an FPGA
// Programmable routing accounts for over half the critical path delay
// High multiplier density in signal processing and communication applications
// Main design philosophy of the DSP block is to minimize the number of soft logic used to implement common DSP algorithms
// FPGA CAD tools will automatically map multiplication to DSP blocks
// Bigger FPGA designs always require a memory buffer
// Making soft memory out of LUTs is over 100x less dense than SRAM cells
// Modern FPGAs are about 25% BRAM
* [[[Bot21]]]
+Andrew Boutros, Betz Vaughn+,
_FPGA architecture: Principles and progression_,
IEEE Circuits and Systems Magazine 21.2 (2021): 4-29.,
2021
link:pass:[https://doi.org/10.1109/MCAS.2021.3071607][🔗^]
link:pass:[https://sci-hub.st/10.1109/MCAS.2021.3071607][📁^]

// TODO: This is an application note; how to cite it?
// TODO: Especially Cri in the link is wrong
// TODO: Source for one definition. Necessary?
// A critical path is a path in the design which must meet certain critical timing requirements in order for the system to function properly
* [[[Cri95]]]
_Critical path analysis for field-programmable gate arrays_,
Microprocessors and Microsystems, Volume 19, Issue 7, Pages 435-439,
1995
link:pass:[https://doi.org/10.1016/0141-9331(95)90010-1][🔗^]
link:pass:[https://sci-hub.st/10.1016/0141-9331(95)90010-1][📁^]

// Compares RTL/HDL to assembly
// High-level languages improved productivity
// HDL has enabled the wide adoption of simulation tools
// First HLS tools 1990s
// In the 2000s: shift to electronic system level (ESL) paradigm that facilitates exploration synthesis and verification of complex SoCs
// Intro of first languages with a system-level abstraction like SystemC or SystemVerilog
// 2000s transaction-level modeling
// ESL paradigm shift caused by rising system complexities
// HLS reduced time for creating hardware
// HLS reduced time for verification
// HLS enables the reuse of the same specification for different targets (ASICs, FPGAs, different ASICS, and FPGAs)
// functional specification = untimed high level description
// NOTE: contains more info about HLS design flow
// HLS tools transform an untimed specification into a fully timed implementation
// HLS tools generate custom architecture to efficiently implement the specification
// HLS tools generate an RTL implementation
// DIAGRAM: High-level synthesis design steps
// Generated architecture (usually) consists of a datapath and a controller.
// Generated architecture also has memory banks and communications interfaces.
// HLS tools usually perform seven tasks:
// 1. Compiling the specification
// 2. Allocating/Creating hardware resources
// 3. Scheduling operations to clock cycles
// 4. Binding operations to functional units
// 5. Binding variables to storage elements
// 6. Binding transfers to connection units
// 7. Generating the RTL architecture
// Steps 2-6 are called interdependent
// Compiling transforms the specification into a formal description
// Compiling performs optimizations
// Formal model classically exhibits data and control dependencies.
// Data flow graph: Every operation is a node, and the edges are values (input, temporary, and output)
// A pure data flow graph (DFG) models data flow only
// In some cases, a pure DFG can be created. Can be done by completely unrolling loops and multiplexing conditional assignments.
// Pure DFG is big and impractical
// Cannot support unbounded iteration and nonstatic control flow (like goto)
// control and data flow graph (CDFG) models data and control flow
// CDFG nodes are called basic blocks and are a straight sequence of statements
// CDFG edges can be conditional and represent if or switch constructs.
// CDFGs are more expressive because they can represent loops with unbounded iteration (those that cannot be unrolled)
// Allocation defines the types and number of resources that are needed to satisfy the design constraints
// Resources are functional units, storage elements, and communication interfaces
// HLS tools have an RTL component library with basic resources.
// Scheduling determines which operations run in which clock cycle.
// All operations must be scheduled into cycles
// If there are no data dependencies between operations, they can be scheduled in parallel
// Every variable that carries values over multiple cycles must be bound to a storage element
// Variables with nonoverlapping lifetimes can be bound to the same storage element
// Every operation must be bound to a functional unit that can perform the operation
// Every connection between functional units and storage elements must be bound to a connection unit
// After allocation, scheduling, and binding, the RTL architecture can be generated and output
// The architecture classically includes a controller and a datapath
// Storage elements: registers, memories, etc.
// functional units: ALUs, multipliers, DSPs, and other custom functions, etc.
// connection units: buses, tristate drivers, multiplexers, etc.
// The datapath consists of the storage elements, functional units, and connection units
// All these components can be connected arbitrarily through buses
// They can also be pipelined
// The controller is a finite state machine (FSM)
// The controller orchestrates the datapath by setting values of control signals of the datapath
// The inputs of the controller can come from primary inputs or from the datapath
// The controller consists of three parts: the next state logic, a state register, and the output logic.
// The next state logic computes the next state of the FSM from the current state and the inputs.
// The state register contains the current state of the controller
// The output logic sets the control signals according to the current state
// The output logic also sets control outputs that can be used as inputs for the datapath
// The controller is usually built with hardwired logic but can be more complex with memories and such
// The controller is usually a custom processor if it is more complex.
// The state register is then called the program counter. This shows that it is just a processor.
// NOTE: Only read until "Several design-flows."
* [[[Cou09]]]
+Philippe Coussy, Daniel D. Gajski, Michael Meredith, Andres Takach+,
_An Introduction to High-Level Synthesis_,
IEEE Design & Test of Computers, Volume 26, Issue 4, Pages 8-17,
2009
link:pass:[https://doi.org/10.1109/MDT.2009.69][🔗^]
link:pass:[https://sci-hub.st/10.1109/MDT.2009.69][📁^]

* [[[Ber09]]]
+Guido Bertoni, Joan Daemen, Michaël Peeters, Gilles Van Assche+
_Keccak sponge function family main document._,
Submission to NIST (Round 2) 3, no. 30 (2009): 320-337,
2009
link:pass:[https://keccak.team/files/Keccak-submission-3.pdf][📁^]

* [[[keccak-crate]]]
_keccak - crates.io_,
Pure Rust implementation of the Keccak sponge function, including the keccak-f and keccak-p variants,
2023
link:pass:[https://crates.io/crates/keccak][🔗^]

<<<

== Appendix

.`minmax` function in {cpp}
[source#minmax-cpp-listing.linenums,cpp]
----
include::experiments/cpp-hls-experiments/minmax.cpp[tag=function]
----

.`md5` function in {cpp}
[source#md5-cpp-listing.linenums,cpp]
----
include::experiments/cpp-hls-experiments/md5.cpp[tag=function]
----

.`keccak` function in {cpp}
[source#keccak-cpp-listing.linenums,cpp]
----
include::experiments/cpp-hls-experiments/keccak.cpp[tag=function]
----

.`keccak` function in Rust
[source#keccak-rust-listing.linenums,rust]
----
include::../rust-keccak/src/keccak.rs[tag=function]
----

.idiomatic `keccak` function in Rust
[source#keccak-idiom-listing.linenums,rust]
----
include::../rust-keccak/src/keccak_idiomatic.rs[tag=function]
----

.`keccak_p` function from <<keccak-crate>>
[source#keccak-crates-implementation-listing.linenums.hundred_max,rust]
----
include::samples/keccak_crates_implementation.rs[tag=function]
----


<<<

// TODO: Make sure this graph is displayed correctly,
.Simplified CFG for the LLVM IR generated by the Rust compiler from the minmax function with optimizations for performance
[graphviz#minmax-speed-cfg.slightly-oversized-content,opts=inline,width=15cm]
----
include::samples/minmax_speed_control_flow.dot[]
----

// Reference thesis:
// * https://webthesis.biblio.polito.it/7573/1/tesi.pdf
// * https://scholarworks.gvsu.edu/cgi/viewcontent.cgi?article=1754&context=theses

include::styles/trailing-scripts.adoc[]


// Final checklist:
// * are all abbreviations defined?
// * are all abbreviations linked to Wikipedia (or somewhere else)?
// * are all references used?
// * are all references linked to the correct source?
// * are all TODOs processed?
// * are the product names consistent? (Bambu)
// * check for duplicate references
// * check for broken references
// * archive.org all links
// * Check for duplication of information
// * oxford comma
// * style code blocks
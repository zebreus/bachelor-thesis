:doctype: book
:last-update-label!:
:imagesdir: images
:source-highlighter: highlight.js
:rouge-style: github
:highlightjs-languages: rust, verilog
// Available themes: https://highlightjs.org/static/demo/
:highlightjs-theme: ../../11.8.0/styles/gradient-light
:toclevels: 2
:stem:
:toc: macro
:sectanchors:
:notitle:
:title-page: false
:stylesheet: Readme.css
:toclevels: 3
:kroki-server-url: http://localhost:8000

image::logo_hda.svg[role=logo]

[.university.text-center]
Darmstadt University of Applied Sciences

[.faculty.text-center]
Faculty of Computer Science

[discrete]
= Is High-level synthesis from Rust possible using existing tools?

[.description.text-center]
Submitted in partial fulfilment of the requirements for the degree of +
Bachelor of Science (B.Sc.)

[.presented-by.text-center]
by +
*Lennart Eichhorn* +
[small]+Matriculation number: 759253+ +


[.other-people]
First Examiner:: Prof. Dr. Stefan Rapp
Second Examiner:: Prof. Dr. Ronald Charles Moore

<<<

[discrete]
== Abstract

// A short summary of the contents in English of about one page. The following
// points should be addressed in particular:

// * Motivation: Why did this work come about? Why is the topic of the
// work interesting (for the general public)? The motivation should be
// abstracted as far as possible from the specific tasks that may be given
// by a company.
// * Content: What is the content of this thesis? What exactly is covered in
// the thesis? The methodology and working method should be briefly
// discussed here.
// * Results: What are the results of this work? A brief overview of the
// most important results as a teaser to read the complete thesis.

// [NOTE]
// ====
// A great guide by Kent Beck how to write good abstracts can be found here:

// <https://plg.uwaterloo.ca/~migod/research/beckOOPSLA.html>
// ====

<<<

toc::[]

<<<

// Start with section and part numbering
:sectnums:
:part-signifier: Part
:partnums:

= Thesis

== Introduction

The popularity of the Rust programming language is rising and it is one of the most loved new programming languages. It integrates modern tooling like standardized dependency management, testing, documentation generation, formatting and building. In the future, the adoption will probably increase further and it could replace {cpp} as the most common system programming language <<Bug22>>.
It is possible that Rust also provides benefits in domains other than systems programming. It has been shown that Rust can be used for other domains such as GPU programming, web development or logic programming <<Sah22>> <<Byc22>> <<Kyr22>>. This enables us to use some of the benefits of Rust <<Bug22>> <<Cos19>>.

This paper will explore how Rust can be used in the domain of FPGA firmware development. Usually, FPGA firmware is developed in a Hardware Description Language (HDL) such as Verilog or VHDL. In these languages, the programmer has to describe the hardware in detail. This is a low-level approach that can lead to efficient designs, but it is quite time-consuming <<Mil20>>. The rust-hdl project enables us to express hardware descriptions in Rust similar to traditional HDLs <<rusthdl>>. In addition to manually writing hardware descriptions in an HDL, it is also possible to use High-level synthesis (HLS) to generate hardware descriptions in HDLs from an algorithmic description written in high-level programming languages. This provides increased productivity at the cost of slightly less optimized designs <<Mil20>>. This process is currently mostly used with C or {cpp} because they are the most common system programming languages. There are multiple HLS tools available that can synthesize HDL code from {cpp} code. Multiple of these tools are based around the LLVM compiler infrastructure <<Nan16>>. There is one reported use of Rust as an HLS language but that is of no use to us as their tools are not available <<Har22>>. As Rust is also based on LLVM, it is possible that it can be used with these tools too. 

To figure out whether Rust can be used as a source language for HLS, we searched for HLS tools that can be used for Rust. We developed a modular process for integrating HLS tools with rust-hdl <<rusthdl>>. We used this process to implement a proof-of-concept integration with the PandA Bambu HLS framework and showed that it is possible to use Rust as a source language for HLS. We also evaluated the resulting solution by synthesizing a simple example. We found that the solution is not yet suitable for production use, but that it is a promising starting point for further development.

// What exactly is the problem
// Why is this a problem

== How is hardware design done today

It is assumed that you have some knowledge of the Rust programming language and systems programming. This section will provide an overview about the topics that are relevant for this paper. It will also provide an overview about the related work that has been done for HLS from Rust.

=== Target platforms

// TODO: Maybe custom IC instead of ASIC
There are two target platforms for logic design: field-programmable gate arrays (FPGA) and application specific integrated circuits (ASIC).

==== What is an FPGA

// // What are CPUs?
// //TODO: Citations
// // TODO: Idk if the intro is appropriate
// CPUs process their instructions one by one. They are purpose-built machines that are carefully designed for processing lots of instructions in sequence. Traditional programming languages reflect this design for the most part. They are designed to make it easy for humans to write programs that can be executed sequentially one instruction at a time. There are approaches to parallelism, but they are either limited to having multiple threads of execution that run from top to bottom simultaneously. Or they have some instructions that perform the same operation on a fixed amount of data elements at the same time.


// // TODO: citation needed
// Field programmable gate arrays (FPGAs) are not designed to process instructions. As the name field programmable gate array implies, they consist of programmable logic gates with programmable connections between them. So while a CPU is a logic circuit designed to do one thing, an FPGA is a circuit designed to emulate other logic circuits. 

// // TODO: FPGA 

// // An FPGA can be used to emulate a CPU.
// // TODO: citation needed
// For example, a design describing the logic gates and connections that make up a CPU can be programed to the FPGA, so it will behave exactly like the CPU and can process instructions. Such a 'soft' CPU will be much bigger and therefore slower than a real CPU because a circuit emulated on an FPGA takes up more space than an application-specific circuit. This is however really useful for prototyping because you can test the CPU design in hardware with external peripherals, like memory and I/O devices.

// // More details on using FPGAs for hardware development
// // TODO: citation needed
// In the past, FPGAs were mostly used like this for prototyping hardware development. Hardware designers designed a circuit in a hardware description language (HDL) and simulated it as a first stage of verification. If that worked it can be deployed to an FPGA, and get tested there. If everything works as expected the design can be taped out and manufactured as an ASIC.

// // In comparison to an FPGA a CPU is an ASIC, I think. But I have no source for that

// // FPGA as computational accelerators
// // TODO: citation needed
// This is still the most common use case for FPGAs, but it is not the only one. In recent years the usage of FPGAs as pseudo-general-purpose computational accelerators became more relevant. Here you do not use them to prototype circuits that will eventually be taped out but as the final platform. FPGAs used in this context are known as _computational FPGAs_. It is somewhat comparable to the use of GPUs as computing accelerators. But where GPUs excel at tasks that perform the same operations in parallel on massive amounts of data, FPGAs can be used for some kind of computations with irregular parallelism with static structure. In opposition to GPUs, it is not yet clear what an appropriate abstraction for the computational pattern used with computational FPGAs is.

// // Modern FPGAs have hardware blocks






// What is an CPU

// What is an GPU

// What is an ASIC / Custom accelerator
// TODO: This is a stub
Another solution is to produce a custom chip that implements the operations in hardware.

// What is an FPGA
Instead of producing the circuit into a chip, it is possible to program the FPGA to emulate the circuit. FPGAs are off-the-shelf reconfigurable ICs capable of implementing any digital circuit. Design cost and time with FPGAs is much lower than for custom chips. Another benfits of using FPGAs is that the circuit can be upgraded after deployment. The downsides of using FPGAs are lower performance and higher power consumption compared to custom chips. They are considered compelling options for small to medium sized projects. Because FPGAs are off-the-shelf components they dont have the high upfront cost of ASICs. <<Bot21>>

// CN: FPGAs are suitable for small to medium sized projects. Because they dont have the high upfront cost of ASICs.
// CN: They use up more space than ASICs, so they are more expensive than the same ASIC produced at high volume.

// TODO: Early FPGA diagram
// TODO: improve note

NOTE: Different FPGA vendors use different names for the same things. <<Lah19>> This paper uses LB, made of LEs, made of LUTs and FFs.

// How do basic FPGAs work
At the core FPGAs are programmable logic elements that can represent a simple logic function. Every logic element has around 4-6 input bits (vendor specific). The inputs are used to address a programmable lookup table. The lookup table contains the truth table of the logic function. The output of the lookup table is coupled with an output register of the LE. This way the LE can also act as a flip-flop. <<Bot21>>
Multiple logic elements are grouped into a logic block (LB). A logic block provides a local interconnect between the logic elements. The local interconnect is used to connect the inputs of the logic block and the outputs of the logic elements to the inputs of the logic elements. The local interconnect is usually realized as a programmable crossbar. <<Bot21>>
An FPGA is made up of multiple logic blocks that are connected by programmable routing. There are multiple ways in which global routing can be realized, but the most popular one is an island-style architecture. It is shown in FIGURE XXX. <<Bot21>>

// TODO: Insert a figure on island style architecture
// FPGAs consist of configurable logic blocks (CLB) that are connected by programmable interconnects. The CLBs consist of multiple basic logic elements (BLE) and a local interconnect. At the core of each BLE is a Lookup table (LUT)<<Lah19>> <<Bot21>> 

// How are FPGAs programmed
// TODO: Cititation needed
All the programmable components are controlled by configuration stored in SRAM cells. FPGAs are programmed with a bitstream that contains the individual bits for every SRAM cell in the correct order. They usually have a serial interface where the FPGA can accept the bitstream and program itself. <<citation-needed>>

// The logic blocks are connected by a network of programmable interconnects. The logic blocks and interconnects are programmed by a bitstream. The bitstream is a binary file that configures the FPGA. The bitstream is generated from a hardware description language (HDL) design. <<Bot21>>

// Explain the concept of the critical path
For every path that the data takes through the circuit it needs a certain amount of time. The longer the path and the more things are in it, the longer it takes to take the path. A critical path is a path that must meet certain timing requirements for the design to function properly. <<Cri95>> If the circuit for example is clocked, then there are probably some actions that should be finished before the next clock cycle. If they do not the clock needs to be slowed down or the circuit will behave in unexpected ways. For this reason the critical path limits the maximum frequency. <<Bot21>>

// What is the difference in modern FPGAs
Modern FPGAs improve critical path delay by hardening certain features of the FPGA. Hardening means that a feature is implemented in a fixed way instead of being programmable. This Which features are hardened depends on the FPGA model. All modern FPGAs include hardened circuitry for arithmetic operations in their logic blocks. The exact details of the hardening depend on the FPGA model, but it usualy involes a fastpath for a carry bit. This is at least three times faster than a pure LUT based implementation. <<Bot21>>

DSP blocks are another common feature on modern FPGAs. They minimize the number of soft logic operations needed to implement common DSP algorithms. Like LBs they are connected to the programmable routing. Depending on the FPGA model, they can be configured to perform operations like multiplication, addition, subtraction, accumulation, and/or multiplication-accumulation in various sizes. <<Bot21>> <<Lah19>>

Bigger FPGA designs almost always require a memory buffer. Building a memory buffer out of logic blocks is possible, but not very efficient. Modern FPGAs include hardened memory blocks that can be used as memory buffers. They are called block RAM (BRAM). BRAMs are over 100 timer more dense than soft memory made from LUTs. BRAMs are about 25% of the area of modern FPGAs. <<Bot21>> <<Lah19>>

// What are the steps to convert an RTL design to an FPGA
// TODO: Mapping the netlist to the FPGA architecture is called technology mapping. Citation needed
Converting an RTL design to a bitstream is a multi-step process. The first step is synthesis. The RTL design is converted to a structural gate-level description. This is called a netlist. The netlist is then mapped to the block types availabe on the specific FPGA model. A series of complex optimizations can also be performed at this stage. After that the blocks  are placed to specific blocks on the FPGA and the connections between them are routed. It is crucial to minimize the routing distance between logic blocks, because routing accounts for over 50% of the critical path delay. As it is now known how every part of the FPGA should behave, the bitstream can be generated. <<Bot21>> 

All these steps are performed by a CAD tool usually provided by the FPGA vendor. For the biggest FPGA vendors these tools are integrated into their respective IDE. <<citation-needed>> Recent developments in the open-source community have led to the development of open-source tools that can perform these steps. <<citation-needed>>

// TODO: Mention timing analyzers and shit


// How do they compare to ASICs

// Notable FPGA vendors?

// Context of this paper

=== Netlists are synthesized to hardware

// TODO: This is a stub
All approaches of hardware design end up generating a structural gate-level description called netlist typically in a subste SystemVerilog. This netlist is then used to generate the actual hardware design in a mostly automated fashion. How this step actually works depends on what kind of Hardware is targeted e.g. FPGA, ASIC or fully custom chips.
//<<Fla20>>

This step is basically manufacturing a design. The focus of this paper is only on design and not on manufacturing, so we will not go into detail about this step. It is just important to know that a netlist is something that can be manufactured / deployed to a FPGA.

=== Design using traditional Hardware Description Languages
// What are common HDLs and what are they used for
// TODO: Systema verilog is most common nowadays
Traditionally, logic design is done in hardware description languages (HDL). There are two established ones, Verilog and VHDL. VHDL has a slightly higher level of abstraction and some features that make it easier to manage bigger projects. In some ways, the relation between Verilog and VHDL is comparable to the relation between C and {cpp} in terms of features and abstraction. Both can be used to model the structure of hardware equally efficiently, so the choice is mostly a matter of personal preference. <<Smi96>>

// TODO: Mention timing

// What do you describe in HDLs
// TODO: Citation needed
Most commonly HDLs are used to describe circuits in a register-transfer level (RTL) abstraction. On RTL these languages describe registers that can hold state and the combinational (time-independent) logic that connects them. In HDLs the registers and combinational logic can be bundled into a module to make it reusable. These modules can then be connected to form a larger circuit. This is the basic structure of an HDL design.

// What distinguishes them from progamming languages

// What is a typical HDL design flow
A typical HDL design flow consists of four phases, design, verification, synthesis and implementation. In the design phase, the circuit is designed in an HDL. The design is then verified in various ways. To verify the behaviour of a design testbenches are defined. These testbenches (usually also written in HDL) instantiate the module of the design under test (DUT), exercise the inputs and verify that the outputs behave as expected. A logic simulator is used to execute the testbenches. This is comparable to unittesting in programming languages. After the design is verified, a logic synthesis tool is used to synthesize the design into a optimized gate-level logic description (netlist). A formal equivalence tool can then be used to verify that the netlist is equivalent to the original design. In the implementation phase, the netlist is mapped to the target hardware. <<Fla20>>
 
// What are the problems with HDLs
The use of HDLs is mostly limited to hardware engineers

// What are the possible solutions

==== SystemVerilog sample

In SystemVerilog units of logic can be encapsulated into modules. Modules can have inputs, outputs and internal state. The following listing shows a simple module that implements a blinker. The blinker has a clock input and a blinker output. At every rising edge of the clock a internal counter is incremented by one. Once the counter reaches a certain value, the blinker output is toggled. The counter is reset to zero when the blinker output is toggled.

.Simple blinker module in SystemVerilog
[source,verilog]
----
include::systemverilog-blinker/blinker.sv[]
----

Verification in SystemVerilog can be achieved by using testbenches that instantiate the module under test and exercise its inputs. Listing XXX shows a testbench for the blinker module. The testbench instantiates the blinker module and connects it to a clock signal. It also asserts that the blinker output toggles every 10 clock cycles.

.Testbench for the blinker module in SystemVerilog
[source,verilog]
----
include::systemverilog-blinker/blinker_tb.sv[]
----

// TODO: citations? 
// TODO: More detail?

=== Design using alternative Hardware description languages

// TODO: This is a stub
There are multiple modern HDLs that try to improve on the shortcomings of Verilog and VHDL. Most of them try to bring some features from modern programming languages to hardware design. (linting, formatting, dependency management, namespaces/scoping, better support for multifile projects, ...). These languages are usually transpiled to Verilog and then synthesized to netlists.

// TODO: List of alternative HDLs

// TODO: Mention timing somewhere

==== rust-hdl

rust-hdl is a Rust crate that allows describing RTL logic in Rust. The logic can then be transpiled to Verilog. rust-hdl also includes tools for simulation and verification. Because a rust-hdl based design is just a Rust program, it can use most of the Rust ecosystem features. This includes the Rust testing framework for verification and simulation. It also makes it possible to reuse and share designs using the Rust packagemanager cargo. <<rusthdl>>

.rust-hdl struct for a simple blinker module.
[source,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-struct]
----

In rust-hdl logic blocks (equivalent to Verilog modules) are defined as structs. The field of these logic blocks corresponds to the external and internal ports of the module. External ports are defined as `Signal` with a direction and a data type. If the logic block is contains another logic block it is also defined here. rust-hdl provides some basic logic blocks for common functions like D flip-flops (DFF) or constants. <<rusthdl>> An example for a logic block definition is shown above in listing XXX.

The combinational logic that connects these signals is described in the update function. Every rust-hdl signal has a .next field that determines the value of the signal after the update function. The update function must assign a value to all the .next fields. If there are multiple assignments to the .next fields the last one is valid, like in normal Rust. As Rust can express much more than just combinational logic, rust-hdl only allows a limited synthesizable subset of Rust in the update function. <<rusthdl>> 

.The restrictions of the synthesizable subset include:
- No local variables
- Assignments are only allowed to .next fields of signals
- Function and method calls are limited to a small subset of library functions
- Only range based for loops are allowed

<<rusthdl>> 

.rust-hdl update function for the blinker.
[source,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-logic]
----

rust-hdl also enforces an additional set of semantic rules about what the update function can contain. For example it does not allow undriven nets, so there must be at least one assignment to the `.next` value of every field. If there are multiple assignment to the next value of a signal, the last one is valid, like in normal Rust. rust-hdl also forbids to use the `.next` value on the right side of an expression. rust-hdl enforces these rules at compiletime and generates somewhat helpful error messages. <<rusthdl>> An example for a update function is shown above in listing XXX.

.Simulating and verifying the blinker
[source,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-test]
----

The Rust unit testing framework can be used to simulate and verify rust-hdl modules. Listing XXX shows a testcase that simulates and verifies that the blinker module toggles its output with the specified frequency (10 cycles in that case). The test case can be run with `cargo test` like any other Rust unit test.

.Generating verilog from the rusthdl blinker
[source,rust]
----
include::rusthdl-blinker/src/main.rs[tag=generate-verilog]
----

rust-hdl can then be used to generate a Verilog description of the design as shown in listing XXX. The generated Verilog code can then be used with any Verilog toolchain to deploy the design to a FPGA. <<rusthdl>>

// Justify why rust-hdl is a good choice even though it is more verbose
// TODO: Compare LoC
rust-hdl code is a more verbose than Verilog code. This is mostly because of the additional type annotations and the need to explicitly assign the .next value of every signal. 


=== Design using High-level synthesis
// What is HLS
High-level synthesis (HLS) is a process that can generate a RTL specification of a circuit from a description in a high-level programming language. This is a more productive approach than writing RTL code directly, but the resulting designs are usually less efficient. The generated RTL code can then be used in the same way as manually written RTL code. <<Mil20>> <<Nan16>> <<Lah19>> <<Cou09>>


// Performance of HLS compared to RTL
// TODO: Weirdly written, (which experiments?)
The main advantages of HLS are reduced design time and lower development cost. On average the development time of HLS designs is only a third than that of equivalent RTL designs. The tradeoff for the lower development time seems to be that HLS designs on average take up around 40% more basic FPGA resources than RTL designs. The performance of HLS designs is around two thirds of that of a RTL design. These metrics generalize over many different HLS tools and input languages and there is a big variance between different experiments. In about 40% of the cases the HLS design was more efficient or performant than the RTL design. These metrics for performance and resource usage show a lot of variance between experiments, in about 40% of designs the HLS design was actually more performant than the RTL design. In about 30% it was more resource efficient. However in terms of development time there is little variance, in 90% of the experiments the HLS design was faster than the RTL design. <<Lah19>>

==== How high-level synthesis works
// Intro
// TODO: Remove?
High-level synthesis tools generate a timed RTL implementation of a circuit from a functional specification. A functional specification is a untimed description of the desired behavior of the circuit. <<Cou09>>

// What are the tasks that HLS performs
Usually HLS tools seperate the HLS process into seven tasks. <<Cou09>> They can be seperated into three stages.

// Compilation of the functional specification
First the functional specification is compiled into a formal model. The formal model is usually a control and data flow graph (CDFG). In the CDFG every node (called basic block) represents a static sequence of statements. The edges between the nodes represent conditional data flow. Opposed to a normal data flow graph (DFG), the edges can be conditional. <<Cou09>> In this step transformations can be applied to the functional specification. These transformations can include loop unrolling, inlining, dead code elimination and other common software compilation optimizations. <<Cou09>> 

// TODO: I dont have a citation, but say this: While these hardware resources can be real hardware resources, in this stage they are more comparable to Verilog modules. they are not (necessarily) real FPGA hardware resources, but can also be arbirary soft logic. ???

// What are the hardware resources
The resulting formal model then has to be mapped onto hardware resources. The types of resources are functional units, storage elements and connection units. Functional units perform the actual computation on the data. They can be multipliers, arithmetic logic units, or other custom functions. Storage elements are used to store values over multiple cycles. They include registers, memories, or other custom storage elements. Connection units represent the connections between the functional units and storage elements. They include resources like multiplexers and buses. HLS tools include RTL descriptions for all of their supported resource types. <<Cou09>>

// Scheduling, allocation, binding
// TODO:  This binding process does not determine the placement of the resources on the FPGA. (maybe it takses it into account, but not neces..)
Allocation determins the kinds and number of resources that are needed to satisfy the design constraints. Scheduling then schedules the operations into cycles based on the available resource types. Operations that do not have data dependencies between them can be scheduled in parallel.The next step after scheduling is called binding. During binding operations get assigned to specific functional units. Variables that carry a value over multiple cycles must be bound to storage elements. Finally it is assigned which connection units connect which functional units and storage elements. Scheduling information can be used to bind multiple variable with non-overlapping lifetimes to the same storage elements. <<Cou09>> 

// How does the resulting RTL look
The final step of HLS is to generate the RTL architecture for the design. Classically the architecture consists of a controller and a data path. The data path contains all the storage elements, functional units and connection units described earlier. The controller is responsible for driving the data path. <<Cou09>> 

// Data path
The data path contains all the storage elements, functional units and connection units described earlier. These components can be connected arbitrarily by buses. The inputs of the data path are data inputs from external sources and control inputs from the controller. The outputs of the data path can be data outputs or control outputs. The data path also receives control signal from the controller. The control signals control how the components get connected; they orchestrate the data flow through the data path. <<Cou09>> 

// Controller
The controller usualy consists of three parts. The state register contains the id of the current state. The output logic generates the control signals that drive the data path based on the current state. The output logic also generates control outputs that can act as inputs for the data path. The next state logic computes the next state of the FSM based on the inputs and the current state. <<Cou09>> 

// Disclaimer
While this is the typical architecture, different HLS tools can also do it differently. An alternative approach is for example to generate a processor with a hardcoded program and instruction set that is optimized for the task at hand. <<citation-needed>>

// TODO: Diagram for typical HLS architecture from Cou09

// TODO: IP Core


==== High-level synthesis tools
// Available HLS tools
HLS tools can be distingushed into two major categories. Those that accept a general purpose language and those that accept a domain specific language (DSL) a an input. Using a DSL as input can lead to better performing designs, but it also raises challenges for adoption. A general purpose language makes it easier for the algorithm designer, who is usually a software developer, to write code. <<Nan16>> The most common input languages for HLS are C based, including C, {cpp}, and SystemC. <<Lah19>> 

// What HLS tools are there
// TODO: Add citation for usually.
// TODO: Add citation for bambu is the biggest. 
HLS tools are usually available as a part of the IDE provided by the FPGA vendors. For example, AMD Xilinx provides the Vivado HLS tool as part of its IDE and Intel Altera includes the Intel HLS Compiler in its Quartus Prime IDE. <<intel-hls>> The most popular HLS tool in academia is Vivado HLS <<Lah19>>. There are also a few open-source HLS tools available. The biggest open-source HLS tool that is currently activly maintained is panda bambu. <<Nan16>> 

bambu provides as a research environment to experiment with new ideas in HLS. It can take C/C++ and LLVM IR as input. <<Fer21>>

In this paper we will use bambu as a HLS tool, because it is open-source and supports the latest LLVM version. 

// What are the problems with those languages
// TODO: This section does probably not belong in this part
Old programming languages, we have better alternatives nowadays
More steps in the design flow
More explicit. Register can be anything


==== Bambu HLS
// Bambu intro
Bambu is a open-source academic HLS tool. <<Fer21>> <<Nan16>> Its architecture is designed to make it easy to experiment with new ideas across high-level synthesis and related topics. It supports input specifications in standart C/C++ or the intermediate representations of GCC or clang/LLVM. <<Fer21>>

// Bambu compared to commercial offerings
// TODO: Nan16 has benchmarks

// Bambu architecture overview
Bambu is based on a three stage design. The frontend is used to convert input specifications in various formats into a static single assignment (SSA) IR. The middle end performs various transformations and analysis on the SSA IR. The backend performs the actual architectural synthsis. <<Fer21>>

// Frontend
The frontend is utilizes a plugin for GCC or clang to process a input specification in any of the formats supported by these two compilers. Noteably this includes support for LLVM IR through clang. Bambu then generates its own SSA IR from the control from the call graph and control flow information provided by GCC or clang. <<Fer21>>


// Middleend
// TODO: Last sentence is shit
The middleend applies a set of analysis and transformations to the specification. This includes common software compilation optimizations such as loop optimizations and dead code eliminatin. It also includes more HLS specific optimizations. For example multiplications and division with constants are replaced with shift and add operations, because real multiplication is expensive in hardware. Bitwidth and range analysis optimizations are also performed, because it does not need to target a data-path with a fixed width (i.e. 32bit, 64bit). <<Fer21>>

// Backend
The backend performs the actual architectural synthesis. This is mostly done as described in section XXX. Bambus biggest difference is that the synthesis process acts on every function individually. It generates a controller and data path for every function. If a function calls into another function the generated logic for the other function is instatiated as a submodule. Bambu optimizes submodules that are shared between multiple modules. Bambu also provides hardcoded optimized modules for functions from common libraries such as `libc` or `libm`. The resulting architecture can be translated to VHDL or Verilog. <<Fer21>>

==== Languages used for High-level synthesis

// TODO explain systems programming language

=== LLVM

// What is LLVM
// TODO: Find source and describe how LLVM is used today
LLVM is a modular compiler framework that can be used to build compilers for many different programming languages. It defines a low-level code representation called LLVM intermediate representation (LLVM IR). <<Lat04>>

// What is LLVM IR
LLVM IR is a strongly typed, static single assignment (SSA) based intermediate representation. LLVM IR is designed to be easy to compile to machine code and to optimize. LLVM IR has a low-level, language independent type system. The type system captures enough type information to safely perform a number of aggressive transformations that would traditionally be attempted only on type-safe languages in source-level compilers. LLVM IR is not intended to be an universal compiler IR, so it does not capture all of the language specific type information. Some of the concepts that are not represented in LLVM IR are classes, inheritance, or exceptionhandling semantics. <<Lat04>>

// How is LLVM used in compilers
Because of the this compilers based on LLVM provide a front-end that process the program code in the source language. The frontend is able to perform language specific optimizations. The compiler frontend emits LLVM IR which is passed to the LLVM backend which performs a variety of transformation and optimizations. The processed LLVM IR is then passed to a code generator backent to translate it into native code for a given target. <<Lat04>> The LLVM project includes code generator backends for many targets. <<Lat04>> <<rustc-guide>>


=== The Rust programming language

// What is Rust
Rust is a modern systems programming language aiming to replace C and {cpp} as the industry standard systems programming language. It offers zero cost memory saftey, a strong type system and a modern toolchain. Rust surpasses all other common memory-safe languages in terms of performance while offering many modern features that more established systems programming languages tend to lack. For these reasons it has been voted the most loved programming language every year since 2016. <<Bug22>> <<Cos19>> <<Kla23>>

// TODO: Insert rust performance diagram

// What is a systems programming language
Systems programming languages are programming languages that can deal with low-level details of memory management, data representation and concurrency.
They are often designed for use in resource-constrained, performance-critical or close-to-hardware programs. They are used to implement operating systems, embedded systems, device drivers and other software that interacts with hardware. Common systems programming languages include C, {CPP} and Rust. <<Kla23>> <<Str12>>

// TODO: Explain how the rust compiler uses LLVM
// Details for this are already summarized at <<rustc-guide>>

// Explain the borrow checker and memory saftey
// TODO: Ask about the first sentence being an exact quote
// TODO: Make the first quote shorter
Rust is the first industry-supported computer programming language to overcome the longstanding trade-off between the control over resource management provided by lower-level languages for systems programming, and the safety guarantees of higher-level languages. <<Bug22>> <<Jun17>> Rust achieves this by enforcing that every variable is always owned by exactly one scope. When that scope ends the variable is destroyed. For this the time between creation and distruction of a variable is called its lifetime. Rust provides semantics for moving ownership between scopes. <<Kla23>>
This model of scope based resource management is called RAII. It is used by many systems programming languages including {cpp}. While {cpp} enables the programmer to break out of that system by using pointers to variables that are not owned by the current scope or its parents, Rust does not allow this. <<Str12>> Rust instead uses a type of reference with attached lifetime information called a borrow. The borrow checker statically guarantees at compiletime that borrows always point to valid objects. It does this by ensuring that the lifetime of every borrow ends at or after the lifetime of the current scope. The compiler also makes sure that there can only be either one mutable borrow or multiple immutable borrows to a value at the same time. This ensures that the values of borrows dont change unexpectedly. The programmer can opt out of the borrow checker by annotation code as `unsafe`. This allows the programmer to use raw pointers and other unsafe constructs. Unsafe code is sometimes neccessary to implement lowlevel data structures, such as Heap memory (`Box` or `Vec`) or types with internal mutability (`Ref`). The standard library provides most of these constructs, so most Rust programs do not need to use unsafe code. <<Kla23>> <<Bug22>> There is ongoing work on verifying that the unsafe code in the standard library is safely encapsulated by its types. <<Jun17>>

// Overview about how the compiler processes Rust code
// TODO: Explain desugaring (the word)
Rust is a compiled language. The Rust compiler (rustc) can compile Rust code to native code for many different platforms. It accomplishes this by compiling Rust to LLVM IR and then using LLVM for code generation. This allows Rust to support many different platforms without having to implement a backend for every platform. It also enables Rust to utilize the large suite of advanced optimizations collected by the LLVM project. The compiler does not generate LLVM IR directly from the input Rust code. Instead it the input code gets passed through multiple IRs, HIR, THIR and MIR. HIR and THIR still resemble Rust code, but some constructs get desugared. The rust compiler uses these stages to perform typechecking and verification. Verified THIR is converted into MIR, which is a CFG based representation of the code. rustc performs flow-sensitive safety checks like borrow-checking on this level. The MIR is also used to apply various Rust-specific optimizations to the code. The Rust compiler can be configured to output the various intermediate representations instead of the generating machine code. <<rustc-guide>>

// Overview about the tooling related features of Rust
A important part of what makes the Rust ecosystem so productive is that Rust offers standardized tooling. Every Rust project (also called crate) contains a `Cargo.toml` file specifying the project metadata and dependencies. The `cargo` tool can then be used to perform all standard tasks like building, executing, unit testing, integration testing, formatting the code, generating documentation, downloading dependencies, building dependencies, managing dependencies, publishing the project, benchmarking, setting up projects and some more. The official community crate registry `crates.io` can be used to easily find dependencies. It also links to the automatically generated documentation for every crate. The tooling alone makes Rust a much better development experience than most systems languages. <<Bug22>>

// 

=== Design using accelerator design languages

Accelerator design languages (ADL) are a family of programming languages that are specifically designed to be synthesized to HDL. They are mostly similar to programming languages but offer many features that are usually only found in HDLs, like more fine-grained control over timing and memories memory access. <<hdl-to-adl>>



== Concept, implementation and architecture

// Short overview of the solution and this section
The goal of this paper is to use Rust as a source language for HLS using an existing HLS tool. First a suitable toolchain has to be designed. Then a concept for integrating the toolchain with rust-hdl has to be developed. Finally, a proof-of-concept implementation will be done to show that the process works and enable evaluation of the resulting solution.

// TODO: Maybe define some criteria for our solution
// * Can synthesize a simple Rust program
// * Can synthesize our md5 implementation
// * Can synthesize most stateless Rust function
// * The synthesized function can use Rust crates from crates.io
// * The existing Rust tooling (linter, formatter, etc.) work with out function

=== Basic toolchain for synthesizing Rust

As there is currently no tool that can synthesize Rust directly the first step in the toolchain needs to convert the Rust code into a language that can be used by an existing HLS tool. There are multiple HLS tools that support C or {cpp} by using LLVM compiler infrastructure. As a result of using LLVM some of these tools can also use LLVMs intermediate representation (LLVM IR) directly as an input language. <<Nan16>> The Rust compiler can compile Rust code to LLVM IR. This will be the starting point of the toolchain

The Rust compiler is frequently updated and the generated LLVM IR usually uses the latest LLVM version. <<citation-needed>> This means that a suited HLS tool needs to be activly maintained to support the generated LLVM IR. The only HLS tool that fulfills these requirements seems to be panda bambu. <<Fer21>> SmartHLS and Vivado HLS may also be capable of operating on LLVM IR, but they are not open source and only available as part of an IDE and not as standalone programs, which makes them unsuitable for our use case.

.The toolchain
[pikchr]
....
   arrow right 150% "Rust" "function"
   box rad 10px "Rust Compiler" fit
   arrow right 190% "LLVM IR" "function"
   box rad 10px "PandA Bambu" fit
   arrow right 130% "Verilog" "RTL"
....

The basic toolchain is relativly straightforward. In a first step the Rust compiler is used to convert a Rust function to a LLVM IR function. The second step passes the generated LLVM IR function to panda bambu which converts it to Verilog. The resulting Verilog can then be used in a larger HDL design.

==== Toolchain proof of concept

// Intro
// TODO: Add a short section on keccak
// TODO: Format keccak name
This toolchain can be used to synthesize Rust functions. The following section will show how a example Rust function can be synthesized. 

.Example Minmax function
[source,rust]
----
/// Minmax function that is as similar as possible as the equivalent cpp function
#[no_mangle]
pub unsafe extern "C" fn minmax(
    numbers: *mut i32,
    numbers_length: i32,
    out_max: &mut i32,
    out_min: &mut i32,
) {
    let input = std::slice::from_raw_parts_mut(numbers, numbers_length as usize);
    let mut local_max = input[0];
    let mut local_min = input[0];

    for i in 0..numbers_length as usize {
        if input[i] > local_max {
            local_max = input[i];
        }
        if input[i] < local_min {
            local_min = input[i];
        }
    }

    *out_max = local_max;
    *out_min = local_min;
}
----

The minmax function shown in listing XXX finds the minimum and maximum in an array of integers. The function takes a pointer to an array of integers (`numbers`), the number of elements in the array (`numbers_length`) , and two pointers to integers (`out_max` and `out_min`). The function finds the smallest and largest value of the array and writes them to the memory locations pointed to by `out_max` and `out_min`. The implementation of the function is kept in a C-like style, so that it can be compared to an equivalend C implementation later.

// TODO: Explain that a Rust slice is just a pointer to an array with a length
// TODO: Specify: bambu does not support the llvm that is generated by slices or smth like that
The function takes a pointer to an array and its size instead of a rust slice, because bambu does not support slices in the interface. This is a limitation of the current implementation of bambu and could be fixed in the future. The function also needs to be annotated with `#[no_mangle]` to instruct the Rust compile to preseve the function name in LLVM IR. This is required because bambu uses the function name to generate the name of the Verilog module. The function is also `extern "C"` to instruct the Rust compiler to generate LLVM IR that is compatible with the standard C ABI. Finally the interface of the function is marked as `unsafe` because it uses raw pointers.

Inside the function the pointer and size are converted to a slice (`input`). Local variables are defined for the minimum and maximum values (`local_max` and `local_min`). The function then iterates over the slice and updates the local minimum and maximum values if a smaller or larger value is found. Finally the local minimum and maximum values are written to the memory locations pointed to by `out_max` and `out_min`. 

.Compile Rust to LLVM IR
[source,console]
----
rustc \
  src/minmax.rs -o minmax.ll --crate-type=lib \
  --emit=llvm-ir # Emit LLVM IR instead of machine code
  -C llvm-args=--opaque-pointers=false # Disable opaque pointers
  -C no-vectorize-loops \
  -C panic=abort \
  -C overflow-checks=off \
  -C opt-level=3 \
  -C linker-plugin-lto=on \
  -C embed-bitcode=on \
  -C lto=fat \
----

The first step is to compile the function to LLVM IR using the rust compiler. The command shown in Listing XXX contains most of the options that are required or recommended to generate LLVM IR that can be used by panda bambu. `src/keccak.rs -o keccak.ll --crate-type=lib` specifies the filenames and that a library and not an executable will be build. The most important options is `--emit=llvm-ir` which tells the compiler to emit LLVM IR instead of machine code.

Recent versions of the Rust compiler will generate LLVM IR with https://llvm.org/docs/OpaquePointers.html[opaque pointers](`ptr` instead of `i32*`) by default. This is not supported by bambu. Opaque pointers can be disable by passing `-C llvm-args="--opaque-pointers=false"` to rustc.
// TODO: Explain LLVM bitcode

// TODO: Does bambu support cpp exceptions?
// TODO: Does bambu support c exit?
// TODO: Describe Rust panic
By default, the Rust compiler generates code that unwinds the stack on panic. The generated LLVM IR will also have an exception-handling personality function added to every function in LLVM IR. This is not synthesizable by bambu. The `-C panic=abort` instructs rustc to instead terminate the program on panic. This is also not synthesizable but bambu seems to compile the code if the panics can never be reached. It may be possible to adjust bambu to provide a custom implementation for panic.

The `-C no-vectorize-loops` option disables loop vectorization. This is required because bambu does not support vector instructions.

Disabling overflow checks for arithmetic operations significantly reduces the number of places where a function can panic. The `-C overflow-checks=off` option disables overflow checks. The Rust compiler can also generate debug assertions. They perform extra safety checks, which can make it easier to find and fix bugs, but these checks usually manifest as panics. To avoid panics the `-C debug-assertions=off` option can be used to disable debug assertions. On every optimization level other than `0` they are automatically disabled, so this option is not required in this example.

The final set of parameters enables various Rust compiler optimizations. These serve three purposes. For once the Rust compiler can perform more optimizations than bambu, because it has more information about the code. The second purpose is to reduce the amount of code that bambu has to synthesize. The third and probably most important purpose is to eliminate dead code that could potentially panic, as that is not supported by bambu. For this reason it is basically always required to set the optimization level to any level higher than `1`.

.Compile LLVM IR to Verilog
[source,console]
----
bambu minmax.ll \
  --compiler=I386_CLANG16 \
  --top-fname=minmax \
  -O3
----

The LLVM IR can then be used with bambu to generate Verilog. The command shown in Listing XXX instructs bambu to generate a Verilog module named `minmax` from the LLVM IR file `minmax.ll`. The `--compiler=I386_CLANG16` option instructs bambu to use clang 16 as the frontend for processing input. bambu supports multiple versions of clang and GCC. The `--top-fname=minmax` option specifies the name of the top level function. The top level function will be used as the entrypoint in the hardware design, comparable to the `main` function in a C program. This will also be the name of the generated Verilog module. The `-O3` option instructs bambu to also perform optimizations in its front and middleend.

// TODO: insert gitpod link or something to the full code sample

==== Interface of the generated Verilog module


.Interface of the counter
[symbolator]
....
module minmax(
  //# {{control|Control}}
  input clk;
  input reset;
  input start_port;
  output done_port;
  //# {{data|Parameters}}
  input [31:0] Pd5;
  input [31:0] Pd6;
  input [31:0] Pd7;
  input [31:0] Pd8;
  //# {{power|Memory}}
  input [63:0] M_Rdata_ram;
  input [1:0] M_DataRdy;
  output [1:0] Mout_oe_ram;
  output [1:0] Mout_we_ram;
  output [63:0] Mout_addr_ram;
  output [63:0] Mout_Wdata_ram;
  output [11:0] Mout_data_ram_size;
);

endmodule
....

.Descriptions of the control signals
[cols="1,4"]
|===
|Port |Description

|`clock`
|The clock signal is used to clock the module

|`reset`
|Resets the module if set to 0.

|`start_port`
|The module will start executing, if this is high and the function is finished. Pinning this to high will cause the function to repeat.

|`done_port`
|Pulses high for one cycle, when the module is done.

|`return_port`
|Contains the return value while `done_port` is high. Can contain random values during function execution. Does only exist if the function has a return value.

|===

The interface of the component is shown in figure XXX and can be split into four categories. The first section contains clocking and control signal including clock, reset signal, start port and done port. The exact meanings of these signal is described in table XXX.

The second section contains the function parameters. When LLVM IR is used as input for bambu the real names of the function parameters get lost and they are replaced with numbered names like `Pd5`. The order of the signals stays the same as the order of the inputs to the original function. The original name of the parameters can be inferred by the order of the numbered names, because bambu always numbers the parameters in the order they appear. In this case `Pd5` is `numbers_length`, `Pd6` is `numbers`, `Pd7` is `out_max` and `Pd8` is `out_min`. Memory pointers got converted to 32 bit numbers, but bambu can also be configured to generate other address sizes.

// TODO: Ask if this kind of memory interface has a name
The original function takes a pointer to memory as input. This means that the generated module needs to have a way to access that memory. Because of this bambu generates a memory interface for the component which needs to be connected to memory. During function execution the component will use this memory interface to retrieve and modify the input values. By default bambu generates a memory interface for a memory with 32-bit data and 32-bit addresses with two parallel direct connections. Most of the signals are double the size they would need to be for a single connection. For these signals the first half controls the first channel and the second half contains the second channel. Table XXX describes the memory interface in more detail.

// TODO: Make tables wider than text if neccessary
.Descriptions of the memory interface
[cols="1,1,3"]
|===
|Port |Size per channel (bit) |Description

|`Mout_oe_ram`
|1
|Set to 1 to read from the channel.

|`Mout_we_ram`
|1
|Set to 1 to write to the channel.

|`Mout_data_ram_size`
|log2(dataWidth) + 1
|Set the width of bits that should be written to the memory. Can be a value between 0 and the width of your data.

|`Mout_addr_ram`
|addressWidth
|Select the address this channel should operate on.

|`M_Wdata_ram`
|dataWidth
|Contains the data that will be written to memory if `Mout_we_ram` is set.

|`M_Rdata_ram`
|dataWidth
|Contains the data that was read from memory if `Mout_oe_ram` was set in the last cycle.

|`M_DataRdy`
|1
|Nonzero if the memory is not ready

|===


=== Integrating the toolchain with rust-hdl

//TODO: Verilator background section
// With rust macros it is possible to execute arbitrary code transformations at compile-time. This can be used 
Using the toolchain as shown in the previous section requires a few manual steps. Especially the changing parameter names are difficult to work with as they need to be adjusted after every modification. To make the toolchain easier to use, a library for integration with rust-hdl was created. The library makes it possible to create a single Rust crate containing both RTL and HLS code in the same project. The library contains steps for synthesizing specific functions from a bigger Rust project, wrapping synthesized Verilog in a rust-hdl structs, highlighting errors in the source code before synthesis, and for generating simulations with Verilator.

// TODO: Move to background
Cargo provides a way to hook into the build process using buildscripts. These are small Rust programs that can modify the Source code before it is passed to the Rust compiler. The Rust compiler also allows custom code transformations at compile-time using procedural macros. Procedural macros backed by Rust functions that are executed at compile-time and can modify the code they are applied to.

The Rust library provides a `#[hls]` macro that can be used to mark Rust modules for HLS. The macro is evaluated two times. Before compilation a buildscript finds all `#[hls]` macros. It then extracts every marked module into a seperate temporary crate. This crate is then synthesized to Verilog using the toolchain mentioned above. A Rust module containing a rust-hdl struct that wraps the synthesized Verilog is then generated. That Rust module is then placed alongside the original source code. During compilation the `#[hls]` macro is evaluated by the Rust compiler. The macro then emits import directives for the module that were synthesized during the buildscript. It also evaluates if the macro is used in a valid context and emits useful error messages if it is not. Diagram XXX shows the macro evaluation process.

.Block diagram of the macro evaluation
[nomnoml]
....
#direction: right
[Rust crate]->[cargo build]

#.downwards: direction=down
[cargo build|
[<downwards> Buildscript|
  [parse rust crate]->[find hls modules]
  [find hls modules]->[extract crate]
  [extract crate]->[compile to LLVM IR]
  [compile to LLVM IR]->[synthesize with bambu]
  [synthesize with bambu]->[parse result]
  [parse result]->[fix parameter names]
  [fix parameter names]->[generate rust_hdl interface]
  [generate rust_hdl interface]->[create rust_hdl module]
  [parse result]->[create C++ library]
  [create C++ library]->[create rust_hdl module]
  [create rust_hdl module]->[place new rust file in original crate]
  [place new rust file in original crate]->[emit directives to link C++ library]
]

[<downwards>Rust compiler|
[<downwards> HLS macro evaluation|
  [parse rust macro]->[check for errors]
  [check for errors]->[emit import directives for synthesized module]
]
[<downwards> Compilation]
[HLS macro evaluation]->[Compilation]
]

[Buildscript]->[Rust compiler]
]
....

Rust-hdl is not able to simulate embedded verilog. Verilator can create a {cpp} library from Verilog that simulates a single component. Such a library is created for every synthesized module. The generated rust-hdl structs contain the glue code neccessary to call their library. The buildscript emits the directives neccessary to link the generated libraries. Then cargo will link the libraries into the final binary after compilation.

By using this approach it is possible to use the toolchain in a rust-hdl project. The modules can be used, simulated, and tested like normal a rust-hdl module. This also allows us to perform behavioral verification directly in rust unit tests. This workflow makes it possible to use HLS from Rust in a fully automated way.

=== Proof-of-concept implementation

// TODO: This is a stub
The proof-of-concept implementation uses a Rust procedural macros to transform a Rust function into a rust-hdl module.

// TODO: Show of an example

==== Explain neccessary compiler flags


== Experiments and results

Hypothesis: HLS from Rust is as performant as HLS from C++
Hypothesis: Using packages makes development faster
Hypothesis: With rust we can easily use packages


=== Show limitations of the synthesizable subset of Rust

Show the limitations of the toolchain. Limitations of the synthesizable subset of Rust.

=== Show how the Rust ecosystem can be used

=== Generated modules

Take a look at the generated modules

=== Performance compared to HLS from {cpp}

Compare performance against {cpp}. basically the same as in first-toolchain.adoc


// Perform experiments and collect data about our solution

== Discussion and evaluation


Discuss limitations

Discuss the results of the performance experiments

Discuss compiler flags? 

// Why is this a solution
// Discuss the results of the experiments

== Conclusion and future work
// TODO: This is a stub
// This is completely written by copilot and should be rewritten by hand
In general the toolchain is working quite well. HLS from Rust is definitly possible with little restrictions. The performance of the generated modules is also quite good. The generated modules are not as good as hand written modules, but they are good enough for most applications. The generated modules are also quite large, but this is not a problem for most applications.

// TODO: This is a stub
// This is completely written by copilot and should be rewritten by hand
The toolchain is also quite easy to use. The only thing that needs to be done is to add a procedural macro to the function that should be synthesized. This is a very simple change and does not require any changes to the function itself. The procedural macro can also be used to add additional information to the function that can be used by the HLS tool. This is very useful for example to specify the clock frequency of the function.


[glossary]
== List of abbreviations
// Abbreviations from here will automatically be linked into the document

// Abbreviations in a random order and links to read more about them
[glossary]
[[FPGA]]FPGA:: Field-programmable gate array link:pass:[https://en.wikipedia.org/wiki/Field-programmable_gate_array][🔗^]
[[HLS]]HLS:: High-level synthesis link:pass:[https://en.wikipedia.org/wiki/High-level_synthesis][🔗^]
[[HDL]]HDL:: Hardware description language link:pass:[https://en.wikipedia.org/wiki/Hardware_description_language][🔗^]
[[ADL]]ADL:: Accelerator design language link:pass:[https://www.sigarch.org/hdl-to-adl/][🔗^]
[[GPU]]GPU:: Graphics processing unit link:pass:[https://en.wikipedia.org/wiki/Graphics_processing_unit][🔗^]
[[LLVM_IR]]LLVM IR:: LLVM intermediate representation link:pass:[https://en.wikipedia.org/wiki/LLVM#Intermediate_representation][🔗^]
[[RTL]]RTL:: Register-transfer level link:pass:[https://en.wikipedia.org/wiki/Register-transfer_level][🔗^]
[[DUT]]DUT:: Design/Device under test link:pass:[https://en.wikipedia.org/wiki/Test_bench][🔗^]
[[ASIC]]ASIC:: Application specific integrated circuit
[[QoR]]QoR:: Quality of results
[[CPU]]CPU:: Central processing unit
[[LUT]]LUT:: Look-up table
[[FF]]FF:: Flip-Flop
[[DFF]]DFF:: D Flip-Flop
[[BRAM]]BRAM:: Block RAM
[[DSP]]DSP:: Digital signal processor 
[[CLB]]CLB:: Configurable logic block
[[LB]]LB:: Logic block
[[LE]]LE:: Logic element
[[RAII]]RAII:: Resource Acquisition is Initialization / Scope-Bound Resource Management
[[HIR]]HIR:: High-Level Intermediate Representation link:pass:[https://rustc-dev-guide.rust-lang.org/hir.html][🔗^]
[[THIR]]THIR:: Typed HIR link:pass:[https://rustc-dev-guide.rust-lang.org/thir.html][🔗^]
[[MIR]]MIR:: Mid-level Intermediate Representation link:pass:[https://rustc-dev-guide.rust-lang.org/mir/index.html][🔗^]
[[PAL]]PAL:: Programmable array logic


[bibliography]
== References

// Claims to have a transpiler from a subset of Rust (RAR) to restriceted algrithmic C (RAC) that can be synthesized to FPGA. No source.
// The first paper to mention HLS from Rust. 
* [[[Har22]]]
Hardin, David,
_Hardware/Software Co-Assurance using the Rust Programming Language and ACL2_,
arXiv preprint arXiv:2205.11709,
2022.
link:pass:[https://arxiv.org/abs/2205.11709v1][🔗^]

* [[[Rog20]]]
Rogers, Samuel and Slycord, Joshua and Baharani, Mohammadreza and Tabkhi, Hamed,
_gem5-SALAM: A System Architecture for LLVM-based Accelerator Modeling_,
2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 471-482,
2020.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9251937][🔗^]

* [[[Li21]]]
Li, Rui and Berkley, Lincoln and Yang, Yihang and Manohar, Rajit,
_Fluid: An Asynchronous High-level Synthesis Tool for Complex Program Structures_,
2021 27th IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC), 1-8,
2020.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9565447][🔗^]

* [[[Lia23]]]
Liang, Geng-Ming and Yuan, Chuan-Yue and Yuan, Meng-Shiun and Chen, Tai-Liang and Chen, Kuan-Hsun and Lee, Jenq-Kuen,
_The Support of MLIR HLS Adaptor for LLVM IR_,
Workshop Proceedings of the 51st International Conference on Parallel Processing, 1-8,
2020.
link:pass:[https://doi.org/10.1145/3547276.3548515][🔗^]

// Bambu provides a research environment to experiment with new ideas across HLS, high-level verification and debugging.
// Bambu input: standart C/c++ specifications, LLVM IR, IRs from GCC
// Includes many optimizations
// Makes it easy to integrate new transformations and optimizations
// Is open-source
// Bambu is a command line tool
// Supports most C/C++ constructs
// Bambu has three phases. frontend, middleend and backend
// Frontend: Uses clang or gcc
// Uses a compiler plugin for both to extract the call graph and control flow information
// Builds its own static single assignment IR
// This decouples the compiler frontend from the rest of the HLS process.
// Vivado HLS has a frontend based on clang
// Middle end:
// Bambu rebuild the call graph and control data flow graph and adds own data structures.
// Applies a set of analyses and transformations.
// Including common software compiliation optimizations
// Including target specific transformations. Such as replacing multiplication and divisions with constants with shift and add operations.
// Can exploit custom sized operators
// TODO: Explore if bambu can use the rust crate for more integer sizes like u21
// Bambu performs bitwidth and range analysis to minimize bit width
// Backend: Bambu performs the actual architectural synthesis here
// The synthesis process acts on every function individually
// Every function has at least two parts: control ogic and data path
// Control logic is a FSM
// Control logic handles the routing of data values and temporal execution of ops
// Bambu steps:
// * Function allocation
// Bambu has a technology library for standart system libraries such as libm or libs
// This step associates High level functions with hardware resources
// Bambu supports sharing functions across module boundries
// * Memory allocation
// Defines momories to store variables.
// Defines how dynamic memory is implemented
// Memories in bambu can be classified as read-only, local, with aligned or unaligned memory access
// Bambu supports accessing protocol based memories
// * Resource allocation
// Maps operations (not mapped onto functions) onto resource units
// Resource units are available in the bambus resource library
// Floating point operations are supported through generating soft floating point cores
// Rich resource library with multiple implementations for the same operation
// Resource library annotated with latency and resource occupation.
// * Scheduling
// Bambu uses List scheduling
// every operation has a priority
// a operation is ready when its deps have been satisfied
// ready operations can be scheduled if the resources are availabe
// Multiple compenting for a resource: higher priority
// Also has a speculative scheudling algo
// * binding
// Pretty much standart
// Bambu considers how profitable it is for two operations to share the same resource
// Resources that occupy a big area are more likely to be shared
// * netlist generation
// Translates the architecture in a RTL description
// In Verilog or VHDL
//
// Research topics for bambu: "They range from parallelized hardware accelerator design, dynamic scheduling, verification and de- bugging, design exploration of the compilation flow, machine learning accelerator design, IR development, and integration with logic synthesis tools" , MLIR
// MLIR dialects that can be translated to LLVM IR
* [[[Fer21]]]
+F. Ferrandi et al.+,
_Invited: Bambu: an Open-Source Research Framework for the High-Level Synthesis of Complex Applications_,
2021 58th ACM/IEEE Design Automation Conference (DAC), 1327-1330,
2021.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9586110][🔗^]
link:pass:[https://re.public.polimi.it/retrieve/668507/dac21_bambu.pdf][📁^]

* [[[Rot10]]]
+Nadav Rotem,+
_C-to-Verilog. com: High-Level Synthesis Using LLVM_,
University of Haifa,
2010.
link:pass:[https://llvm.org/devmtg/2010-11/Rotem-CToVerilog.pdf][🔗^]

* [[[Sch20]]]
Fabian Schuiki, Andreas Kurth, Tobias Grosser, and Luca Benini,
_LLHD: a multi-level intermediate representation for hardware description languages_,
In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2020), 258-271,
2020.
link:pass:[https://doi.org/10.1145/3385412.3386024][🔗^]

// Multiple HLS tools use LLVM
// C/Cpp are most popular languages for HLS
// NOTE: I focused on FPGA descriptions
// Clock frequency scaling in CPU stalled around 2005
// A alternative approach for high-throughput and energy efficient processing is to use specific accelerators
// Specialized accelerators are hard to design and program
// RTL requires advanced hardware expertise
// RTL specifies cycle-by-cycle behavior explicitly
// RTL is a low-level abstraction
// RTL leads to longer development times
// FPGAs with HLS can reduce that.
// FPGAs are configurable integrated circuits
// Most FPGAs are reconfigurable
// FPGA vendors provide toolchains to synthesize HTL to bitstream
// bitstream gets programmed to the FPGA
// HLS tools start from a HLL and automatically produce a circuit specification in RTL
// HLS offers enable software engineers to benefit from the performance and energy efficiency of hardware without having hardware expertise
// HLS tools enable hardware engineers to design systems faster
// HLS tools enable hardware engineers to rapidly explore the desing space
// Microsoft uses FPGAs for accelerating bing search
//
// 
* [[[Nan16]]]
+R. Nane et al.+,
_A Survey and Evaluation of FPGA High-Level Synthesis Tools_,
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 1591-1604,
2016.
link:pass:[https://ieeexplore.ieee.org/abstract/document/7368920][🔗^]
link:pass:[https://sci-hub.st/10.1109/tcad.2015.2513673][📁^]

* [[[Nor18]]]
+D. H. Noronha, B. Salehpour and S. J. E. Wilton+,
_LeFlow: Enabling Flexible FPGA High-Level Synthesis of Tensorflow Deep Neural Networks_,
Fifth International Workshop on FPGAs for Software Programmers, 1-8,
2018.
link:pass:[https://ieeexplore.ieee.org/abstract/document/8470462][🔗^]

* [[[Soz22]]]
Sozzo, Emanuele Del, et al.,
_Pushing the level of abstraction of digital system design: A survey on how to program FPGAs_,
ACM Computing Surveys, 1-48,
2022.
link:pass:[https://dl.acm.org/doi/abs/10.1145/3532989][🔗^]

* [[[XLS]]]
_XLS project page_
link:pass:[https://google.github.io/xls/][🔗^]

* [[[DSLX]]]
_DSLX Reference_
link:pass:[https://google.github.io/xls/dslx_reference/][🔗^]

* [[[rusthdl]]]
_rust-hdl project overview_
link:pass:[https://github.com/samitbasu/rust-hdl][🔗^]


* [[[Zen12]]]
_Identifying Barriers to Adoption for Rust through Online Discourse_
link:pass:[https://arxiv.org/pdf/1901.01001.pdf][🔗^]

* [[[so-trends]]]
_Stack Overflow Trends_
https://insights.stackoverflow.com/trends?tags=rust%2Cc%2B%2B

// Rust has an ecosystem that greatly simplifies any software projec
// Rust is great
// Rust has been the "most loved" language since 2016
// Rust is meant to supersede C/C++
// Rust focus is on saftey and performance
// Libraries exist for any need you may have
// Dependencies can be installed using the official cargo tool
// Rust is the first industry-supported computer programming language to overcome the longstanding trade-off between the control over resource management provided by lower-level languages for systems programming, and the safety guarantees of higher-level languages
// Rust enables many common systems programming pitfalls to be detected at compiletime
// Rust surpasses all other common memory-safe languages in terms of performance
// Rust has data-race prevention
// Considering performance Rust is one of the best languages
// Considering saftey Rust is the best language
// Rust offers many modern features that the more established systems languages tend to lack.
// Cargo is the package manager for Rust
// Cargo is the build system for Rust
// Cargo facilitates downloading and building dependencies
// Cargo facilitates unit testing and integration testing
// Cargo facilitates benchmarking
// Cargo facilitates build management with different profiles
// Cargo facilitates documentation generation from comments
// Rustfmt facilitates code formatting
// Dependency management is handled with a configuration file
// Dependencies are automatically installed during compilation
// Dependencies can be easliy found on the official community crates registry
// Cargo allows to view unified documentation for all dependencies
// Unit tests are written in the same file as the code they test
// Benchmarking is done in a similar fashion to unit testing
// The tooling alone makes Rust a much better development experience than most systems languages
// The tooling is most likely a considerable contributor to its rise.
// Rust is approaching the status of a mainstream language in health informatics applications
// The criticisms of Rust tend to originate from its lack of maturity
// C and C++ are well-adopted and much more established in the industry than Rust
// lack of demand for Rust developers in the market
* [[[Bug22]]]
+William Bugden, Ayman Alahmar+,
_Rust: The Programming Language for Safety and Performance_,
asXiv,
2022.
link:pass:[https://arxiv.org/pdf/2206.05503.pdf][🔗^]

// Rust can be used for GPU programming
* [[[Byc22]]]
+Andrey Bychkov, Vsevolod Nikolskiy+,
_Rust Language for GPU Programming_,
In: Voevodin, V., Sobolev, S., Yakobovskiy, M., Shagaliev, R. (eds) Supercomputing. RuSCDays 2022. Lecture Notes in Computer Science, vol 13708. Springer, Cham, 2022, pp. 522-32,
2022
link:pass:[https://doi.org/10.1007/978-3-031-22941-1_38][🔗^]

// Rust can be used for web programming
* [[[Kyr22]]]
+Kyriakou K-ID, Tselikas ND+,
_Complementing JavaScript in High-Performance Node.js and Web Applications with Rust and WebAssembly._,
Electronics 11, no. 19: 3217,
2022
link:pass:[https://doi.org/10.3390/electronics11193217][🔗^]

// Probably one of the greatest features of the language is the package manager, called cargo.
// Rust is a high-level language
// Rust is very efficient in terms of performance
// Rust is based on the principle of zero cost abstractions
// Rust provides a memory safety mechanism without using a garbage collector called the borrow checker
// Rust is a strongly typed language
// Rust is provides out of the box a package manager used for importing dependencies, building and distribution of a project.
// If a variable is declared in a specific context it will be freed when the context is over.
// The ownership of a variable can be passed to another context
// More basic description of Rust ownership stuff, will skip that for now
// Development in {cpp} on a production level requires the use of additional tools such as CMake, Make, etc. This adds a layer of complexity.
// Rust has mandatory tooling for building, distributing and depending on a project
// In Rust only a manifest file is needed to configure the project for any scenario possible
// Rust can be compiled to webassembly
// Rust is more energy efficient than any other language expect C for IoT applications
// Rust is faster than any other language expect C for IoT applications
// Rust can easily integrate with C or C++ code
// Rust solves the problem of memory safety without using a garbage collector
// Microsoft states that 70% of security flaws discovered in their systems are related to memory safety
* [[[Cos19]]]
+Cosmin Cartas+,
_Rust - The Programming Language for Every Industry_,
ECONOMY INFORMATICS JOURNAL, 19, 45-51,
2019
link:pass:[https://doi.org/10.12948/ei2019.01.05][🔗^]

// state-of-art bottom-up logic programming within the Rust ecosystem
* [[[Sah22]]]
+Arash Sahebolamri, Thomas Gilray, Kristopher Micinski+,
_Seamless Deductive Inference via Macros_,
Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction, 77-88,
2022
link:pass:[https://doi.org/10.1145/3497776.3517779][🔗^]

// Productivity in HLS is better than HDL
// HLS offers easier design and testing
// HDL implementation is better than HLS
* [[[Mil20]]]
+Roberto Millón, Emmanuel Frati, Enzo Rucci+,
_A Comparative Study between HLS and HDL on SoC for Image Processing Applications_,
Revista elektron, Vol. 4, No. 2, 100-106,
2020
link:pass:[https://doi.org/10.37537/rev.elektron.4.2.117.2020][🔗^]
http://elektron.fi.uba.ar/index.php/elektron/article/view/117/219[📁^]

// Describing the traditional HDL design flow (in 1996)
// TODO: Find newer source
* [[[Smi96]]]
+Douglas J. Smith+,
_VHDL & Verilog compared & contrasted—plus modeled example written in VHDL, Verilog and C._,
In Proceedings of the 33rd annual Design Automation Conference, pp. 771-776,
1996
link:pass:[https://dl.acm.org/doi/pdf/10.1145/240518.240664][🔗^]

// 
* [[[Fla20]]]
+Peter Flake, Phil Moorby, Steve Golson, Arturo Salz, and Simon J. Davidmann+,
_Verilog HDL and its ancestors and descendants._,
Proc. ACM Program. Lang. 4, no. HOPL (2020): 87-1,
2020
link:pass:[https://www.researchgate.net/profile/Arturo-Salz-2/publication/342137214_Verilog_HDL_and_its_ancestors_and_descendants/links/613fc7b45d9d0e131b427dbb/Verilog-HDL-and-its-ancestors-and-descendants.pdf][🔗^]

* [[[intel-hls]]]
_Intel® High Level Synthesis Compiler_
https://www.intel.de/content/www/de/de/software/programmable/quartus-prime/hls-compiler.html

* [[[hdl-to-adl]]]
_From Hardware Description Languages to Accelerator Design Languages_
https://www.sigarch.org/hdl-to-adl/


// Survey literature from 2010 to 2016
// Probably the best comparison of HLS and RTL
// ALso the newest
// Shows that the quality of results of RTL is better than that of HLS
// Shows that development time with HLS is a third of that the RTL flow
// Shows that the productivity of a designer is over four times higher with HLS than with RTL
// Vivado HLS is the most common HLS tool. At least it is used significantly more than any other HLS tool in papers.
// Xilinx is the leading FPGA vendor
// FPGAs are made of configurable logic blocks (CLB, different vendors, different names).
// The CLBs are connected with programmable interconnects.
// The CLBs consist of a few logic cells, logic elements, or adaptive logic modules (ALM) (LC, LE and ALM are the same. Different vendors, different names).
// Logic cells are made of a comination of programmable look-up tables (LUTs) and flip-flops (FFs).
// FPGAs can also have other resources, but these are vendor specific. Most commonly DSP blocks and BRAM blocks.
// There are 4 performance metrics that are commonly used to compare HLS and RTL: performance, execution time, latency, maximum frequency
// For project bigger than 250 lines of code HLS also needs fewer lines of code than RTL
// Reduction in development time for HLS seems independet of project size.
// On average HLS uses 41% more basic FPGA resources than RTL
// The usage of advanced FPGA resources of HLS is similar to RTL
// C based languages are the most common, then OpenCL based, then high-level language based.
// CUDA/OpenCL based HLS is especially resource consuming and has the worst performance
// The performance of HLS designs is similar to the performance of RTL designs.
// THe only example in academia where the development time of HLS was more than RTL was when the developer had to learn the HLS tool in the process.
// Only looks at small to medium designs, 50-500 lines of code
// It is easier to adopt HLS than RTL for people who have experience in software design
// HLS allows for efficient behavioral verification
// The HLS output must still be verified for non-behavioral aspects. This traditional verification is difficult, because there is no direct relationship to the source code.
// HLS halves verification time in many cases
// HLS is a particularly good choice when time to market is a dominant issue and there is no compelling need to gain the ultimate performance or smallest resource usage for the product
// There is no standart example to compare HLS and RTL
* [[[Lah19]]]
+Sakari Lahti, Panu Sjövall, Jarno Vanne, Timo D. Hämäläinen+,
_Are We There Yet? A Study on the State of High-Level Synthesis_,
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 38, no. 5, pp. 898-911,
2019
link:pass:[https://doi.org/10.1109/TCAD.2018.2834439][🔗^]
link:pass:[https://sci-hub.st/10.1109/tcad.2018.2834439][📁^]


// Studied Rust’s ownership discipline in the presence of unsafe code.
// Shows that various important Rust libraries with unsafe implementations, many of them involving interior mutability, are safely encapsulated by their type
// NOTE: Did only read abstract and conclusion
* [[[Jun17]]]
+Ralf Jung, Jacques-Henri Jourdan, Robbert Krebbers, Derek Dreyer+,
_RustBelt: Securing the Foundations of the Rust Programming Language_,
Proc. ACM Program. Lang. 2, POPL, Article 66 (January 2018), 34 pages.,
2017
link:pass:[https://doi.org/10.1145/3158154][🔗^]

// Cpp uses RAII
// "In particular, a programmer can choose to write a low-level-C style and/or violate every rule of good programming. That is not my topic here."
* [[[Str12]]]
+Bjarne Stroustrup+,
_Foundations of C++_,
Programming Languages and Systems. ESOP. Springer, pp. 1-25,
2012
link:pass:[https://doi.org/10.1007/978-3-642-28869-2_1][🔗^]

* [[[Kla23]]]
+Steve Klabnik, Carol Nichols+,
_The Rust programming language_,
No Starch Press,
2023
link:pass:[https://doc.rust-lang.org/book/foreword.html][🔗^]

// Rust compiler has multiple intermediate representations (IRs)
// * MIR (Mid-level IR)
// * HIR (High-level IR)
// * THIR (Typed HIR)
// * LLVM IR
// Typechecking happens on HIR
// Optimization happens on MIR
// MIR is a typed SSA
// Borrowchecking happens at MIR level
// Optimizations also happen in LLVM
// LLVM is used as the backend
// LLVM can generate machine code for many architectures
// LLVM is a collection of modular and reusable compiler and toolchain technologies
// LLVM contains a pluggable compiler backend used by rustc and clang
// Clang is a C compiler
// LLVM takes LLVM IR
// Rust compiler uses LLVM because
// * They don't have to write their own backend. Reduces implementation and maintenance effort.
// * Benefit from the large suite of advanced optimizations that LLVM provides
// * Rust can be compiled to any of the platforms that LLVM supports.
// * Community benfits. Things like spectre and meltdown only need to be fixed in LLVM and many compilers benfits from that
// rustc groups LLVM IR into "modules" known as codegen units
// Rustc can use LLVM to codegen multiple of these modules in parallel utilizing multiple CPU cores
// The resulting object files are then linked together by the linker
* [[[rustc-guide]]]
_Rust Compiler Development Guide_,
2023
link:pass:[https://rustc-dev-guide.rust-lang.org/backend/codegen.html][🔗^]
link:pass:[https://rustc-dev-guide.rust-lang.org/mir/optimizations.html][🔗^]

// LLVM is a compiler framework
// Defines a a low-level code representation in static single assignment (SSA) form
// LLVM IR
// Describes a program using an abstract RISC-like instruction set with higher level information
// LLVM IR contains type information
// LLVM IR contains explicit control flow graphs
// LLVM IR contains explicit dataflow representation (using SSA)
// Has a low-level, language independent type system
// Has instructions for performing type conversions and low-level address arithmetic while preserving type information.
// Low-level exception handling instructions
// LLVM is not intended to be a universal compiler IR
// does not represent high-level language features directly
// LLVM has no notion of highlevel constructs such as classes, inheritance, or exceptionhandling semantics
// LLVM does not specify a runtime system or particular object model
// "Type information captured by LLVM is enough to safely perform a number of aggressive transformations that would traditionally be attempted only on type-safe languages in source-level compilers"
// NOTE: I skipped section 2
// "The goal of the LLVM compiler framework is to enable sophisticated transformations at link-time, install-time, runtime, and idle-time, by operating on the LLVM representation of a program at all stages"
// static compiler front-ends emit code in the LLVM representation
// combined by the LLVM linker
// Linker performs a variety of link time optimizations
// The resulting code is then translated to native code for a given target.
// Language specific optimizations must be performed in the frontend
// External static LLVM compilers are known as front-ends
// Frontends translate source language programs into LLVM IR
// Can perform aggressive interprocedural optimizations across the entire program
// Some of the interprocedural optimizations are:  inlining, dead global elimination, dead argument elimination, dead type elimination, constant propagation, array bounds check elimination, simple structure field reordering, and Automatic Pool Allocation
// Uses code generator backends to translate LLVM IR into native code for a given target
* [[[Lat04]]]
+C. Lattner, V. Adve+,
_LLVM: a compilation framework for lifelong program analysis & transformation_,
International Symposium on Code Generation and Optimization, 2004. CGO 2004., San Jose, CA, USA, 2004, pp. 75-86
link:pass:[https://doi.org/10.1109/CGO.2004.1281665][🔗^]
link:pass:[https://sci-hub.st/10.1109/cgo.2004.1281665][📁^]
// Shows that HLS is twice as fast as HDL 
// M. Pelcat, C. Bourrasset, L. Maggiani and F. Berry, "Design productivity of a high level synthesis compiler versus HDL," 2016 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS), Agios Konstantinos, Greece, 2016, pp. 140-147, doi: 10.1109/SAMOS.2016.7818341.
// https://ieeexplore.ieee.org/abstract/document/7818341

// FPGA inception 30 years ago
// FPGAs bring faster design cycles then custom chips
// FPGAs lower dev cost then custom chips
// low-level hardware recnfigurability
// FPGA architecutre offers many design choices
// FPGAs consist of different types of programmable blocks
// "FPGAs are recofigurable computer chips that can be programmed to implement any digital circuit"
// prefabricated routing tracks with programmable switches
// Functionality of all FPGA blocks is controlled by SRAM cells
// Milloions of SRAM cells
// HDL is converted to bitstream
// Bitstream is used to programm all configuration SRAM cells
// Lower NRE cost then ASICs
// Shorter time to market then ASICs
// off the shelf FPGA can be used to implement design in a matter of weeks
// Skipping phisical design, layout, fabrication and verification
// Allow continous hardware upgrades by loading new bitstreams in the field
// Considered a compelling solution for small and medium sized designs 
// Exact hardware for every application
// Exact datapath width, pipeline stages, parallel units as required
// Can achieve higher efficiency than CPU or GPU
// Can implement instruction-free streaming hardware
// Can implement a custom instruction set
// Adopted in many domains.
// "adoption of FPGAs in many application domains including wireless communi- cations, embedded signal processing, networking, ASIC prototyping, high-frequency trading, and many more"
// Deployed on large scale in datacenters, packet processing, machine learning
// Lower efficiency than ASICs
// FPGA on average 35 times larger than ASIC implementation
// FPGA on average 4 times slower than ASIC implementation
// For designs that utilize other FPGA blocks the gap is smaller, still 9 times large
// FPGA architects seek to reduce gap while maintaining programmability
// Early FPGAs ere simple arrays of logic blocks
// Modern FPGAs are complex heterogeneous architectures that have more block types
// Modern FPGA have blocks like BRAM, DSP, processors, external interfaces
// FPGA architectures are evaluated based on the efficiency implementing a wide variety of designs
// There are academic test suites for evaluating FPGA architectures
// VTR is a CAD system for layouting designs on FPGAs
// CAD system applies a series of complex optimizations 
// CAD system converts RTL design to netlist.
// CAD system maps netlist to FPGA blocks
// CAD system places blocks on FPGA and routes the connections between them
// CAD system outputs bitstream implementation
// total area is a key metric
// "Total area is the sum of the areas of the FPGA blocks used by the application, along with the programmable routing included with them."
// Timing analyzer finds the critical path through blocks and routing
// Critical path limits maximum clock frequency
// Power consumption is estimated based on resources used and signal toggle rate
// _hardened_ blocks are blocks that are implemented as ASICs
// What functionality to harden is a design choice
// What area of the FPGA to use for hardened blocks is a design choice
// Hardened blocks can still have some level of configurability
// How flexible the hardened blocks are is a design choice
// Hardened blocks are faster, smaller and more power efficient than programmable blocks
// Tradeoff between flexibility and efficiency
// Unused hardened blocks are wasted silicon
// Problems with slow routing to hardened blocks, if they are far away
// PAL first reconfigurable computing devices
// PAL does not scale well, area increases quadratically with IO size
// CPLD includes mulitple PALs and programmable routing in a package
// 1984 Xilinx pioneers first LUT based FPGA
// SRAM based LUTs with interconnects between them
// Scales well
// Much higher area efficiency than and/or based designs
// LUTs form the fundamental logic element in all commercial FPGAs
// Alternative designs perform worse than LUTs
// K-LUT implements a K-input LUT
// K-LUT stores truth table in SRAM cells,
// K input signals are used as multiplexer to select line
// truth table contains 2^K values
// A basic logic element (BLE) is a K-LUT with an output register
// A BLE can implement DFF or a K-LUT
// A BLE has K inputs and 2 outputs, one for routing and one for feedback inside the LE
// Logic blocks are composed of multiple (N) BLEs
// Logic blocks have a local interconnect
// The local interconnect connects the inputs of the LB and the feedback outputs of the BLEs to the inputs of the BLEs.
// The local interconnect is often a arranged as a local full or partial crossbar.
// See figure 4 in the paper.
// Over time K and N have increased.
// More K means more functionality in a single LUT
// More K leads to less logic in the critical path
// More N means less demand for fast inter-LB routing
// The area of the LUT increases exponentially with K as more SRAM cells are needed (2^K)
// More K linearly degrades the speed of the LUT
// if the local interconnect is a crossbbar, its size increases quadratically with N
// if the local interconnect is a crossbbar, its size decreases linearly with N
// Empirically the best size for K is 4-6 and for N it is 3-10
// First LUT-based FPGA from Xilinx: N = 2, K = 3
// around 2000: 4-LUTs common
// Study: 4-LUTs vs 6-LUTS: 6-LUTS:14% more perf, 17% bigger
// Fracturable LUTs can be broken down into smaller LUTs, but limitations like shared inputs
// FPGA architectures from Xilinx and Altera converge to relatively large LBs wth 8 and 10 N
// Future designs even bigger LBs
// inter-LB wire delay scales poorly with process shrink
// Larger LB sizes can lead to faster CAD tool runtimes
// Modern FPGAs have more than 1 FFs per BLE
// Even though there are optimization the core ideas stayed similiar
// 22% of logic elements in FPGAs are implementing arithmetic
// These operations can be implemented with LUTs but are inefficient
// A ripple carry adder requires 2 * the number of bits LUTs
// This leads to high logic utilization and long critical paths
// All modern FPGAs include hardened arithmetic circuitry in their logic blocks
// How the arithmetic is accelerated is a design choice
// Can be a dedicated adder between two LUTs
// Can be just a fastpath for the carry bit
// At least 3x faster than LUT based implementations
// NOTE: there is more detail on the different types of arithmetic optimizations in the paper
// Recently deeplearning has become a key workload
// Deeplearning has multiply-accumulate operations at its core, which could benefit from hardened bigger hardened arithmetic
// Programmabble routing is over 50% of the area of an FPGA
// Programmable routing accounts for over half the critical path delay
// High multiplier density in signal processing and communication applications
// Main design philosophy of DSP block is to minimize the number of soft logic used to implement common DSP algorithms
// FPGA CAD tools will automatically map multiplication to DSP blocks
// Bigger FPGA designs always always require a memory buffer
// Making soft memory out of LUTs is over 100x less dense than SRAM cells
// Modern FPGAs are about 25% BRAM
* [[[Bot21]]]
+Andrew Boutros, Betz Vaughn+,
_FPGA architecture: Principles and progression_,
IEEE Circuits and Systems Magazine 21.2 (2021): 4-29.,
2021
link:pass:[https://doi.org/10.1109/MCAS.2021.3071607][🔗^]
link:pass:[https://sci-hub.st/10.1109/MCAS.2021.3071607][📁^]

// TODO: This is an application note, how to cite?
// TODO: Especially Cri in the link is wrong
// TODO: Source for one definition. Necessary?
// A critical path is a path in the design which must meet certain critical timing requirements in order for the system to function properly
* [[[Cri95]]]
_Critical path analysis for field-programmable gate arrays_,
Microprocessors and Microsystems, Volume 19, Issue 7, Pages 435-439,
1995
link:pass:[https://doi.org/10.1016/0141-9331(95)90010-1][🔗^]
link:pass:[https://sci-hub.st/10.1016/0141-9331(95)90010-1][📁^]

// Compares RTL/HDL to assembly
// High level languages improved productivity
// HDL have enabled wide adoption of simulation tools
// First HLS tools 1990s
// In the 2000s: shift to electronic system level (ESL) paradigm that facilitates exploration sythesis and verification of complex SoCs
// Intro of first languages with system-level abstraction like SystemC or SystemVerilog
// 2000s transaction level modeling
// ESL paradigm shift caused by rising system complexities
// HLS reduced time for creating hardware
// HLS reduced time for verification
// HLS enables the reuse of the same specification for different targets (ASICs, FPGAs, different ASICS and FPGAs)
// functional specification = untimed highlevel description
// NOTE: contains more info about HLS design flow
// HLS tools transform an untimed specification into a fully timed implementation
// HLS tools generate custom architecture to efficiently implement the specification
// HLS tools generate a RTL implementation
// DIAGRAM: Highlevel synthesis design steps
// Generated architecture (usually) consists of a datapath and a controller.
// Generated architecture also has memorybanks  and communications interfactes.
// HLS tools usually perform 7 tasks:
// 1. Compiling the specification
// 2. Allocating/Creating hardware resources
// 3. Scheduling operations to clock cycles
// 4. Binding operations to functional units
// 5. Binding variables to storage elements
// 6. Binding transfers to connection units
// 7. Generating the RTL architecture
// Steps 2-6 are called interdependent
// Compiling transforms the specification into a formal description
// Compiling performs optimizations
// Formal model clasically exhibits data and control dependencies.
// Data flow graph: Every operation is a node and the edges are values (input, temporary and output)
// A pure data flow graph (DFG) models data flow only
// In some cases a pure DFG can be created. Can be done by completly unrolling loops and multiplexing conditional assignments.
// Pure DFG is big and impractical
// Cannot support unbounded iteration and nonstatic control flow (like goto)
// control and data flow graph (CDFG) models data and control flow
// CDFG nodes are called basic blocks and are a straight sequence of statements
// CDFG edges can be conditional and represent if or switch constructs.
// CDFGs are more expressive ecause the can represent loops with unbounded iteration (those that cannot be unrolled)
// Allocation defines the types and number of resources the are needed to satisfy the design constraints
// Resources are functional units, storage elements and communication interfaces
// HLS tools have a RTL component library with basic resources.
// Scheduling determines which operations run in which clock cycle.
// All operations must be scheduled into cycles
// If there are no data dependencies between operations they can be scheduled in parallel
// Every variable that carries values over multiple cycles must be bound to a storage element
// Variables with nonoverlapping lifetimes can be bound to the same storage element
// Every operation must be bound to a functional unit that can perform the operation
// Every connection between functional units and storage element must be bound to a connection unit
// After allocation, scheduling and binding the RTL architecture can be generated and output
// The architecture is classically includes a controller and a data path
// storeage elements: registers, memories, ...
// functional units: ALUs, multipliers, DSPs, other custom functions, ...
// connection units: buses, tristate drivers, multiplexers, ...
// The data path consists of the the storage elements, functional units and connection units
// All these components can be connected arbitrarily through buses
// They can also be pipelined
// The controller is a finite state machine (FSM)
// The controller orchestrates the data path by setting values of control signals of the data path
// The inputs of the controller can come from primary inputs or from the data path
// The controller consists of three parts: the next state logic, a state register and the output logic
// The next state logic computes the next state of the FSM from the current state and the inputs
// The state register contains the current state of the controller
// The output logic sets the control signals according to the current state
// The output logic also sets control outputs that can be used as inputs for the data path
// The controller is usually build with hardwired logic, but can be more complex with memories and such
// If the controller is more complex it is usually a custom processor.
// The state register is then call program counter. This shows that it is just a processor.
// NOTE: Only read until "Several design flows"
* [[[Cou09]]]
+Philippe Coussy, Daniel D. Gajski, Michael Meredith, Andres Takach+,
_An Introduction to High-Level Synthesis_,
IEEE Design & Test of Computers, Volume 26, Issue 4, Pages 8-17,
2009
link:pass:[https://doi.org/10.1109/MDT.2009.69][🔗^]
link:pass:[https://sci-hub.st/10.1109/MDT.2009.69][📁^]

// Reference thesis:
// * https://webthesis.biblio.polito.it/7573/1/tesi.pdf
// * https://scholarworks.gvsu.edu/cgi/viewcontent.cgi?article=1754&context=theses

include::styles/trailing-scripts.adoc[]


// Final checklist:
// * are all abbreviations defined?
// * are all abbreviations linked to wikipedia (or somewhere else)?
// * are all references used?
// * are all references linked to the correct source?
// * are all TODOs processed?
// * are the product names consistent? (BambU)
// * check for duplicate references
// * check for broken references
// * archive.org all links
// * Check for duplication of information
// * oxford comma

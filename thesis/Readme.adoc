:doctype: book
:last-update-label!:
:imagesdir: images
:source-highlighter: highlight.js
:rouge-style: github
// Available themes: https://highlightjs.org/static/demo/
// :highlightjs-theme: ../../11.8.0/styles/dark
:highlightjs-theme: thesis
:highlightjsdir: highlightjs
:toclevels: 2
:stem:
:toc: macro
:sectanchors:
:notitle:
:title-page: false
:stylesheet: Readme.css
:toclevels: 3
:kroki-server-url: http://localhost:8000
:listing-caption: Listing
:kroki-fetch-diagram: true
:kroki-default-options: inline
:xrefstyle: short
ifdef::env-web-pdf[]
:docinfo: shared-footer
endif::env-web-pdf[]

image::logo_hda.svg[role=logo]

[.university.text-center]
Darmstadt University of Applied Sciences

[.faculty.text-center]
Faculty of Computer Science

[discrete#main-title]
= Is High-level synthesis from Rust possible using existing tools?

[.description.text-center]
Submitted in partial fulfilment of the requirements for the degree of +
Bachelor of Science (B.Sc.)

[.presented-by.text-center]
by +
*Lennart Eichhorn* +
[small]+Matriculation number: 759253+ +


[.other-people]
First Examiner:: Prof. Dr. Stefan Rapp
Second Examiner:: Prof. Dr. Ronald Charles Moore

<<<

CAUTION: Make sure that the conclusion does not introduce new things.

CAUTION: What is missing? What would you like to see?

CAUTION: Weird expressions.

CAUTION: Weird order of chapters, chapternames?

CAUTION: The thing that is used as a source for HLS is a specification. Use that word more.

<<<

[discrete]
== Erklärung

Ich versichere hiermit, dass ich die vorliegende Arbeit selbstständig verfasst
und keine anderen als die im Literaturverzeichnis angegebenen Quellen benutzt habe.

Alle Stellen, die wörtlich oder sinngemäß aus veröffentlichten oder noch
nicht veröffentlichten Quellen entnommen sind, sind als solche kenntlich
gemacht.

Die Zeichnungen oder Abbildungen in dieser Arbeit sind von mir selbst
erstellt worden oder mit einem entsprechenden Quellennachweis versehen.

Diese Arbeit ist in gleicher oder ähnlicher Form noch bei keiner anderen
Prüfungsbehörde eingereicht worden.

_Darmstadt, 5. Juli 2023_

[.signature-required]
Lennart Eichhorn

<<<

[discrete]
== Abstract

// A short summary of the contents in English of about one page. The following
// points should be addressed in particular:

// * Motivation: Why did this work come about? Why is the topic of the
// work interesting (for the general public)? The motivation should be
// abstracted as far as possible from the specific tasks that may be given
// by a company.
// * Content: What is the content of this thesis? What exactly is covered in
// the thesis? The methodology and working method should be briefly
// discussed here.
// * Results: What are the results of this work? A brief overview of the
// most important results as a teaser to read the complete thesis.

// Motivation
The aim of this thesis is to explore whether it is possible to use Rust as a source language for high-level synthesis. PandA bambu is an open-source research framework for high-level synthesis. It primarily supports C or {cpp} as input languages via Clang or GCC as frontends. The Clang frontend makes it possible to process programs in LLVMs intermediate representation (LLVM IR). Rust is a popular modern systems programming language that uses LLVM as a backend for code generation. Because of this it is able to output LLVM IR. This should enable us to use Rust as a source-language for HLS. 

// Content
This thesis investigates how a toolchain that performs HLS from Rust with bambu would look like,
which restrictions apply on the Rust code that is used to generate LLVM IR for HLS,
how the resulting designs compare to similar designs generated from C or {cpp} code,
whether the rust ecosystem can be used, and
how to integrate the toolchain with the rust-hdl project.

//Results
This evaluation shows that it is possible to use Rust as a source language for high-level synthesis. A library for using HLS in rust-hdl, a Rust based HDL, was developed and used to generate designs from different algorithms. When comparing them to designs generated from equivalent {cpp} code they were similar in terms of size and performance. Most of Rust is supported for HLS with bambu, with the biggest restriction being that the specification is not allowed to able to panic. The rust ecosystem can be used and the library supports synthesizing specifications with dependencies on other crates.

// Mini-discussion / future work
There is potential for improvements as the Rust compiler is not optimized for generating LLVM IR for HLS and bambu is mostly optimized on LLVM IR generated by Clang. The library is somewhat modular and could be used to integrate other high-level synthesis tools with rust.


[discrete]
== Zusammenfassung

// Motivation
Das Ziel dieser Arbeit ist es zu untersuchen, ob es möglich ist Rust als Quellsprache für High-Level-Synthese (HLS) zu verwenden. PandA bambu ist ein Open-Source Forschungsframework für HLS. Es unterstützt primär C oder {cpp} als Eingabesprachen über Clang, bzw GCC als Frontend. Das Clang Frontend ermöglicht es Programme in LLVM Intermediate Representation (LLVM IR) zu verarbeiten. Rust ist eine beliebte moderne Systemsprache, die LLVM als Backend für Codegenerierung verwendet. Deshalb sollte es möglich sein Rust als Quellsprache für HLS zu verwenden.

//Inhalt
Diese Arbeit untersucht wie eine Toolchain aussehen würde, die HLS aus Rust mit bambu durchführt,
welche Einschränkungen auf den Rust Code, der verwendet wird um LLVM IR für HLS zu generieren, zutreffen,
wie die resultierenden Designs im Vergleich zu ähnlichen Designs, die aus C oder {cpp} Code generiert wurden, sind,
ob das Rust Ökosystem verwendet werden kann, und
wie die Toolchain mit dem rust-hdl Projekt integriert werden kann.

//Ergebnisse
Es wird gezeigt, dass es möglich ist Rust als Quellsprache für High-Level-Synthese zu verwenden. Eine Bibliothek für die Verwendung von HLS in rust-hdl, einer Rust basierten HDL, wurde entwickelt und verwendet um Designs aus verschiedenen Algorithmen zu generieren. Im Vergleich zu Designs, die aus äquivalentem {cpp} Code generiert wurden, waren sie in Bezug auf Größe und Performance in den meisten Fällen ähnlich. Die meisten Features von Rust werden unterstützt, mit der größten Einschränkung, dass die Spezifikation keine nicht behebaren Fehler zur Laufzeit auslösen darf (`panic!`). Das Rust Ökosystem kann verwendet werden und die Bibliothek unterstützt die Synthese von Spezifikationen mit Abhängigkeiten zu anderen Crates.

// Mini-Diskussion / zukünftige Arbeiten
Es gibt Potenzial für Verbesserungen, da der Rust Compiler nicht für die Generierung von LLVM IR für HLS optimiert ist und bambu größtenteils auf LLVM IR optimiert ist, das von Clang generiert wurde. Die Bibliothek ist relativ modular und könnte verwendet werden um andere High-Level-Synthese-Tools die LLVM unterstützen zu erforschen.

<<<

toc::[]

<<<

// Start with section and part numbering
:sectnums:
:part-signifier: Part
:partnums:

= Thesis

== Introduction

The popularity of the Rust programming language is rising and it is one of the most loved new programming languages. It integrates modern tooling like standardized dependency management, testing, documentation generation, formatting and building. In the future, the adoption will probably increase further and it could replace {cpp} as the most common system programming language <<Bug22>>.
It is possible that Rust also provides benefits in domains other than systems programming. It has been shown that Rust can be used for other domains such as GPU programming, web development or logic programming <<Sah22>> <<Byc22>> <<Kyr22>>. This enables us to use some of the benefits of Rust <<Bug22>> <<Cos19>>.

This paper will explore how Rust can be used in the domain of FPGA firmware development. Usually, FPGA firmware is developed in a Hardware Description Language (HDL) such as Verilog or VHDL. In these languages, the programmer has to describe the hardware in detail. This is a low-level approach that can lead to efficient designs, but it is quite time-consuming <<Mil20>>. The rust-hdl project enables us to express hardware descriptions in Rust similar to traditional HDLs <<rusthdl>>. In addition to manually writing hardware descriptions in an HDL, it is also possible to use High-level synthesis (HLS) to generate hardware descriptions in HDLs from an algorithmic description written in high-level programming languages. This provides increased productivity at the cost of slightly less optimized designs <<Mil20>>. This process is currently mostly used with C or {cpp} because they are the most common system programming languages. There are multiple HLS tools available that can synthesize HDL code from {cpp} code. Multiple of these tools are based around the LLVM compiler infrastructure <<Nan16>>. There is one reported use of Rust as an HLS language but that is of no use to us as their tools are not available <<Har22>>. As Rust is also based on LLVM, it is possible that it can be used with these tools too. 

// TODO: Look in git history, I think I had a better intro at least parts of it
To figure out whether Rust can be used as a source language for HLS, we searched for HLS tools that can be used for Rust. We developed a modular process for integrating HLS tools with rust-hdl <<rusthdl>>. We used this process to implement a proof-of-concept integration with the PandA Bambu HLS framework and show that it is possible to use Rust as a source language for HLS. We evaluated the performance by comparing the resulting designs for different algorithms with designs generated from equivalent C code. We found that in most cases the solution produced designs comparable to those based on {cpp} based workflows.

// What exactly is the problem
// Why is this a problem

== How is hardware design done today

It is assumed that you have some knowledge of the Rust programming language and systems programming. This section will provide an overview about the topics that are relevant for this paper. It will also provide an overview about the related work that has been done for HLS from Rust.

=== Target platforms

// TODO: Maybe custom IC instead of ASIC
There are two target platforms for logic design: field-programmable gate arrays (FPGA) and application specific integrated circuits (ASIC).

==== What is an FPGA

// // What are CPUs?
// //TODO: Citations
// // TODO: Idk if the intro is appropriate
// CPUs process their instructions one by one. They are purpose-built machines that are carefully designed for processing lots of instructions in sequence. Traditional programming languages reflect this design for the most part. They are designed to make it easy for humans to write programs that can be executed sequentially one instruction at a time. There are approaches to parallelism, but they are either limited to having multiple threads of execution that run from top to bottom simultaneously. Or they have some instructions that perform the same operation on a fixed amount of data elements at the same time.


// // TODO: citation needed
// Field programmable gate arrays (FPGAs) are not designed to process instructions. As the name field programmable gate array implies, they consist of programmable logic gates with programmable connections between them. So while a CPU is a logic circuit designed to do one thing, an FPGA is a circuit designed to emulate other logic circuits. 

// // TODO: FPGA 

// // An FPGA can be used to emulate a CPU.
// // TODO: citation needed
// For example, a design describing the logic gates and connections that make up a CPU can be programed to the FPGA, so it will behave exactly like the CPU and can process instructions. Such a 'soft' CPU will be much bigger and therefore slower than a real CPU because a circuit emulated on an FPGA takes up more space than an application-specific circuit. This is however really useful for prototyping because you can test the CPU design in hardware with external peripherals, like memory and I/O devices.

// // More details on using FPGAs for hardware development
// // TODO: citation needed
// In the past, FPGAs were mostly used like this for prototyping hardware development. Hardware designers designed a circuit in a hardware description language (HDL) and simulated it as a first stage of verification. If that worked it can be deployed to an FPGA, and get tested there. If everything works as expected the design can be taped out and manufactured as an ASIC.

// // In comparison to an FPGA a CPU is an ASIC, I think. But I have no source for that

// // FPGA as computational accelerators
// // TODO: citation needed
// This is still the most common use case for FPGAs, but it is not the only one. In recent years the usage of FPGAs as pseudo-general-purpose computational accelerators became more relevant. Here you do not use them to prototype circuits that will eventually be taped out but as the final platform. FPGAs used in this context are known as _computational FPGAs_. It is somewhat comparable to the use of GPUs as computing accelerators. But where GPUs excel at tasks that perform the same operations in parallel on massive amounts of data, FPGAs can be used for some kind of computations with irregular parallelism with static structure. In opposition to GPUs, it is not yet clear what an appropriate abstraction for the computational pattern used with computational FPGAs is.

// // Modern FPGAs have hardware blocks






// What is an CPU

// What is an GPU

// What is an ASIC / Custom accelerator
// TODO: This is a stub
Another solution is to produce a custom chip that implements the operations in hardware.

// What is an FPGA
Instead of producing the circuit into a chip, it is possible to program the FPGA to emulate the circuit. FPGAs are off-the-shelf reconfigurable ICs capable of implementing any digital circuit. Design cost and time with FPGAs is much lower than for custom chips. Another benfits of using FPGAs is that the circuit can be upgraded after deployment. The downsides of using FPGAs are lower performance and higher power consumption compared to custom chips. They are considered compelling options for small to medium sized projects. Because FPGAs are off-the-shelf components they dont have the high upfront cost of ASICs. <<Bot21>>

// CN: FPGAs are suitable for small to medium sized projects. Because they dont have the high upfront cost of ASICs.
// CN: They use up more space than ASICs, so they are more expensive than the same ASIC produced at high volume.

// TODO: Early FPGA diagram
// TODO: improve note

NOTE: Different FPGA vendors use different names for the same things. <<Lah19>> This paper uses LB, made of LEs, made of LUTs and FFs.

// How do basic FPGAs work
At the core FPGAs are programmable logic elements that can represent a simple logic function. Every logic element has around 4-6 input bits (vendor specific). The inputs are used to address a programmable lookup table. The lookup table contains the truth table of the logic function. The output of the lookup table is coupled with an output register of the LE. This way the LE can also act as a flip-flop. <<Bot21>>
Multiple logic elements are grouped into a logic block (LB). A logic block provides a local interconnect between the logic elements. The local interconnect is used to connect the inputs of the logic block and the outputs of the logic elements to the inputs of the logic elements. The local interconnect is usually realized as a programmable crossbar. <<Bot21>>
An FPGA is made up of multiple logic blocks that are connected by programmable routing. There are multiple ways in which global routing can be realized, but the most popular one is an island-style architecture. It is shown in FIGURE XXX. <<Bot21>>

// TODO: Insert a figure on island style architecture
// FPGAs consist of configurable logic blocks (CLB) that are connected by programmable interconnects. The CLBs consist of multiple basic logic elements (BLE) and a local interconnect. At the core of each BLE is a Lookup table (LUT)<<Lah19>> <<Bot21>> 

// How are FPGAs programmed
// TODO: Cititation needed
All the programmable components are controlled by configuration stored in SRAM cells. FPGAs are programmed with a bitstream that contains the individual bits for every SRAM cell in the correct order. They usually have a serial interface where the FPGA can accept the bitstream and program itself. <<citation-needed>>

// The logic blocks are connected by a network of programmable interconnects. The logic blocks and interconnects are programmed by a bitstream. The bitstream is a binary file that configures the FPGA. The bitstream is generated from a hardware description language (HDL) design. <<Bot21>>

// Explain the concept of the critical path
For every path that the data takes through the circuit it needs a certain amount of time. The longer the path and the more things are in it, the longer it takes to take the path. A critical path is a path that must meet certain timing requirements for the design to function properly. <<Cri95>> If the circuit for example is clocked, then there are probably some actions that should be finished before the next clock cycle. If they do not the clock needs to be slowed down or the circuit will behave in unexpected ways. For this reason the critical path limits the maximum frequency. <<Bot21>>

// What is the difference in modern FPGAs
Modern FPGAs improve critical path delay by hardening certain features of the FPGA. Hardening means that a feature is implemented in a fixed way instead of being programmable. This Which features are hardened depends on the FPGA model. All modern FPGAs include hardened circuitry for arithmetic operations in their logic blocks. The exact details of the hardening depend on the FPGA model, but it usualy involes a fastpath for a carry bit. This is at least three times faster than a pure LUT based implementation. <<Bot21>>

DSP blocks are another common feature on modern FPGAs. They minimize the number of soft logic operations needed to implement common DSP algorithms. Like LBs they are connected to the programmable routing. Depending on the FPGA model, they can be configured to perform operations like multiplication, addition, subtraction, accumulation, and/or multiplication-accumulation in various sizes. <<Bot21>> <<Lah19>>

Bigger FPGA designs almost always require a memory buffer. Building a memory buffer out of logic blocks is possible, but not very efficient. Modern FPGAs include hardened memory blocks that can be used as memory buffers. They are called block RAM (BRAM). BRAMs are over 100 timer more dense than soft memory made from LUTs. BRAMs are about 25% of the area of modern FPGAs. <<Bot21>> <<Lah19>>

// What are the steps to convert an RTL design to an FPGA
// TODO: Mapping the netlist to the FPGA architecture is called technology mapping. Citation needed
Converting an RTL design to a bitstream is a multi-step process. The first step is synthesis. The RTL design is converted to a structural gate-level description. This is called a netlist. The netlist is then mapped to the block types availabe on the specific FPGA model. A series of complex optimizations can also be performed at this stage. After that the blocks  are placed to specific blocks on the FPGA and the connections between them are routed. It is crucial to minimize the routing distance between logic blocks, because routing accounts for over 50% of the critical path delay. As it is now known how every part of the FPGA should behave, the bitstream can be generated. <<Bot21>> 

All these steps are performed by a CAD tool usually provided by the FPGA vendor. For the biggest FPGA vendors these tools are integrated into their respective IDE. <<citation-needed>> Recent developments in the open-source community have led to the development of open-source tools that can perform these steps. <<citation-needed>>

// TODO: Mention timing analyzers and shit


// How do they compare to ASICs

// Notable FPGA vendors?

// Context of this paper

=== Netlists are synthesized to hardware

// TODO: This is a stub
All approaches of hardware design end up generating a structural gate-level description called netlist typically in a subste SystemVerilog. This netlist is then used to generate the actual hardware design in a mostly automated fashion. How this step actually works depends on what kind of Hardware is targeted e.g. FPGA, ASIC or fully custom chips.
//<<Fla20>>

This step is basically manufacturing a design. The focus of this paper is only on design and not on manufacturing, so we will not go into detail about this step. It is just important to know that a netlist is something that can be manufactured / deployed to a FPGA.

=== Design using traditional Hardware Description Languages
// What are common HDLs and what are they used for
// TODO: Systema verilog is most common nowadays
Traditionally, logic design is done in hardware description languages (HDL). There are two established ones, Verilog and VHDL. VHDL has a slightly higher level of abstraction and some features that make it easier to manage bigger projects. In some ways, the relation between Verilog and VHDL is comparable to the relation between C and {cpp} in terms of features and abstraction. Both can be used to model the structure of hardware equally efficiently, so the choice is mostly a matter of personal preference. <<Smi96>>

// TODO: Mention timing

// What do you describe in HDLs
// TODO: Citation needed
Most commonly HDLs are used to describe circuits in a register-transfer level (RTL) abstraction. On RTL these languages describe registers that can hold state and the combinational (time-independent) logic that connects them. In HDLs the registers and combinational logic can be bundled into a module to make it reusable. These modules can then be connected to form a larger circuit. This is the basic structure of an HDL design.

// What distinguishes them from progamming languages

// What is a typical HDL design flow
A typical HDL design flow consists of four phases, design, verification, synthesis and implementation. In the design phase, the circuit is designed in an HDL. The design is then verified in various ways. To verify the behaviour of a design testbenches are defined. These testbenches (usually also written in HDL) instantiate the module of the design under test (DUT), exercise the inputs and verify that the outputs behave as expected. A logic simulator is used to execute the testbenches. This is comparable to unittesting in programming languages. After the design is verified, a logic synthesis tool is used to synthesize the design into a optimized gate-level logic description (netlist). A formal equivalence tool can then be used to verify that the netlist is equivalent to the original design. In the implementation phase, the netlist is mapped to the target hardware. <<Fla20>>
 
// What are the problems with HDLs
The use of HDLs is mostly limited to hardware engineers

// What are the possible solutions

==== SystemVerilog sample

In SystemVerilog units of logic can be encapsulated into modules. Modules can have inputs, outputs and internal state. <<verilog-blinker-listing>> shows a simple module that implements a blinker. The blinker has a clock input and a blinker output. At every rising edge of the clock a internal counter is incremented by one. Once the counter reaches a certain value, the blinker output is toggled. The counter is reset to zero when the blinker output is toggled.

.Simple blinker module in SystemVerilog
[source.linenums#verilog-blinker-listing,verilog]
----
include::systemverilog-blinker/blinker.sv[]
----

Verification in SystemVerilog can be achieved by using testbenches that instantiate the module under test and exercise its inputs. <<blinker-testbench-listing>> shows a testbench for the blinker module. The testbench instantiates the blinker module and connects it to a clock signal. It also asserts that the blinker output toggles every 10 clock cycles.

.Testbench for the blinker module in SystemVerilog
[source.linenums#blinker-testbench-listing,verilog]
----
include::systemverilog-blinker/blinker_tb.sv[]
----

// TODO: citations? 
// TODO: More detail?

=== Design using alternative Hardware description languages

// TODO: This is a stub
There are multiple modern HDLs that try to improve on the shortcomings of Verilog and VHDL. Most of them try to bring some features from modern programming languages to hardware design. (linting, formatting, dependency management, namespaces/scoping, better support for multifile projects, ...). These languages are usually transpiled to Verilog and then synthesized to netlists.

// TODO: List of alternative HDLs

// TODO: Mention timing somewhere

==== rust-hdl

rust-hdl is a Rust crate that allows describing RTL logic in Rust. The logic can then be transpiled to Verilog. rust-hdl also includes tools for simulation and verification. Because a rust-hdl based design is just a Rust program, it can use most of the Rust ecosystem features. This includes the Rust testing framework for verification and simulation. It also makes it possible to reuse and share designs using the Rust packagemanager cargo. <<rusthdl>>

.rust-hdl struct for a simple blinker module.
[source.linenums#rusthdl-blinker-struct-listing,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-struct]
----

In rust-hdl logic blocks (equivalent to Verilog modules) are defined as structs. The field of these logic blocks corresponds to the external and internal ports of the module. External ports are defined as `Signal` with a direction and a data type. If the logic block is contains another logic block it is also defined here. rust-hdl provides some basic logic blocks for common functions like D flip-flops (DFF) or constants. <<rusthdl>> An example for a logic block definition is shown above in <<rusthdl-blinker-struct-listing>>.

The combinational logic that connects these signals is described in the update function. Every rust-hdl signal has a .next field that determines the value of the signal after the update function. The update function must assign a value to all the .next fields. If there are multiple assignments to the .next fields the last one is valid, like in normal Rust. As Rust can express much more than just combinational logic, rust-hdl only allows a limited synthesizable subset of Rust in the update function. <<rusthdl>> 

.The restrictions of the synthesizable subset include:
- No local variables
- Assignments are only allowed to .next fields of signals
- Function and method calls are limited to a small subset of library functions
- Only range based for loops are allowed

<<rusthdl>> 

.rust-hdl update function for the blinker.
[source.linenums#rusthdl-blinker-update-listing,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-logic]
----

rust-hdl also enforces an additional set of semantic rules about what the update function can contain. For example it does not allow undriven nets, so there must be at least one assignment to the `.next` value of every field. If there are multiple assignment to the next value of a signal, the last one is valid, like in normal Rust. rust-hdl also forbids to use the `.next` value on the right side of an expression. rust-hdl enforces these rules at compiletime and generates somewhat helpful error messages. <<rusthdl>> An example for a update function is shown above in <<rusthdl-blinker-update-listing>>.

.Simulating and verifying the blinker
[source.linenums#rusthdl-blinker-simulate-listing,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-test]
----

The Rust unit testing framework can be used to simulate and verify rust-hdl modules. <<rusthdl-blinker-simulate-listing>> shows a testcase that simulates and verifies that the blinker module toggles its output with the specified frequency (10 cycles in that case). The test case can be run with `cargo test` like any other Rust unit test.

.Generating verilog from the rusthdl blinker
[source.linenums#rusthdl-blinker-generate-listing,rust]
----
include::rusthdl-blinker/src/main.rs[tag=generate-verilog]
----

rust-hdl can then be used to generate a Verilog description of the design as shown in <<rusthdl-blinker-generate-listing>>. The generated Verilog code can then be used with any Verilog toolchain to deploy the design to a FPGA. <<rusthdl>>

// Justify why rust-hdl is a good choice even though it is more verbose
// TODO: Compare LoC
rust-hdl code is a more verbose than Verilog code. This is mostly because of the additional type annotations and the need to explicitly assign the .next value of every signal. 


=== Design using High-level synthesis
// What is HLS
High-level synthesis (HLS) is a process that can generate a RTL specification of a circuit from a description in a high-level programming language. This is a more productive approach than writing RTL code directly, but the resulting designs are usually less efficient. The generated RTL code can then be used in the same way as manually written RTL code. <<Mil20>> <<Nan16>> <<Lah19>> <<Cou09>>


// Performance of HLS compared to RTL
// TODO: Weirdly written, (which experiments?)
The main advantages of HLS are reduced design time and lower development cost. On average the development time of HLS designs is only a third than that of equivalent RTL designs. The tradeoff for the lower development time seems to be that HLS designs on average take up around 40% more basic FPGA resources than RTL designs. The performance of HLS designs is around two thirds of that of a RTL design. These metrics generalize over many different HLS tools and input languages and there is a big variance between different experiments. In about 40% of the cases the HLS design was more efficient or performant than the RTL design. These metrics for performance and resource usage show a lot of variance between experiments, in about 40% of designs the HLS design was actually more performant than the RTL design. In about 30% it was more resource efficient. However in terms of development time there is little variance, in 90% of the experiments the HLS design was faster than the RTL design. <<Lah19>>

==== How high-level synthesis works
// Intro
// TODO: Remove?
High-level synthesis tools generate a timed RTL implementation of a circuit from a functional specification. A functional specification is a untimed description of the desired behavior of the circuit. <<Cou09>>

// What are the tasks that HLS performs
Usually HLS tools seperate the HLS process into seven tasks. <<Cou09>> They can be seperated into three stages.

// Compilation of the functional specification
First the functional specification is compiled into a formal model. The formal model is usually a control and data flow graph (CDFG). In the CDFG every node (called basic block) represents a static sequence of statements. The edges between the nodes represent conditional data flow. Opposed to a normal data flow graph (DFG), the edges can be conditional. <<Cou09>> In this step transformations can be applied to the functional specification. These transformations can include loop unrolling, inlining, dead code elimination and other common software compilation optimizations. <<Cou09>> 

// TODO: I dont have a citation, but say this: While these hardware resources can be real hardware resources, in this stage they are more comparable to Verilog modules. they are not (necessarily) real FPGA hardware resources, but can also be arbirary soft logic. ???

// What are the hardware resources
The resulting formal model then has to be mapped onto hardware resources. The types of resources are functional units, storage elements and connection units. Functional units perform the actual computation on the data. They can be multipliers, arithmetic logic units, or other custom functions. Storage elements are used to store values over multiple cycles. They include registers, memories, or other custom storage elements. Connection units represent the connections between the functional units and storage elements. They include resources like multiplexers and buses. HLS tools include RTL descriptions for all of their supported resource types. <<Cou09>>

// Scheduling, allocation, binding
// TODO:  This binding process does not determine the placement of the resources on the FPGA. (maybe it takses it into account, but not neces..)
Allocation determins the kinds and number of resources that are needed to satisfy the design constraints. Scheduling then schedules the operations into cycles based on the available resource types. Operations that do not have data dependencies between them can be scheduled in parallel.The next step after scheduling is called binding. During binding operations get assigned to specific functional units. Variables that carry a value over multiple cycles must be bound to storage elements. Finally it is assigned which connection units connect which functional units and storage elements. Scheduling information can be used to bind multiple variable with non-overlapping lifetimes to the same storage elements. <<Cou09>> 

// How does the resulting RTL look
The final step of HLS is to generate the RTL architecture for the design. Classically the architecture consists of a controller and a data path. The data path contains all the storage elements, functional units and connection units described earlier. The controller is responsible for driving the data path. <<Cou09>> 

// Data path
The data path contains all the storage elements, functional units and connection units described earlier. These components can be connected arbitrarily by buses. The inputs of the data path are data inputs from external sources and control inputs from the controller. The outputs of the data path can be data outputs or control outputs. The data path also receives control signal from the controller. The control signals control how the components get connected; they orchestrate the data flow through the data path. <<Cou09>> 

// Controller
The controller usualy consists of three parts. The state register contains the id of the current state. The output logic generates the control signals that drive the data path based on the current state. The output logic also generates control outputs that can act as inputs for the data path. The next state logic computes the next state of the FSM based on the inputs and the current state. <<Cou09>> 

// Disclaimer
While this is the typical architecture, different HLS tools can also do it differently. An alternative approach is for example to generate a processor with a hardcoded program and instruction set that is optimized for the task at hand. <<citation-needed>>

// TODO: Diagram for typical HLS architecture from Cou09

// TODO: IP Core


==== High-level synthesis tools
// Available HLS tools
HLS tools can be distingushed into two major categories. Those that accept a general purpose language and those that accept a domain specific language (DSL) a an input. Using a DSL as input can lead to better performing designs, but it also raises challenges for adoption. A general purpose language makes it easier for the algorithm designer, who is usually a software developer, to write code. <<Nan16>> The most common input languages for HLS are C based, including C, {cpp}, and SystemC. <<Lah19>> 

// What HLS tools are there
// TODO: Add citation for usually.
// TODO: Add citation for bambu is the biggest. 
HLS tools are usually available as a part of the IDE provided by the FPGA vendors. For example, AMD Xilinx provides the Vivado HLS tool as part of its IDE and Intel Altera includes the Intel HLS Compiler in its Quartus Prime IDE. <<intel-hls>> The most popular HLS tool in academia is Vivado HLS <<Lah19>>. There are also a few open-source HLS tools available. The biggest open-source HLS tool that is currently activly maintained is panda bambu. <<Nan16>> 

bambu provides as a research environment to experiment with new ideas in HLS. It can take C/C++ and LLVM IR as input. <<Fer21>>

In this paper we will use bambu as a HLS tool, because it is open-source and supports the latest LLVM version. 

// What are the problems with those languages
// TODO: This section does probably not belong in this part
Old programming languages, we have better alternatives nowadays
More steps in the design flow
More explicit. Register can be anything


==== Bambu HLS
// Bambu intro
Bambu is a open-source academic HLS tool. <<Fer21>> <<Nan16>> Its architecture is designed to make it easy to experiment with new ideas across high-level synthesis and related topics. It supports input specifications in standart C/C++ or the intermediate representations of GCC or clang/LLVM. <<Fer21>>

// Bambu compared to commercial offerings
// TODO: Nan16 has benchmarks

// Bambu architecture overview
Bambu is based on a three stage design. The frontend is used to convert input specifications in various formats into a static single assignment (SSA) IR. The middle end performs various transformations and analysis on the SSA IR. The backend performs the actual architectural synthsis. <<Fer21>>

// Frontend
The frontend is utilizes a plugin for GCC or clang to process a input specification in any of the formats supported by these two compilers. Noteably this includes support for LLVM IR through clang. Bambu then generates its own SSA IR from the control from the call graph and control flow information provided by GCC or clang. <<Fer21>>


// Middleend
// TODO: Last sentence is shit
The middleend applies a set of analysis and transformations to the specification. This includes common software compilation optimizations such as loop optimizations and dead code eliminatin. It also includes more HLS specific optimizations. For example multiplications and division with constants are replaced with shift and add operations, because real multiplication is expensive in hardware. Bitwidth and range analysis optimizations are also performed, because it does not need to target a data-path with a fixed width (i.e. 32bit, 64bit). <<Fer21>>

// Backend
The backend performs the actual architectural synthesis. This is mostly done as described in <<_how_high_level_synthesis_works>>. Bambus biggest difference is that the synthesis process acts on every function individually. It generates a controller and data path for every function. If a function calls into another function the generated logic for the other function is instatiated as a submodule. Bambu optimizes submodules that are shared between multiple modules. Bambu also provides hardcoded optimized modules for functions from common libraries such as `libc` or `libm`. The resulting architecture can be translated to VHDL or Verilog. <<Fer21>>

==== Languages used for High-level synthesis

// TODO explain systems programming language

=== LLVM

// What is LLVM
// TODO: Find source and describe how LLVM is used today
LLVM is a modular compiler framework that can be used to build compilers for many different programming languages. It defines a low-level code representation called LLVM intermediate representation (LLVM IR). <<Lat04>>

// What is LLVM IR
LLVM IR is a strongly typed, static single assignment (SSA) based intermediate representation. LLVM IR is designed to be easy to compile to machine code and to optimize. LLVM IR has a low-level, language independent type system. The type system captures enough type information to safely perform a number of aggressive transformations that would traditionally be attempted only on type-safe languages in source-level compilers. LLVM IR is not intended to be an universal compiler IR, so it does not capture all of the language specific type information. Some of the concepts that are not represented in LLVM IR are classes, inheritance, or exceptionhandling semantics. <<Lat04>>

// How is LLVM used in compilers
Because of the this compilers based on LLVM provide a front-end that process the program code in the source language. The frontend is able to perform language specific optimizations. The compiler frontend emits LLVM IR which is passed to the LLVM backend which performs a variety of transformation and optimizations. The processed LLVM IR is then passed to a code generator backent to translate it into native code for a given target. <<Lat04>> The LLVM project includes code generator backends for many targets. <<Lat04>> <<rustc-guide>>


=== The Rust programming language

// What is Rust
Rust is a modern systems programming language aiming to replace C and {cpp} as the industry standard systems programming language. It offers zero cost memory saftey, a strong type system and a modern toolchain. Rust surpasses all other common memory-safe languages in terms of performance while offering many modern features that more established systems programming languages tend to lack. For these reasons it has been voted the most loved programming language every year since 2016. <<Bug22>> <<Cos19>> <<Kla23>>

// TODO: Insert rust performance diagram

// What is a systems programming language
Systems programming languages are programming languages that can deal with low-level details of memory management, data representation and concurrency.
They are often designed for use in resource-constrained, performance-critical or close-to-hardware programs. They are used to implement operating systems, embedded systems, device drivers and other software that interacts with hardware. Common systems programming languages include C, {CPP} and Rust. <<Kla23>> <<Str12>>

// TODO: Explain how the rust compiler uses LLVM
// Details for this are already summarized at <<rustc-guide>>

// Explain the borrow checker and memory saftey
// TODO: Ask about the first sentence being an exact quote
// TODO: Make the first quote shorter
Rust is the first industry-supported computer programming language to overcome the longstanding trade-off between the control over resource management provided by lower-level languages for systems programming, and the safety guarantees of higher-level languages. <<Bug22>> <<Jun17>> Rust achieves this by enforcing that every variable is always owned by exactly one scope. When that scope ends the variable is destroyed. For this the time between creation and distruction of a variable is called its lifetime. Rust provides semantics for moving ownership between scopes. <<Kla23>>
This model of scope based resource management is called RAII. It is used by many systems programming languages including {cpp}. While {cpp} enables the programmer to break out of that system by using pointers to variables that are not owned by the current scope or its parents, Rust does not allow this. <<Str12>> Rust instead uses a type of reference with attached lifetime information called a borrow. The borrow checker statically guarantees at compiletime that borrows always point to valid objects. It does this by ensuring that the lifetime of every borrow ends at or after the lifetime of the current scope. The compiler also makes sure that there can only be either one mutable borrow or multiple immutable borrows to a value at the same time. This ensures that the values of borrows dont change unexpectedly. The programmer can opt out of the borrow checker by annotation code as `unsafe`. This allows the programmer to use raw pointers and other unsafe constructs. Unsafe code is sometimes neccessary to implement lowlevel data structures, such as Heap memory (`Box` or `Vec`) or types with internal mutability (`Ref`). The standard library provides most of these constructs, so most Rust programs do not need to use unsafe code. <<Kla23>> <<Bug22>> There is ongoing work on verifying that the unsafe code in the standard library is safely encapsulated by its types. <<Jun17>>

// Overview about how the compiler processes Rust code
// TODO: Explain desugaring (the word)
Rust is a compiled language. The Rust compiler (rustc) can compile Rust code to native code for many different platforms. It accomplishes this by compiling Rust to LLVM IR and then using LLVM for code generation. This allows Rust to support many different platforms without having to implement a backend for every platform. It also enables Rust to utilize the large suite of advanced optimizations collected by the LLVM project. The compiler does not generate LLVM IR directly from the input Rust code. Instead it the input code gets passed through multiple IRs, HIR, THIR and MIR. HIR and THIR still resemble Rust code, but some constructs get desugared. The rust compiler uses these stages to perform typechecking and verification. Verified THIR is converted into MIR, which is a CFG based representation of the code. rustc performs flow-sensitive safety checks like borrow-checking on this level. The MIR is also used to apply various Rust-specific optimizations to the code. The Rust compiler can be configured to output the various intermediate representations instead of the generating machine code. <<rustc-guide>>

// Overview about the tooling related features of Rust
A important part of what makes the Rust ecosystem so productive is that Rust offers standardized tooling. Every Rust project (also called crate) contains a `Cargo.toml` file specifying the project metadata and dependencies. The `cargo` tool can then be used to perform all standard tasks like building, executing, unit testing, integration testing, formatting the code, generating documentation, downloading dependencies, building dependencies, managing dependencies, publishing the project, benchmarking, setting up projects and some more. The official community crate registry `crates.io` can be used to easily find dependencies. It also links to the automatically generated documentation for every crate. The tooling alone makes Rust a much better development experience than most systems languages. <<Bug22>>

// 

=== Design using accelerator design languages

Accelerator design languages (ADL) are a family of programming languages that are specifically designed to be synthesized to HDL. They are mostly similar to programming languages but offer many features that are usually only found in HDLs, like more fine-grained control over timing and memories memory access. <<hdl-to-adl>>



== Concept, implementation and architecture

// Short overview of the solution and this section
The goal of this paper is to use Rust as a source language for HLS using an existing HLS tool. First a suitable toolchain has to be designed. Then a concept for integrating the toolchain with rust-hdl has to be developed. Finally, a proof-of-concept implementation will be done to show that the process works and enable evaluation of the resulting solution.

// TODO: Maybe define some criteria for our solution
// * Can synthesize a simple Rust program
// * Can synthesize our md5 implementation
// * Can synthesize most stateless Rust function
// * The synthesized function can use Rust crates from crates.io
// * The existing Rust tooling (linter, formatter, etc.) work with out function

=== Basic toolchain for synthesizing Rust

As there is currently no tool that can synthesize Rust directly the first step in the toolchain needs to convert the Rust code into a language that can be used by an existing HLS tool. There are multiple HLS tools that support C or {cpp} by using LLVM compiler infrastructure. As a result of using LLVM some of these tools can also use LLVMs intermediate representation (LLVM IR) directly as an input language. <<Nan16>> The Rust compiler can compile Rust code to LLVM IR. This will be the starting point of the toolchain

The Rust compiler is frequently updated and the generated LLVM IR usually uses the latest LLVM version. <<citation-needed>> This means that a suited HLS tool needs to be activly maintained to support the generated LLVM IR. The only HLS tool that fulfills these requirements seems to be panda bambu. <<Fer21>> SmartHLS and Vivado HLS may also be capable of operating on LLVM IR, but they are not open source and only available as part of an IDE and not as standalone programs, which makes them unsuitable for our use case.

.The toolchain
[pikchr]
....
   arrow right 150% "Rust" "function"
   box rad 10px "Rust Compiler" fit
   arrow right 190% "LLVM IR" "function"
   box rad 10px "PandA Bambu" fit
   arrow right 130% "Verilog" "RTL"
....

The basic toolchain is relativly straightforward. In a first step the Rust compiler is used to convert a Rust function to a LLVM IR function. The second step passes the generated LLVM IR function to panda bambu which converts it to Verilog. The resulting Verilog can then be used in a larger HDL design.

==== Toolchain proof of concept

// Intro
// TODO: Add a short section on keccak
// TODO: Format keccak name
This toolchain can be used to synthesize Rust functions. The following section will show how a example Rust function can be synthesized. 

.`minmax` function in Rust
[source#minmax-rust-listing.linenums,rust]
----
include::../rust-minmax/src/minmax.rs[tag=function]
----

The `minmax` function shown in <<minmax-rust-listing>> finds the minimum and maximum in an array of integers. The implementation of the function is kept in a C-like style, so that it can be compared to an equivalend C implementation later. The function takes a pointer to an array of integers (`numbers`), the number of elements in the array (`numbers_length`) , and two pointers to integers (`out_max` and `out_min`). The function finds the smallest and largest value of the array and writes them to the memory locations pointed to by `out_max` and `out_min`.

// TODO: Explain that a Rust slice is just a pointer to an array with a length
// TODO: Specify: bambu does not support the llvm that is generated by slices or smth like that
The function takes a pointer to an array and its size instead of a rust slice, because bambu does not support slices in the interface. This is a limitation of the current implementation of bambu and could be fixed in the future. The function also needs to be annotated with `#[no_mangle]` to instruct the Rust compile to preseve the function name in LLVM IR. This is required because bambu uses the function name to generate the name of the Verilog module. The function is also `extern "C"` to instruct the Rust compiler to generate LLVM IR that is compatible with the standard C ABI. Finally the interface of the function is marked as `unsafe` because it uses raw pointers.

Inside the function the pointer and size are converted to a slice (`input`). Local variables are defined for the minimum and maximum values (`local_max` and `local_min`). The function then iterates over the slice and updates the local minimum and maximum values if a smaller or larger value is found. Finally the local minimum and maximum values are written to the memory locations pointed to by `out_max` and `out_min`. 

.Compile Rust to LLVM IR
[source#rust-to-llvmir-listing,shell]
----
rustc \
  src/minmax.rs -o minmax.ll --crate-type=lib \
  --emit=llvm-ir # Emit LLVM IR instead of machine code
  -C llvm-args=--opaque-pointers=false # Disable opaque pointers
  -C no-vectorize-loops \
  -C panic=abort \
  -C overflow-checks=off \
  -C opt-level=3 \
  -C linker-plugin-lto=on \
  -C embed-bitcode=on \
  -C lto=fat
----

The first step is to compile the function to LLVM IR using the rust compiler. The command shown in <<rust-to-llvmir-listing>> contains most of the options that are required or recommended to generate LLVM IR that can be used by panda bambu. `src/keccak.rs -o keccak.ll --crate-type=lib` specifies the filenames and that a library and not an executable will be build. The most important options is `--emit=llvm-ir` which tells the compiler to emit LLVM IR instead of machine code.

Recent versions of the Rust compiler will generate LLVM IR with https://llvm.org/docs/OpaquePointers.html[opaque pointers](`ptr` instead of `i32*`) by default. This is not supported by bambu. Opaque pointers can be disable by passing `-C llvm-args="--opaque-pointers=false"` to rustc.
// TODO: Explain LLVM bitcode

// TODO: Does bambu support cpp exceptions?
// TODO: Does bambu support c exit?
// TODO: Describe Rust panic
By default, the Rust compiler generates code that unwinds the stack on panic. The generated LLVM IR will also have an exception-handling personality function added to every function in LLVM IR. This is not synthesizable by bambu. The `-C panic=abort` instructs rustc to instead terminate the program on panic. This is also not synthesizable but bambu seems to compile the code if the panics can never be reached. It may be possible to adjust bambu to provide a custom implementation for panic.

The `-C no-vectorize-loops` option disables loop vectorization. This is required because bambu does not support vector instructions.

Disabling overflow checks for arithmetic operations significantly reduces the number of places where a function can panic. The `-C overflow-checks=off` option disables overflow checks. The Rust compiler can also generate debug assertions. They perform extra safety checks, which can make it easier to find and fix bugs, but these checks usually manifest as panics. To avoid panics the `-C debug-assertions=off` option can be used to disable debug assertions. On every optimization level other than `0` they are automatically disabled, so this option is not required in this example.

The final set of parameters enables various Rust compiler optimizations. These serve three purposes. For once the Rust compiler can perform more optimizations than bambu, because it has more information about the code. The second purpose is to reduce the amount of code that bambu has to synthesize. The third and probably most important purpose is to eliminate dead code that could potentially panic, as that is not supported by bambu. For this reason it is basically always required to set the optimization level to any level higher than `1`.

.High-level synthesize LLVM IR into Verilog
[source#llvmir-to-verilog-listing,shell]
----
bambu minmax.ll \
  --compiler=I386_CLANG16 \
  --top-fname=minmax \
  -O3
----

The LLVM IR can then be used with bambu to generate Verilog. The command shown in <<llvmir-to-verilog-listing>> instructs bambu to generate a Verilog module named `minmax` from the LLVM IR file `minmax.ll`. The `--compiler=I386_CLANG16` option instructs bambu to use clang 16 as the frontend for processing input. bambu supports multiple versions of clang and GCC. The `--top-fname=minmax` option specifies the name of the top level function. The top level function will be used as the entrypoint in the hardware design, comparable to the `main` function in a C program. This will also be the name of the generated Verilog module. The `-O3` option instructs bambu to also perform optimizations in its front and middleend.

// TODO: insert gitpod link or something to the full code sample

==== Interface of the generated Verilog module


.Interface of the generated module
[symbolator#minmax-generated-interface]
....
module minmax(
  //# {{control|Control}}
  input clk;
  input reset;
  input start_port;
  output done_port;
  //# {{data|Parameters}}
  input [31:0] Pd5;
  input [31:0] Pd6;
  input [31:0] Pd7;
  input [31:0] Pd8;
  //# {{power|Memory}}
  input [63:0] M_Rdata_ram;
  input [1:0] M_DataRdy;
  output [1:0] Mout_oe_ram;
  output [1:0] Mout_we_ram;
  output [63:0] Mout_addr_ram;
  output [63:0] Mout_Wdata_ram;
  output [11:0] Mout_data_ram_size;
);

endmodule
....

.Descriptions of the control signals
[#control-signals-table,cols="1,4"]
|===
|Port |Description

|`clock`
|The clock signal is used to clock the module

|`reset`
|Resets the module if set to 0.

|`start_port`
|The module will start executing, if this is high and the function is finished. Pinning this to high will cause the function to repeat.

|`done_port`
|Pulses high for one cycle, when the module is done.

|`return_port`
|Contains the return value while `done_port` is high. Can contain random values during function execution. Does only exist if the function has a return value.

|===

The interface of the component is shown in <<minmax-generated-interface>> and can be split into four categories. The first section contains clocking and control signal including clock, reset signal, start port and done port. The exact meanings of these signal is described in <<control-signals-table>>.

The second section contains the function parameters. When LLVM IR is used as input for bambu the real names of the function parameters get lost and they are replaced with numbered names like `Pd5`. The order of the signals stays the same as the order of the inputs to the original function. The original name of the parameters can be inferred by the order of the numbered names, because bambu always numbers the parameters in the order they appear. In this case `Pd5` is `numbers_length`, `Pd6` is `numbers`, `Pd7` is `out_max` and `Pd8` is `out_min`. Memory pointers got converted to 32 bit numbers, but bambu can also be configured to generate other address sizes.

// TODO: Ask if this kind of memory interface has a name
The original function takes a pointer to memory as input. This means that the generated module needs to have a way to access that memory. Because of this bambu generates a memory interface for the component which needs to be connected to memory. During function execution the component will use this memory interface to retrieve and modify the input values. By default bambu generates a memory interface for a memory with 32-bit data and 32-bit addresses with two parallel direct connections. Most of the signals are double the size they would need to be for a single connection. For these signals the first half controls the first channel and the second half contains the second channel. <<memory-interface-table>> describes the memory interface in more detail.

// TODO: Make tables wider than text if neccessary
.Descriptions of the memory interface
[#memory-interface-table,cols="1,1,3"]
|===
|Port |Size per channel (bit) |Description

|`Mout_oe_ram`
|1
|Set to 1 to read from the channel.

|`Mout_we_ram`
|1
|Set to 1 to write to the channel.

|`Mout_data_ram_size`
|log2(dataWidth) + 1
|Set the width of bits that should be written to the memory. Can be a value between 0 and the width of your data.

|`Mout_addr_ram`
|addressWidth
|Select the address this channel should operate on.

|`M_Wdata_ram`
|dataWidth
|Contains the data that will be written to memory if `Mout_we_ram` is set.

|`M_Rdata_ram`
|dataWidth
|Contains the data that was read from memory if `Mout_oe_ram` was set in the last cycle.

|`M_DataRdy`
|1
|Nonzero if the memory is not ready

|===


=== Integrating the toolchain with rust-hdl

//TODO: Verilator background section
// With rust macros it is possible to execute arbitrary code transformations at compile-time. This can be used 
Using the toolchain as shown in the previous section requires a few manual steps. Especially the changing parameter names are difficult to work with as they need to be adjusted after every modification. To make the toolchain easier to use, a library for integration with rust-hdl was created. The library makes it possible to create a single Rust crate containing both RTL and HLS code in the same project. The library contains steps for synthesizing specific functions from a bigger Rust project, wrapping synthesized Verilog in a rust-hdl structs, highlighting errors in the source code before synthesis, and for generating simulations with Verilator.

// TODO: Move to background
Cargo provides a way to hook into the build process using buildscripts. These are small Rust programs that can modify the Source code before it is passed to the Rust compiler. The Rust compiler also allows custom code transformations at compile-time using procedural macros. Procedural macros backed by Rust functions that are executed at compile-time and can modify the code they are applied to.

The Rust library provides a `+#[hls]+` macro that can be used to mark Rust modules for HLS. The macro is evaluated two times. Before compilation a buildscript finds all `+#[hls]+` macros. It then extracts every marked module into a seperate temporary crate. This crate is then synthesized to Verilog using the toolchain mentioned above. A Rust module containing a rust-hdl struct that wraps the synthesized Verilog is then generated. That Rust module is then placed alongside the original source code. During compilation the `+#[hls]+` macro is evaluated by the Rust compiler. The macro then emits import directives for the module that were synthesized during the buildscript. It also evaluates if the macro is used in a valid context and emits useful error messages if it is not. <<macro-evaluation-overview>> shows an overview of the macro evaluation process.

.Block diagram of the macro evaluation before and during compilation
[nomnoml.completly-oversized-content#macro-evaluation-overview,opts=inline,width=20cm]
....
#gutter: 10
#fontSize: 18
#leading: 1.25
#lineWidth: 2
#padding: 14
#spacing: 40
#stroke: #000000
#font: Spectral
#direction: right
#fill: #f7f8f7; #ffffff; #f7f8f7; #ffffff; #f7f8f7; #ffffff
#.downwards: direction=down
[cargo build|
[<downwards> Buildscript|
  [parse rust crate]->[find hls modules]
  [find hls modules]->[extract crate]
  [extract crate]->[compile to LLVM IR]
  [compile to LLVM IR]->[synthesize with bambu]
  [synthesize with bambu]->[parse result]
  [parse result]->[fix parameter names]
  [fix parameter names]->[generate rust_hdl interface]
  [generate rust_hdl interface]->[create rust_hdl module]
  [parse result]->[create C++ library]
  [create C++ library]->[create rust_hdl module]
  [create rust_hdl module]->[place new rust file in original crate]
  [place new rust file in original crate]->[emit directives to link C++ library]
]

[<downwards>Rust compiler|
[<downwards> HLS macro evaluation|
  [parse rust macro]->[check for errors]
  [check for errors]->[emit import directives for synthesized module]
]
[<downwards> Compilation]
[HLS macro evaluation]->[Compilation]
]

[Buildscript]->[Rust compiler]
]
....

Rust-hdl is not able to simulate embedded verilog. Verilator can create a {cpp} library from Verilog that simulates a single component. Such a library is created for every synthesized module. The generated rust-hdl structs contain the glue code neccessary to call their library. The buildscript emits the directives neccessary to link the generated libraries. Then cargo will link the libraries into the final binary after compilation.

By using this approach it is possible to use the toolchain in a rust-hdl project. The modules can be used, simulated, and tested like normal a rust-hdl module. This also allows us to perform behavioral verification directly in rust unit tests. This workflow makes it possible to use HLS from Rust in a fully automated way.

=== Example of using the integration

// TODO: Write section
Lorem ipsum dolor sit amet.

== Experiments and results

Hypothesis: HLS from Rust is as performant as HLS from C++
Hypothesis: Using packages makes development faster
Hypothesis: With rust we can easily use packages

=== Experimental setup
// Algorithm -(implementation)> implementation -(synthesis)> design

// TODO: I think there is probably a better word for `equivalent Rust implementation`. Maybe `algorithmicly identical Rust implementation`?
To determine how HLS from Rust compares to HLS from C++ a set of algorithms will be compared. Every algorithm will have a {cpp} implementation and a equivalent Rust implementation. The equivalent Rust implementation is algorithmicly as close to the {cpp} implementation as possible. When possible a idiomatic Rust implementation will also be compared. The goal of the equivalent Rust implementation is to compare the performance of the toolchain and compiler. Comparing the idiomatic Rust implementation can show whether the toolchain is able to synthetize idiomatic Rust code. If a implementation for a specific algorithm is available on crates.io that implemention we also be compared. This will show whether we can use the Rust ecosystem to speed up development.

// TODO: Criticize that the GCC version is outdated af
The {cpp} implementations will be synthesized using bambu with both the GCC (Version 8) and clang (Version 16) frontend. Rust implementations will be synthesized using the toolchain described in the previous section, which is also based around bambu. All implementations will be synthesized once with optimizations for size (`-Os`) and once with optimizations for speed (`-O3`).

// TODO area is also RAM and DSPs

Different designs will be evaluated for the area of a FPGA they take up, their clock maximum frequency and the amount of clock cycles they take for a single operation. In case a design has a varying amount of clock cycles the average of all testcases will be used. The area and maximum frequency of a design will be taken from the placement report of nextpnr. nextpnr will be configured to target a Lattice ECP5 FPGA with 44k LUT and a speed grade of 6 (LFE5U-45F-6BG381C) for all tests. The area of a design is in this case measured as the sum of the number of LUTs (`TRELLIS_COMB`) and the number of flip-flops (`TRELLIS_FF`) from the nextpnr report. The amount of clock cycles will be measured using the rust_hdl simulator for Rust designs and the Verilator simulator for C++ designs. 

The focus of this evaluation is only to compare the behaviour and not on the exact implementation of the generated hardware designs. Because of this the synthesized Verilog files will be mostly treated as black-boxes.

=== Tested algorithms
//TODO: Stop using algorithm and function interchangably

Three functions will be compared.

The minmax function finds the minimum and maximum value in a array of integers. This algorithm is chosen because it is simple and can be implemented in a single function. This makes it easy to compare the performance of the toolchain and compiler. It is also one of the examples included in the bambu repository.

// TODO: Background section on keccak
// TODO: Stylize keccak-f[1600] with latexmath
The keccak-f[1600] function is a cryptographic function often used as a hash function. It was chosen because it is a real world algorithm that is used in many applications. This makes it a good example of how well the toolchain can optimize complex algorithms. It takes a pointer to 25 64-bit numbers and permutates them in place.

// TODO: This is not good. Rewrite
The md5 hash function is another hash function. It was also chosen because it is a real world algorithm. While md5 is no longer considered secure, it is also reasonably easy to implement.

// TODO: Find better functions. At least not 2 hash functions

// TODO: If there is time add an FIR filter: https://www.hackster.io/michi_michi/fpga-fir-filter-hls-kria-kv260-pynq-2eec35

// TODO: === Show limitations of the synthesizable subset of Rust
// Show the limitations of the toolchain. Limitations of the synthesizable subset of Rust.
// Somewhere but not here

Each {cpp} implemetation was tested in four configuration. Either compiled with bambus gcc or clang frontend, each once with optimization profiles for speed (`-O5`) and size (`-Os`). The equivalent Rust implementations were also tested once with optimization profiles for speed and size. When optimizing a Rust implementation for speed rustc is set to `-O3` and bambu to `-O5`. When optimizing for size rustc is set to `-Oz` and bambu to `-Os`.

// TODO:  === Show how the Rust ecosystem can be used
// not here
// TODO:  === Generated modules
// not here

=== Testing the minmax functions

Three implementations of a minmax function will be compared. The Rust implementation was already shown above in <<minmax-rust-listing>>. It is based on the {cpp} implementation shown in <<minmax-cpp-listing>>. The third implementation is another Rust implementation that is using iterators and a return value. It will be refered to as the idiomatic Rust implementation. It is shown in <<minmax-idiom-listing>>.

.Idiomatic `minmax` implementation in Rust
[source#minmax-idiom-listing.linenums,rust]
----
include::../rust-minmax/src/minmax_idiomatic.rs[tag=function]
----

The Rust compiler should be able to compile the idiomatic Rust implementation to mostly the same code as the equivalent Rust implementation. The biggest difference is that the idiomatic function returns a struct instead of writing its result to memory. The expecation is that the idiomatic Rust design always performs a bit better than the equivalent Rust design, as it does not have to access memory for writing the result. It has a specific `return_port` that contains the result.


// TODO: Replace this with the text from the blogpost as that is better.
The Rust, idiomatic Rust and Clang designs optimized for speed all take up around 3x as much area as the designs optimized for size. With the same optimization settings all designs that are based on LLVM toolchains need roughly the same space. The GCC designs do not show that, the design optimized for speed is only a little 1.2x bigger than the one optimized for size. That one is also the smallest design overall. Notably even the GCC design optmized for speed is the third smallest design. Both designs based on the idiomatic Rust implementation takes up little less space than the respective design based on the C-like Rust implementation.

It could be that LLVM performs some optimization that significantly increases the required area when optimizing for speed. GCC does not seem to perform that kind of optimizations.

.Area of the different minmax designs
:chart-id:
:vega-lite-filename: processed-charts/minmax_overview_area.vl.json
include::vega-chart.adoc[]

Each design was tested with test cases ranging from 0 to 50 elements. <<minmax-average-cycles>> show the average number of cycles for every design. 

.Average clock cycles for each minmax design
:chart-id: id=minmax-average-cycles
:vega-lite-filename: processed-charts/minmax_average_cycles.vl.json
include::vega-chart.adoc[]

The measurements show that the designs optimized for speed are all faster than the designs optimized for size. The six designs that were optimized by an LLVM toolchain show a similar performance with the same optimization goal.

The GCC designs are significantly both significantly slower. The GCC design optimized for speed has comparable performance to the LLVM designs optimized for size. The GCC design optimized for size takes the most cycles. It needs 30% more cycles than the next slowest designs.

There seem to be three performance classes, the fastest one comprised of the LLVM designs optimized for speed, the second fastest one comprised of the LLVM designs optimized for size and the GCC design optimized for speed and the slowest one comprised of the GCC design optimized for size. 

In both of the two fastest classes the repsective idiomatic Rust design is the fastest one. The equivalent Rust design is the second fastest one in both classes, then come the {cpp} designs.

As expected the fastest class is comprised of the LLVM designs optimized for speed, with the designs optimized for size being in the second fastest class. The GCC designs are all one class slower than the LLVM designs with the same optimization goal.

The fastest class contains all the big designs, while the small designs are all in the slower two classes. It seems like the LLVM optimizations for speed actually increased the performance at the cost of area. This was to be expected.

// TODO ^ Remove some sentences

<<minmax-detailed-cycles>> shows the exact number of cycles for each test case. The three performance classes are clearly visible.

.Clock cycles in relation to the input length
:chart-id: id=minmax-detailed-cycles
:vega-lite-filename: processed-charts/minmax_detailed_cycles.vl.json
include::vega-chart.adoc[]

The slowest class takes stem:[3] cycles for every additional input. The second fastest class takes stem:[2] cycles for every additional input.

Interestingly the fastest designs require 3 extra cycles per input for three additional inputs, but every fourth input they actually require 3 less cycles. This averages out to stem:[1.5] cycles for every additional input. It should be noted again that these are also the biggest designs.

The LLVM designs optimized for speed have unrolled the loop by a factor of four. This can be seen in the control-flow graph (CFG) of the LLVM IR shown in <<minmax-speed-cfg>>. The function processes a batch of 4 elements until there are less than 4 elements left. Then it processes the remaining elements one by one. Bambu seems preserve this structure in the generated designs. This explains why the big designs are also the fastest ones, as the additional size contains the 4x operation. The small designs save area by only having the 1x operation.

The idiomatic Rust design is consistently that fastest design taking one cycle less than the next fastest design. This constant advantage is probably because it does not need to write its results to memory after it has processed all elements. The C-like Rust designs are 2 cycles faster then the equivalent {cpp} designs compiled with Clang. 

The GCC designs are all slower than the LLVM designs, this could be the trade-off for the smaller design size.

The other relevant factor for estimating the performance of a design is the maximum frequency it can run at. <<minmax-max-frequency>> gives an overview of the maximum frequency for each design.

.Maximum frequency for each minmax design
:chart-id: id=minmax-max-frequency
:vega-lite-filename: processed-charts/minmax_max_frequency.vl.json
include::vega-chart.adoc[]

Most notably the LLVM designs optimized for speed are the ones that clock the slowest. The two Rust designs optimized for speed can clock slightly higher than the {cpp}/Clang design optimized for speed. As they are also the designs that take up the most space this is to be expected, as they probably have the longer critical paths.

The five smaller designs all clock around 2x faster than the bigger designs. Their maximum frequency seems to be roughly ordered by size of the design.

While the small designs can run at much higher frequencies they also require more clock cycles. The relevant metric for comparing the performance is the execution time. It is calculated by multiplying the number of cycles with the inverse of the maximum frequency. <<minmax-execution-time>> shows the average execution time for each design.

.Average execution time per test
:chart-id: id=minmax-execution-time
:vega-lite-filename: processed-charts/minmax_performance.vl.json
include::vega-chart.adoc[]

The LLVM designs optimized for speed perform between 25-30% better than the designs optimized for size. So they still seem to split into two groups, although the divide is not as big as with the other measurements.

As expected the significantly slower frequency of the big designs is more than enough to negate the advantage they gained by requireing less cycles. 

As the GCC designs both clocked at nearly the same frequency the lower cycle-count of the design optimized for speed makes it perform better than the one optimized for size. It performs comparable to the LLVM-based designs optimized for size, while the one optimized for size performs comparable to the LLVM-based designs optimized for speed. 

// TODO: Look into specifing how big things are getting parrallelized



.Minmax Area to performance
:chart-id: id=minmax-area-performance
:vega-lite-filename: processed-charts/minmax_area_performance.vl.json
include::vega-chart.adoc[]

The big designs perform worse and take up more area than the small designs. The idiomatic Rust design (size) is better in terms of size and performance than nearly every other design. The only exception is the GCC design optimized for size which is slightly smaller but performs significantly worse. These results can be seen in <<minmax-area-performance>>.

The task of computing the minimum and maximum can be easily parrallelized. The input can be split over multiple instances of the function and the results of each can be processed by another minmax function afterwards. In <<minmax-space-efficieny>> the designs are compared by their performance per area.

.Minmax Space efficiency
:chart-id: id=minmax-space-efficieny
:vega-lite-filename: processed-charts/minmax_space_efficiency.vl.json
include::vega-chart.adoc[]

The idiomatic Rust design (size) is the most space efficient design. The two GCC designs are second and third. The GCC design optimized for speed is a bit faster but also a bit bigger than the one optimized for size. This trade-off equalize again in this metric, as their performance per area is nearly identical.

The big designs perform really poorly in this metric, because they perform worse and take up more area than the small designs.

// TODO: Change idiomatic Rust design to use return via memory

// Experiment fazit

// TODO: Convert bullet points to text

The designs generated by LLVM based toolchains with the same optimization goal perform quite similar.

The Rust designs perform slightly better, but the difference is not significant.

The idiomatic Rust design performs a bit better than the equivalent Rust design but that could be because it has a slightly different interface, so the results may not be directly comparable.

The small designs performed better on all metrics except for cycle count.

For GCC the optimization goal seems to align with the actual benefits of the design. The design optimized for Code size also resulted in a smaller design, while the design optimized for speed resulted in a faster design.

For LLVM that is only partially true, while the design optimized for size resulted in a smaller design it was also faster than the design optimized for speed.

LLVM seems to perform more aggressive loop unrolling than GCC.

Bambu seems to preserve unrolled loops in LLVM IR.

=== Testing the keccak functions

==== Tested implementations

// TODO: Provide source to the keccak implementation
The reference {cpp} implementation is shown in <<keccak-cpp-listing>>. It is based on a reference implementation provided by the Keccak team. It takes a pointer to 25 64-bit elements and mutates them in 24 rounds. Each round consists of 5 functions that transform the elements in some way. The values after 24 rounds are the result of the function.

.Interface of the keccak function
[source#keccak-rust-interface.linenums,rust]
----
include::../rust-keccak/src/keccak.rs[tag=main-function]
----

The Rust implementation is shown in <<keccak-rust-listing>>. It is kept as close to the reference implementation as possible. As the reference uses pointers all functions of this direct port need to be marked as unsafe, as it is not possible to safely handle access mutable pointers.

The idiomatic Rust implementation is nearly identical except that it casts the pointer to an array of size 25. This allows the compiler to verify that the array is never accessed out of bounds. It also could allow the compiler to optimize the code better, as it can assume that the array is never accessed out of bounds. The idiomatic Rust implementation can be seen in <<keccak-idiom-listing>>.

To verify whether it is practical to use functions from the Rust ecosystem, the fourth implementation uses the keccak crate from crates.io <<keccak-crate>>. It provides optimized implementations of the keccak sponge functions in different sizes. The tested function, shown in <<keccak-crate-listing>>, just calls the correct implementation from the crate.

==== Results

<<keccak-overview-area>> show the results of the area measurements. The Rust and idiomatic Rust designs seem to be nearly the same size on both optimizations.

.Area of the different keccak designs
:chart-id: id=keccak-overview-area
:vega-lite-filename: processed-charts/keccak_overview_area.vl.json
include::vega-chart.adoc[]

The three LLVM designs that are based on the reference implementation and optimized for speed are also similar in size. This could suggest that LLVM was able to optimize the code in a similar fashion. When optimized for size however the Rust based designs are significantly bigger.

The Rust design from <<keccak-crate>> generates two nearly identically sized designs. They have roughly the same size as the other Rust designs optimized for size.

The GCC design optimized for speed is the biggest design and the one optimized for size is the second smallest. This is what we would expect from the optimization goals.

// Performance
Performance of each design was measured for three testcases. Every testcase is a 25 element array of 64-bit integers. The input in the first testcase was all zeros and for the second all 25s.T The third testcase received the result of the first testcase as its input. The amount of cycles was consistent among the testcases for all designs. <<keccak-cycles>> shows the results of the cycle count measurements.

The 25 inputs need to be read at least once and written at least once as they double as outputs. Accessing memory takes one cycle and has a latency of two cycles, so the theoretical minimum for the number of cycles is 51, if it is not performed inplace. If the algorithm is performed directly in memory the number of memory accesses will be significantly larger.

.Average clock cycles per test of the different keccak designs
:chart-id: id=keccak-cycles
:vega-lite-filename: processed-charts/keccak_average_cycles.vl.json
include::vega-chart.adoc[]

The LLVM designs optimized for speed all perform quite well with the Clang design being the fastest. Interestingly the keccak-crate design optimized for size also performs quite well. Looking at the traces for these five designs reveals that they all perform only the minmum required amount of 50 memory accesses. The trace for the Clang design is shown in <<keccak-clang-speed-trace>>. The traces for the fast Rust designs are similar but they take 48 cycles to do the calculation inbetween the memory accesses. It seems like they require two cycles per round (24 * 2 = 48) while the Clang design only needs one cycle per round but has an additional cycle of overhead (24 + 1).

//TODO: Mention that the crate designs also perform random nonsensical memory accesses.



.Trace of the Clang design optimized for speed
[wavedrom.really-slightly-oversized-content,id="keccak-clang-speed-trace"]
....
include::samples/keccak_clang_speed.wavejson.json[]
....


The generated with the function from crates.io also performs memory reads during the calculation. These memory access are not part of the algorithm and seem to be random. The addresses they try to reach are out of bounds and random values are return during the tests. This does not seem to influence the correctness of the result nor the performance of the calculation. The random reads can be clearly seen in <<keccak-crates-speed-trace>>.

.Trace of the crates.io design optimized for size
[wavedrom.oversized-content,id="keccak-crates-speed-trace"]
....
include::samples/keccak_crates_speed.wavejson.json[]
....

// TODO: Attach snippet from the traces
The other designs all need significantly more cycles. They will be called slow designs. The difference between the fastest slow design and the slowest slow design is roughly factor 10. But even the fastest slow design is 50x slower than the fast designs. The main reason for their high cyclecount is that they do work directly on the memory. The trace for the GCC design optimized for speed (<<keccak-gcc-speed-trace>>) shows that it constantly performs memory accesses. The traces for the other slow designs are similar. The exact cycle count seems to depend on how much of the calculations is performed in external memory.

.Excerpt from the trace of the GCC design optimized for speed
[wavedrom.slightly-oversized-content,id="keccak-gcc-speed-trace"]
....
include::samples/keccak_gcc_speed.wavejson.json[]
....

// Speculation
It could be that the designs optimized for size split up the work in multiple small functions that each load the values from memory and store them again.

The maximum frequency for each keccak design is shown in <<keccak-frequency>>. The fastest designs are tthe Rust designs optimized for speed, except for the crates.io design. It seems like the tradeoff of performing each round of keccak in two cycles instead of one is worth it, as they are able to run at a higher frequency compared to the LLVM design. Notably both crates designs are running at the slowest frequency of all designs. This has probably something to do with the unneccessary memory accesses they perform.
// TODO: Future: Investigate why the crates designs are so slow



.Maximum frequency of the different keccak designs
:chart-id: id=keccak-frequency
:vega-lite-filename: processed-charts/keccak_max_frequency.vl.json
include::vega-chart.adoc[]

The other Rust designs optimized for size run at a lower frequency than the other designs optimized for size. The size-optimized {cpp} designs run at the third and fourth highest frequency.

.Execution time per design
:chart-id: id=keccak-performance
:vega-lite-filename: processed-charts/keccak_performance.vl.json
include::vega-chart.adoc[]

The execution time of the designs optimized for speed is roughly the same. Of them the Rust designs are a bit faster than the LLVM designs. Even though these designs take more cycles their higher clockspeed is able to compensate for that. The crates.io design is the slowest of the fast designs.

The designs optimized for size are all significantly slower. The GCC design optimized for speed is the fastest of the designs that access memory. It is still more than 20 times slower than the next fastest design.

.Keccak area to performance
:vega-lite-filename: processed-charts/keccak_area_performance.vl.json
include::vega-chart.adoc[]

.Keccak space efficiency
:vega-lite-filename: processed-charts/keccak_space_efficiency.vl.json
include::vega-chart.adoc[]

The fast designs offer way more performance per area than the slow designs. While the size-optimized designs generated from {cpp} are the smallest designs overall their abysmal performance makes them not very space efficient. The Rust designs optimized for size are the worst, as they are both slow and large. The Rust designs optimized for speed are the most space efficient designs. The crates.io design is in the middle, as their performance is slightly slower than the other fast designs, and they are as big as the large designs.

// Notes on crates.io design

In case of the crates.io design it did not make any difference whether it was optimized for speed or for size. <<keccak-crates-implementation-listing>> shows that their implementation explicitly inlines and unrolls steps of a round. Besides the explicit unrolling the other big difference of that implementation is that the number of rounds is variable. The Rust compiler seems to respect these optimizations. It generates basically the same LLVM IR for both optimizations. The biggest difference is that the version optimized for speed inlines the control of the main loop into the loop-body.

// TODO: Insert CFG for both LLVM IR files. 
// experiments/rust-hls-experiments/keccak_crates_speed/rust_hls/keccak_hls_synthesized/keccak.ll
// Strip all implementation except for differences from both cfgs.


// TODO:
The LLVM IR generated from the other Rust implementations when optimizing for speed is nearly identical to the LLVM IR generated from the crates.io design. It is peculiar that the crates.io designs are bigger and run at a much slower speed, as they are both based on such similar LLVM IR. The most notable differentiation between them is in accessing the `KECCAK_ROUND_CONSTANTS` array. In each of the 24 rounds a different constant from that array is needed. The array accesses use the same instructions in both cases, albeit in slightly different places. The crates.io implementation fetches the round constant in the middle of the round and the other LLVM IR at the end. This should make no difference as they have the same data dependencies and can be reordered. The crates.io design uses a pointer to an integer `i64*` to refer to the array and the other implementation uses a pointer to an array of known size `[24 x i64]*`.

It can be assumed that this causes bambu to perform poorly for some reason. It could be that the generated design tries to access the array at an outside memory location. This would also explain the 24 memory accesses to out-of-bounds memory locations that are performed during the calculations. These values from external memory are apparently not used during calculation. It seems to be a bug in bambu where the generated design tries to access external memory, when the actual values are in internal memory. It can be assumed that bambu cannot prove that the instruction only accesses the internal memory. However the bug is that it tries to access external memory in addition to the internal memory. It may be that Clang does not generate instructions like this, so the bug did never occur until now.

The crates.io design also performs a check to verify that the number of rounds is not greater than 24 and panics if it is. It is safe to assume that bambu detects that this will never happen in our case and optimizes it away. If bambu did not do that it would not be able to synthesize the design. 
// While the designs are mostly treated as block-boxes, the generated Verilog reveals that the ((((not sure controller or datapath)))) of the cratessio design contain a connection to memory, while it does not for the others.


// TODO: Add md5 back in if there is time left

// === Testing the md5 functions

// Three implementations will be tested for the md5 function. 

// The {cpp} reference implementation is based on a 

// .Area of the different md5 designs
// :vega-lite-filename: processed-charts/md5_overview_area.vl.json
// include::vega-chart.adoc[]

// .Execution time per design
// :vega-lite-filename: processed-charts/md5_performance.vl.json
// include::vega-chart.adoc[]

// .Maximum frequency of the different md5 designs
// :vega-lite-filename: processed-charts/md5_max_frequency.vl.json
// include::vega-chart.adoc[]

// .Average clock cycles per test of the different md5 designs
// :vega-lite-filename: processed-charts/md5_average_cycles.vl.json
// include::vega-chart.adoc[]

// .MD5 Area to performance
// :vega-lite-filename: processed-charts/md5_area_performance.vl.json
// include::vega-chart.adoc[]

// .MD5 Space efficiency
// :vega-lite-filename: processed-charts/md5_space_efficiency.vl.json
// include::vega-chart.adoc[]


== Evaluation

=== Supported features / Limitations

Extern c interfact

no panic, exit, etc

not exhaustive

We have shown that

Discuss limitations

Discuss the results of the performance experiments

Discuss compiler flags? 

// Why is this a solution
// Discuss the results of the experiments

== Conclusion



// Repeat problem
As mentioned previously current HLS tools focus on the common systems-programming languages C or {cpp} as their input language. Rust could provide a more modern alternative if it was supported by HLS tools.

// Really short summary
It was shown that bambu can be used to perform HLS from Rust. The Rust compiler can output LLVM IR which can be used as an input for bambu. When the Rust compiler is optimizing for speed the generated designs are similar in terms of performance and size to the designs generated from {cpp}/Clang. When optimizing for size the results were mixed, some designs were smaller and faster, some were larger and bigger, and some were similar. They showed that the Rust based designs usually perform similar to with a {cpp}/Clang based designs. It was also demonstrated that it does not make a difference whether the Rust specifications are C-like or more Rust-like. An specification using a implementation from crates.io was able to be synthesized with promising results. In that specific case the resulting design was larger than neccessary which is probably caused by a bug in bambu. As expected the tooling is not as mature as for {cpp}. 

// Gotchas when using Rust
There are multiple challenges when using the Rust compiler to generate LLVM IR that is compatible with bambu. Bambu does not support all instructions so the Rust compiler needs to be configured to only generate correct IR. Bambus LLVM support is focused on LLVM IR generated by Clang. As a result of this bambu is not well tested on LLVM IR generated by other tools. While bambu is able to process and compile them the resulting designs might not be as optimized as they are with Clang. This can lead to bugs like the one described in <<insert-id>>. Rust also performs bounds checks on all memory accesses that could overflow. If these bound-checks cannot be removed during optimization the resulting LLVM IR contains calls to panic which cannot be synthesized by bambu. By default Rust also performs overflow checks on all integer arithmetics, which also can result in panics. They can be disabled. Another limitation is that only Rust functions with a C-compatible interface can be synthesized.

// Toolchain
// TODO: The second sentence is shit.
To make experimentation with HLS from Rust easier, a framework for integrating the toolchain with the Rust based HDL rust_hdl was developed. It is available as a rust crate named rust_hls. The framework makes it possible to write HLS specifications and the HDL designs that used the HLS designs generated from the specifications in the same project. It also enables to write tests for the specification and the generateted design side-by-side as Rust unit tests. It achieves this by searching the project for modules marked as HLS specifications, running the toolchain for the found specifications, and embedding the generated modules into the original project. If the a functions marked as HLS specification shows obvious problems a error will be emitted before synthesis is performed. It also allows individual configuration of the toolchain for each specification. 

== Future work


// How can the Rust compiler be made to generate better LLVM IR for HLS?

// Measure more test cases

// Test out the limitations of the toolchain

// 



// Can it be better than CPP when the tools are adjusted for Rust LLVM?
Future work on HLS from Rust should focus on improving support for Rust generated LLVM IR in HLS tools. While bambu currently is able to synthesize the LLVM IR generated by Rust in some cases it seems possible to improve the generated designs. It should be possible to find these cases and improve bambu to handle them better.

It would also be interesting to see if the LLVM IR generated by Rust can be improved to generate better designs. Our toolchain builds every dependency of the project seperated and then links them together. This is not ideal as it does not allow the compiler to perform cross-crate link-time optimizations. This would probably require to adjust cargo or the Rust compiler to allow link-time optimizations when compiling to LLVM IR. The toolchain only adds the flags to the Rust compiler that are absolutly necessary to generate LLVM IR that can be synthesized by bambu. Besides that it uses rustcs default optimization profiles for speed and size. There are probably significant improvements possible by evaluating which optimizations are useful for HLS and which are not.


[glossary]
== List of abbreviations
// Abbreviations from here will automatically be linked into the document

// Abbreviations in a random order and links to read more about them
[glossary]
[[FPGA]]FPGA:: Field-programmable gate array link:pass:[https://en.wikipedia.org/wiki/Field-programmable_gate_array][🔗^]
[[HLS]]HLS:: High-level synthesis link:pass:[https://en.wikipedia.org/wiki/High-level_synthesis][🔗^]
[[HDL]]HDL:: Hardware description language link:pass:[https://en.wikipedia.org/wiki/Hardware_description_language][🔗^]
[[ADL]]ADL:: Accelerator design language link:pass:[https://www.sigarch.org/hdl-to-adl/][🔗^]
[[GPU]]GPU:: Graphics processing unit link:pass:[https://en.wikipedia.org/wiki/Graphics_processing_unit][🔗^]
[[LLVM_IR]]LLVM IR:: LLVM intermediate representation link:pass:[https://en.wikipedia.org/wiki/LLVM#Intermediate_representation][🔗^]
[[RTL]]RTL:: Register-transfer level link:pass:[https://en.wikipedia.org/wiki/Register-transfer_level][🔗^]
[[DUT]]DUT:: Design/Device under test link:pass:[https://en.wikipedia.org/wiki/Test_bench][🔗^]
[[ASIC]]ASIC:: Application specific integrated circuit
[[QoR]]QoR:: Quality of results
[[CPU]]CPU:: Central processing unit
[[LUT]]LUT:: Look-up table
[[FF]]FF:: Flip-Flop
[[DFF]]DFF:: D Flip-Flop
[[BRAM]]BRAM:: Block RAM
[[DSP]]DSP:: Digital signal processor 
[[CLB]]CLB:: Configurable logic block
[[LB]]LB:: Logic block
[[LE]]LE:: Logic element
[[RAII]]RAII:: Resource Acquisition is Initialization / Scope-Bound Resource Management
[[HIR]]HIR:: High-Level Intermediate Representation link:pass:[https://rustc-dev-guide.rust-lang.org/hir.html][🔗^]
[[THIR]]THIR:: Typed HIR link:pass:[https://rustc-dev-guide.rust-lang.org/thir.html][🔗^]
[[MIR]]MIR:: Mid-level Intermediate Representation link:pass:[https://rustc-dev-guide.rust-lang.org/mir/index.html][🔗^]
[[PAL]]PAL:: Programmable array logic
[[CFG]]CFG:: Control-flow graph link:pass:[https://en.wikipedia.org/wiki/Control-flow_graph][🔗^]


[bibliography]
== References

// Claims to have a transpiler from a subset of Rust (RAR) to restriceted algrithmic C (RAC) that can be synthesized to FPGA. No source.
// The first paper to mention HLS from Rust. 
* [[[Har22]]]
Hardin, David,
_Hardware/Software Co-Assurance using the Rust Programming Language and ACL2_,
arXiv preprint arXiv:2205.11709,
2022.
link:pass:[https://arxiv.org/abs/2205.11709v1][🔗^]

* [[[Rog20]]]
Rogers, Samuel and Slycord, Joshua and Baharani, Mohammadreza and Tabkhi, Hamed,
_gem5-SALAM: A System Architecture for LLVM-based Accelerator Modeling_,
2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 471-482,
2020.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9251937][🔗^]

* [[[Li21]]]
Li, Rui and Berkley, Lincoln and Yang, Yihang and Manohar, Rajit,
_Fluid: An Asynchronous High-level Synthesis Tool for Complex Program Structures_,
2021 27th IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC), 1-8,
2020.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9565447][🔗^]

* [[[Lia23]]]
Liang, Geng-Ming and Yuan, Chuan-Yue and Yuan, Meng-Shiun and Chen, Tai-Liang and Chen, Kuan-Hsun and Lee, Jenq-Kuen,
_The Support of MLIR HLS Adaptor for LLVM IR_,
Workshop Proceedings of the 51st International Conference on Parallel Processing, 1-8,
2020.
link:pass:[https://doi.org/10.1145/3547276.3548515][🔗^]

// Bambu provides a research environment to experiment with new ideas across HLS, high-level verification and debugging.
// Bambu input: standart C/c++ specifications, LLVM IR, IRs from GCC
// Includes many optimizations
// Makes it easy to integrate new transformations and optimizations
// Is open-source
// Bambu is a command line tool
// Supports most C/C++ constructs
// Bambu has three phases. frontend, middleend and backend
// Frontend: Uses clang or gcc
// Uses a compiler plugin for both to extract the call graph and control flow information
// Builds its own static single assignment IR
// This decouples the compiler frontend from the rest of the HLS process.
// Vivado HLS has a frontend based on clang
// Middle end:
// Bambu rebuild the call graph and control data flow graph and adds own data structures.
// Applies a set of analyses and transformations.
// Including common software compiliation optimizations
// Including target specific transformations. Such as replacing multiplication and divisions with constants with shift and add operations.
// Can exploit custom sized operators
// TODO: Explore if bambu can use the rust crate for more integer sizes like u21
// Bambu performs bitwidth and range analysis to minimize bit width
// Backend: Bambu performs the actual architectural synthesis here
// The synthesis process acts on every function individually
// Every function has at least two parts: control ogic and data path
// Control logic is a FSM
// Control logic handles the routing of data values and temporal execution of ops
// Bambu steps:
// * Function allocation
// Bambu has a technology library for standart system libraries such as libm or libs
// This step associates High level functions with hardware resources
// Bambu supports sharing functions across module boundries
// * Memory allocation
// Defines momories to store variables.
// Defines how dynamic memory is implemented
// Memories in bambu can be classified as read-only, local, with aligned or unaligned memory access
// Bambu supports accessing protocol based memories
// * Resource allocation
// Maps operations (not mapped onto functions) onto resource units
// Resource units are available in the bambus resource library
// Floating point operations are supported through generating soft floating point cores
// Rich resource library with multiple implementations for the same operation
// Resource library annotated with latency and resource occupation.
// * Scheduling
// Bambu uses List scheduling
// every operation has a priority
// a operation is ready when its deps have been satisfied
// ready operations can be scheduled if the resources are availabe
// Multiple compenting for a resource: higher priority
// Also has a speculative scheudling algo
// * binding
// Pretty much standart
// Bambu considers how profitable it is for two operations to share the same resource
// Resources that occupy a big area are more likely to be shared
// * netlist generation
// Translates the architecture in a RTL description
// In Verilog or VHDL
//
// Research topics for bambu: "They range from parallelized hardware accelerator design, dynamic scheduling, verification and de- bugging, design exploration of the compilation flow, machine learning accelerator design, IR development, and integration with logic synthesis tools" , MLIR
// MLIR dialects that can be translated to LLVM IR
* [[[Fer21]]]
+F. Ferrandi et al.+,
_Invited: Bambu: an Open-Source Research Framework for the High-Level Synthesis of Complex Applications_,
2021 58th ACM/IEEE Design Automation Conference (DAC), 1327-1330,
2021.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9586110][🔗^]
link:pass:[https://re.public.polimi.it/retrieve/668507/dac21_bambu.pdf][📁^]

* [[[Rot10]]]
+Nadav Rotem,+
_C-to-Verilog. com: High-Level Synthesis Using LLVM_,
University of Haifa,
2010.
link:pass:[https://llvm.org/devmtg/2010-11/Rotem-CToVerilog.pdf][🔗^]

* [[[Sch20]]]
Fabian Schuiki, Andreas Kurth, Tobias Grosser, and Luca Benini,
_LLHD: a multi-level intermediate representation for hardware description languages_,
In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2020), 258-271,
2020.
link:pass:[https://doi.org/10.1145/3385412.3386024][🔗^]

// Multiple HLS tools use LLVM
// C/Cpp are most popular languages for HLS
// NOTE: I focused on FPGA descriptions
// Clock frequency scaling in CPU stalled around 2005
// A alternative approach for high-throughput and energy efficient processing is to use specific accelerators
// Specialized accelerators are hard to design and program
// RTL requires advanced hardware expertise
// RTL specifies cycle-by-cycle behavior explicitly
// RTL is a low-level abstraction
// RTL leads to longer development times
// FPGAs with HLS can reduce that.
// FPGAs are configurable integrated circuits
// Most FPGAs are reconfigurable
// FPGA vendors provide toolchains to synthesize HTL to bitstream
// bitstream gets programmed to the FPGA
// HLS tools start from a HLL and automatically produce a circuit specification in RTL
// HLS offers enable software engineers to benefit from the performance and energy efficiency of hardware without having hardware expertise
// HLS tools enable hardware engineers to design systems faster
// HLS tools enable hardware engineers to rapidly explore the desing space
// Microsoft uses FPGAs for accelerating bing search
//
// 
* [[[Nan16]]]
+R. Nane et al.+,
_A Survey and Evaluation of FPGA High-Level Synthesis Tools_,
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 1591-1604,
2016.
link:pass:[https://ieeexplore.ieee.org/abstract/document/7368920][🔗^]
link:pass:[https://sci-hub.st/10.1109/tcad.2015.2513673][📁^]

* [[[Nor18]]]
+D. H. Noronha, B. Salehpour and S. J. E. Wilton+,
_LeFlow: Enabling Flexible FPGA High-Level Synthesis of Tensorflow Deep Neural Networks_,
Fifth International Workshop on FPGAs for Software Programmers, 1-8,
2018.
link:pass:[https://ieeexplore.ieee.org/abstract/document/8470462][🔗^]

* [[[Soz22]]]
Sozzo, Emanuele Del, et al.,
_Pushing the level of abstraction of digital system design: A survey on how to program FPGAs_,
ACM Computing Surveys, 1-48,
2022.
link:pass:[https://dl.acm.org/doi/abs/10.1145/3532989][🔗^]

* [[[XLS]]]
_XLS project page_
link:pass:[https://google.github.io/xls/][🔗^]

* [[[DSLX]]]
_DSLX Reference_
link:pass:[https://google.github.io/xls/dslx_reference/][🔗^]

* [[[rusthdl]]]
_rust-hdl project overview_
link:pass:[https://github.com/samitbasu/rust-hdl][🔗^]


* [[[Zen12]]]
_Identifying Barriers to Adoption for Rust through Online Discourse_
link:pass:[https://arxiv.org/pdf/1901.01001.pdf][🔗^]

* [[[so-trends]]]
_Stack Overflow Trends_
https://insights.stackoverflow.com/trends?tags=rust%2Cc%2B%2B

// Rust has an ecosystem that greatly simplifies any software projec
// Rust is great
// Rust has been the "most loved" language since 2016
// Rust is meant to supersede C/C++
// Rust focus is on saftey and performance
// Libraries exist for any need you may have
// Dependencies can be installed using the official cargo tool
// Rust is the first industry-supported computer programming language to overcome the longstanding trade-off between the control over resource management provided by lower-level languages for systems programming, and the safety guarantees of higher-level languages
// Rust enables many common systems programming pitfalls to be detected at compiletime
// Rust surpasses all other common memory-safe languages in terms of performance
// Rust has data-race prevention
// Considering performance Rust is one of the best languages
// Considering saftey Rust is the best language
// Rust offers many modern features that the more established systems languages tend to lack.
// Cargo is the package manager for Rust
// Cargo is the build system for Rust
// Cargo facilitates downloading and building dependencies
// Cargo facilitates unit testing and integration testing
// Cargo facilitates benchmarking
// Cargo facilitates build management with different profiles
// Cargo facilitates documentation generation from comments
// Rustfmt facilitates code formatting
// Dependency management is handled with a configuration file
// Dependencies are automatically installed during compilation
// Dependencies can be easliy found on the official community crates registry
// Cargo allows to view unified documentation for all dependencies
// Unit tests are written in the same file as the code they test
// Benchmarking is done in a similar fashion to unit testing
// The tooling alone makes Rust a much better development experience than most systems languages
// The tooling is most likely a considerable contributor to its rise.
// Rust is approaching the status of a mainstream language in health informatics applications
// The criticisms of Rust tend to originate from its lack of maturity
// C and C++ are well-adopted and much more established in the industry than Rust
// lack of demand for Rust developers in the market
* [[[Bug22]]]
+William Bugden, Ayman Alahmar+,
_Rust: The Programming Language for Safety and Performance_,
asXiv,
2022.
link:pass:[https://arxiv.org/pdf/2206.05503.pdf][🔗^]

// Rust can be used for GPU programming
* [[[Byc22]]]
+Andrey Bychkov, Vsevolod Nikolskiy+,
_Rust Language for GPU Programming_,
In: Voevodin, V., Sobolev, S., Yakobovskiy, M., Shagaliev, R. (eds) Supercomputing. RuSCDays 2022. Lecture Notes in Computer Science, vol 13708. Springer, Cham, 2022, pp. 522-32,
2022
link:pass:[https://doi.org/10.1007/978-3-031-22941-1_38][🔗^]

// Rust can be used for web programming
* [[[Kyr22]]]
+Kyriakou K-ID, Tselikas ND+,
_Complementing JavaScript in High-Performance Node.js and Web Applications with Rust and WebAssembly._,
Electronics 11, no. 19: 3217,
2022
link:pass:[https://doi.org/10.3390/electronics11193217][🔗^]

// Probably one of the greatest features of the language is the package manager, called cargo.
// Rust is a high-level language
// Rust is very efficient in terms of performance
// Rust is based on the principle of zero cost abstractions
// Rust provides a memory safety mechanism without using a garbage collector called the borrow checker
// Rust is a strongly typed language
// Rust is provides out of the box a package manager used for importing dependencies, building and distribution of a project.
// If a variable is declared in a specific context it will be freed when the context is over.
// The ownership of a variable can be passed to another context
// More basic description of Rust ownership stuff, will skip that for now
// Development in {cpp} on a production level requires the use of additional tools such as CMake, Make, etc. This adds a layer of complexity.
// Rust has mandatory tooling for building, distributing and depending on a project
// In Rust only a manifest file is needed to configure the project for any scenario possible
// Rust can be compiled to webassembly
// Rust is more energy efficient than any other language expect C for IoT applications
// Rust is faster than any other language expect C for IoT applications
// Rust can easily integrate with C or C++ code
// Rust solves the problem of memory safety without using a garbage collector
// Microsoft states that 70% of security flaws discovered in their systems are related to memory safety
* [[[Cos19]]]
+Cosmin Cartas+,
_Rust - The Programming Language for Every Industry_,
ECONOMY INFORMATICS JOURNAL, 19, 45-51,
2019
link:pass:[https://doi.org/10.12948/ei2019.01.05][🔗^]

// state-of-art bottom-up logic programming within the Rust ecosystem
* [[[Sah22]]]
+Arash Sahebolamri, Thomas Gilray, Kristopher Micinski+,
_Seamless Deductive Inference via Macros_,
Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction, 77-88,
2022
link:pass:[https://doi.org/10.1145/3497776.3517779][🔗^]

// Productivity in HLS is better than HDL
// HLS offers easier design and testing
// HDL implementation is better than HLS
* [[[Mil20]]]
+Roberto Millón, Emmanuel Frati, Enzo Rucci+,
_A Comparative Study between HLS and HDL on SoC for Image Processing Applications_,
Revista elektron, Vol. 4, No. 2, 100-106,
2020
link:pass:[https://doi.org/10.37537/rev.elektron.4.2.117.2020][🔗^]
http://elektron.fi.uba.ar/index.php/elektron/article/view/117/219[📁^]

// Describing the traditional HDL design flow (in 1996)
// TODO: Find newer source
* [[[Smi96]]]
+Douglas J. Smith+,
_VHDL & Verilog compared & contrasted—plus modeled example written in VHDL, Verilog and C._,
In Proceedings of the 33rd annual Design Automation Conference, pp. 771-776,
1996
link:pass:[https://dl.acm.org/doi/pdf/10.1145/240518.240664][🔗^]

// 
* [[[Fla20]]]
+Peter Flake, Phil Moorby, Steve Golson, Arturo Salz, and Simon J. Davidmann+,
_Verilog HDL and its ancestors and descendants._,
Proc. ACM Program. Lang. 4, no. HOPL (2020): 87-1,
2020
link:pass:[https://www.researchgate.net/profile/Arturo-Salz-2/publication/342137214_Verilog_HDL_and_its_ancestors_and_descendants/links/613fc7b45d9d0e131b427dbb/Verilog-HDL-and-its-ancestors-and-descendants.pdf][🔗^]

* [[[intel-hls]]]
_Intel® High Level Synthesis Compiler_
https://www.intel.de/content/www/de/de/software/programmable/quartus-prime/hls-compiler.html

* [[[hdl-to-adl]]]
_From Hardware Description Languages to Accelerator Design Languages_
https://www.sigarch.org/hdl-to-adl/


// Survey literature from 2010 to 2016
// Probably the best comparison of HLS and RTL
// ALso the newest
// Shows that the quality of results of RTL is better than that of HLS
// Shows that development time with HLS is a third of that the RTL flow
// Shows that the productivity of a designer is over four times higher with HLS than with RTL
// Vivado HLS is the most common HLS tool. At least it is used significantly more than any other HLS tool in papers.
// Xilinx is the leading FPGA vendor
// FPGAs are made of configurable logic blocks (CLB, different vendors, different names).
// The CLBs are connected with programmable interconnects.
// The CLBs consist of a few logic cells, logic elements, or adaptive logic modules (ALM) (LC, LE and ALM are the same. Different vendors, different names).
// Logic cells are made of a comination of programmable look-up tables (LUTs) and flip-flops (FFs).
// FPGAs can also have other resources, but these are vendor specific. Most commonly DSP blocks and BRAM blocks.
// There are 4 performance metrics that are commonly used to compare HLS and RTL: performance, execution time, latency, maximum frequency
// For project bigger than 250 lines of code HLS also needs fewer lines of code than RTL
// Reduction in development time for HLS seems independet of project size.
// On average HLS uses 41% more basic FPGA resources than RTL
// The usage of advanced FPGA resources of HLS is similar to RTL
// C based languages are the most common, then OpenCL based, then high-level language based.
// CUDA/OpenCL based HLS is especially resource consuming and has the worst performance
// The performance of HLS designs is similar to the performance of RTL designs.
// THe only example in academia where the development time of HLS was more than RTL was when the developer had to learn the HLS tool in the process.
// Only looks at small to medium designs, 50-500 lines of code
// It is easier to adopt HLS than RTL for people who have experience in software design
// HLS allows for efficient behavioral verification
// The HLS output must still be verified for non-behavioral aspects. This traditional verification is difficult, because there is no direct relationship to the source code.
// HLS halves verification time in many cases
// HLS is a particularly good choice when time to market is a dominant issue and there is no compelling need to gain the ultimate performance or smallest resource usage for the product
// There is no standart example to compare HLS and RTL
* [[[Lah19]]]
+Sakari Lahti, Panu Sjövall, Jarno Vanne, Timo D. Hämäläinen+,
_Are We There Yet? A Study on the State of High-Level Synthesis_,
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 38, no. 5, pp. 898-911,
2019
link:pass:[https://doi.org/10.1109/TCAD.2018.2834439][🔗^]
link:pass:[https://sci-hub.st/10.1109/tcad.2018.2834439][📁^]


// Studied Rust’s ownership discipline in the presence of unsafe code.
// Shows that various important Rust libraries with unsafe implementations, many of them involving interior mutability, are safely encapsulated by their type
// NOTE: Did only read abstract and conclusion
* [[[Jun17]]]
+Ralf Jung, Jacques-Henri Jourdan, Robbert Krebbers, Derek Dreyer+,
_RustBelt: Securing the Foundations of the Rust Programming Language_,
Proc. ACM Program. Lang. 2, POPL, Article 66 (January 2018), 34 pages.,
2017
link:pass:[https://doi.org/10.1145/3158154][🔗^]

// Cpp uses RAII
// "In particular, a programmer can choose to write a low-level-C style and/or violate every rule of good programming. That is not my topic here."
* [[[Str12]]]
+Bjarne Stroustrup+,
_Foundations of C++_,
Programming Languages and Systems. ESOP. Springer, pp. 1-25,
2012
link:pass:[https://doi.org/10.1007/978-3-642-28869-2_1][🔗^]

* [[[Kla23]]]
+Steve Klabnik, Carol Nichols+,
_The Rust programming language_,
No Starch Press,
2023
link:pass:[https://doc.rust-lang.org/book/foreword.html][🔗^]

// Rust compiler has multiple intermediate representations (IRs)
// * MIR (Mid-level IR)
// * HIR (High-level IR)
// * THIR (Typed HIR)
// * LLVM IR
// Typechecking happens on HIR
// Optimization happens on MIR
// MIR is a typed SSA
// Borrowchecking happens at MIR level
// Optimizations also happen in LLVM
// LLVM is used as the backend
// LLVM can generate machine code for many architectures
// LLVM is a collection of modular and reusable compiler and toolchain technologies
// LLVM contains a pluggable compiler backend used by rustc and clang
// Clang is a C compiler
// LLVM takes LLVM IR
// Rust compiler uses LLVM because
// * They don't have to write their own backend. Reduces implementation and maintenance effort.
// * Benefit from the large suite of advanced optimizations that LLVM provides
// * Rust can be compiled to any of the platforms that LLVM supports.
// * Community benfits. Things like spectre and meltdown only need to be fixed in LLVM and many compilers benfits from that
// rustc groups LLVM IR into "modules" known as codegen units
// Rustc can use LLVM to codegen multiple of these modules in parallel utilizing multiple CPU cores
// The resulting object files are then linked together by the linker
* [[[rustc-guide]]]
_Rust Compiler Development Guide_,
2023
link:pass:[https://rustc-dev-guide.rust-lang.org/backend/codegen.html][🔗^]
link:pass:[https://rustc-dev-guide.rust-lang.org/mir/optimizations.html][🔗^]

// LLVM is a compiler framework
// Defines a a low-level code representation in static single assignment (SSA) form
// LLVM IR
// Describes a program using an abstract RISC-like instruction set with higher level information
// LLVM IR contains type information
// LLVM IR contains explicit control flow graphs
// LLVM IR contains explicit dataflow representation (using SSA)
// Has a low-level, language independent type system
// Has instructions for performing type conversions and low-level address arithmetic while preserving type information.
// Low-level exception handling instructions
// LLVM is not intended to be a universal compiler IR
// does not represent high-level language features directly
// LLVM has no notion of highlevel constructs such as classes, inheritance, or exceptionhandling semantics
// LLVM does not specify a runtime system or particular object model
// "Type information captured by LLVM is enough to safely perform a number of aggressive transformations that would traditionally be attempted only on type-safe languages in source-level compilers"
// NOTE: I skipped section 2
// "The goal of the LLVM compiler framework is to enable sophisticated transformations at link-time, install-time, runtime, and idle-time, by operating on the LLVM representation of a program at all stages"
// static compiler front-ends emit code in the LLVM representation
// combined by the LLVM linker
// Linker performs a variety of link time optimizations
// The resulting code is then translated to native code for a given target.
// Language specific optimizations must be performed in the frontend
// External static LLVM compilers are known as front-ends
// Frontends translate source language programs into LLVM IR
// Can perform aggressive interprocedural optimizations across the entire program
// Some of the interprocedural optimizations are:  inlining, dead global elimination, dead argument elimination, dead type elimination, constant propagation, array bounds check elimination, simple structure field reordering, and Automatic Pool Allocation
// Uses code generator backends to translate LLVM IR into native code for a given target
* [[[Lat04]]]
+C. Lattner, V. Adve+,
_LLVM: a compilation framework for lifelong program analysis & transformation_,
International Symposium on Code Generation and Optimization, 2004. CGO 2004., San Jose, CA, USA, 2004, pp. 75-86
link:pass:[https://doi.org/10.1109/CGO.2004.1281665][🔗^]
link:pass:[https://sci-hub.st/10.1109/cgo.2004.1281665][📁^]
// Shows that HLS is twice as fast as HDL 
// M. Pelcat, C. Bourrasset, L. Maggiani and F. Berry, "Design productivity of a high level synthesis compiler versus HDL," 2016 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS), Agios Konstantinos, Greece, 2016, pp. 140-147, doi: 10.1109/SAMOS.2016.7818341.
// https://ieeexplore.ieee.org/abstract/document/7818341

// FPGA inception 30 years ago
// FPGAs bring faster design cycles then custom chips
// FPGAs lower dev cost then custom chips
// low-level hardware recnfigurability
// FPGA architecutre offers many design choices
// FPGAs consist of different types of programmable blocks
// "FPGAs are recofigurable computer chips that can be programmed to implement any digital circuit"
// prefabricated routing tracks with programmable switches
// Functionality of all FPGA blocks is controlled by SRAM cells
// Milloions of SRAM cells
// HDL is converted to bitstream
// Bitstream is used to programm all configuration SRAM cells
// Lower NRE cost then ASICs
// Shorter time to market then ASICs
// off the shelf FPGA can be used to implement design in a matter of weeks
// Skipping phisical design, layout, fabrication and verification
// Allow continous hardware upgrades by loading new bitstreams in the field
// Considered a compelling solution for small and medium sized designs 
// Exact hardware for every application
// Exact datapath width, pipeline stages, parallel units as required
// Can achieve higher efficiency than CPU or GPU
// Can implement instruction-free streaming hardware
// Can implement a custom instruction set
// Adopted in many domains.
// "adoption of FPGAs in many application domains including wireless communi- cations, embedded signal processing, networking, ASIC prototyping, high-frequency trading, and many more"
// Deployed on large scale in datacenters, packet processing, machine learning
// Lower efficiency than ASICs
// FPGA on average 35 times larger than ASIC implementation
// FPGA on average 4 times slower than ASIC implementation
// For designs that utilize other FPGA blocks the gap is smaller, still 9 times large
// FPGA architects seek to reduce gap while maintaining programmability
// Early FPGAs ere simple arrays of logic blocks
// Modern FPGAs are complex heterogeneous architectures that have more block types
// Modern FPGA have blocks like BRAM, DSP, processors, external interfaces
// FPGA architectures are evaluated based on the efficiency implementing a wide variety of designs
// There are academic test suites for evaluating FPGA architectures
// VTR is a CAD system for layouting designs on FPGAs
// CAD system applies a series of complex optimizations 
// CAD system converts RTL design to netlist.
// CAD system maps netlist to FPGA blocks
// CAD system places blocks on FPGA and routes the connections between them
// CAD system outputs bitstream implementation
// total area is a key metric
// "Total area is the sum of the areas of the FPGA blocks used by the application, along with the programmable routing included with them."
// Timing analyzer finds the critical path through blocks and routing
// Critical path limits maximum clock frequency
// Power consumption is estimated based on resources used and signal toggle rate
// _hardened_ blocks are blocks that are implemented as ASICs
// What functionality to harden is a design choice
// What area of the FPGA to use for hardened blocks is a design choice
// Hardened blocks can still have some level of configurability
// How flexible the hardened blocks are is a design choice
// Hardened blocks are faster, smaller and more power efficient than programmable blocks
// Tradeoff between flexibility and efficiency
// Unused hardened blocks are wasted silicon
// Problems with slow routing to hardened blocks, if they are far away
// PAL first reconfigurable computing devices
// PAL does not scale well, area increases quadratically with IO size
// CPLD includes mulitple PALs and programmable routing in a package
// 1984 Xilinx pioneers first LUT based FPGA
// SRAM based LUTs with interconnects between them
// Scales well
// Much higher area efficiency than and/or based designs
// LUTs form the fundamental logic element in all commercial FPGAs
// Alternative designs perform worse than LUTs
// K-LUT implements a K-input LUT
// K-LUT stores truth table in SRAM cells,
// K input signals are used as multiplexer to select line
// truth table contains 2^K values
// A basic logic element (BLE) is a K-LUT with an output register
// A BLE can implement DFF or a K-LUT
// A BLE has K inputs and 2 outputs, one for routing and one for feedback inside the LE
// Logic blocks are composed of multiple (N) BLEs
// Logic blocks have a local interconnect
// The local interconnect connects the inputs of the LB and the feedback outputs of the BLEs to the inputs of the BLEs.
// The local interconnect is often a arranged as a local full or partial crossbar.
// See figure 4 in the paper.
// Over time K and N have increased.
// More K means more functionality in a single LUT
// More K leads to less logic in the critical path
// More N means less demand for fast inter-LB routing
// The area of the LUT increases exponentially with K as more SRAM cells are needed (2^K)
// More K linearly degrades the speed of the LUT
// if the local interconnect is a crossbbar, its size increases quadratically with N
// if the local interconnect is a crossbbar, its size decreases linearly with N
// Empirically the best size for K is 4-6 and for N it is 3-10
// First LUT-based FPGA from Xilinx: N = 2, K = 3
// around 2000: 4-LUTs common
// Study: 4-LUTs vs 6-LUTS: 6-LUTS:14% more perf, 17% bigger
// Fracturable LUTs can be broken down into smaller LUTs, but limitations like shared inputs
// FPGA architectures from Xilinx and Altera converge to relatively large LBs wth 8 and 10 N
// Future designs even bigger LBs
// inter-LB wire delay scales poorly with process shrink
// Larger LB sizes can lead to faster CAD tool runtimes
// Modern FPGAs have more than 1 FFs per BLE
// Even though there are optimization the core ideas stayed similiar
// 22% of logic elements in FPGAs are implementing arithmetic
// These operations can be implemented with LUTs but are inefficient
// A ripple carry adder requires 2 * the number of bits LUTs
// This leads to high logic utilization and long critical paths
// All modern FPGAs include hardened arithmetic circuitry in their logic blocks
// How the arithmetic is accelerated is a design choice
// Can be a dedicated adder between two LUTs
// Can be just a fastpath for the carry bit
// At least 3x faster than LUT based implementations
// NOTE: there is more detail on the different types of arithmetic optimizations in the paper
// Recently deeplearning has become a key workload
// Deeplearning has multiply-accumulate operations at its core, which could benefit from hardened bigger hardened arithmetic
// Programmabble routing is over 50% of the area of an FPGA
// Programmable routing accounts for over half the critical path delay
// High multiplier density in signal processing and communication applications
// Main design philosophy of DSP block is to minimize the number of soft logic used to implement common DSP algorithms
// FPGA CAD tools will automatically map multiplication to DSP blocks
// Bigger FPGA designs always always require a memory buffer
// Making soft memory out of LUTs is over 100x less dense than SRAM cells
// Modern FPGAs are about 25% BRAM
* [[[Bot21]]]
+Andrew Boutros, Betz Vaughn+,
_FPGA architecture: Principles and progression_,
IEEE Circuits and Systems Magazine 21.2 (2021): 4-29.,
2021
link:pass:[https://doi.org/10.1109/MCAS.2021.3071607][🔗^]
link:pass:[https://sci-hub.st/10.1109/MCAS.2021.3071607][📁^]

// TODO: This is an application note, how to cite?
// TODO: Especially Cri in the link is wrong
// TODO: Source for one definition. Necessary?
// A critical path is a path in the design which must meet certain critical timing requirements in order for the system to function properly
* [[[Cri95]]]
_Critical path analysis for field-programmable gate arrays_,
Microprocessors and Microsystems, Volume 19, Issue 7, Pages 435-439,
1995
link:pass:[https://doi.org/10.1016/0141-9331(95)90010-1][🔗^]
link:pass:[https://sci-hub.st/10.1016/0141-9331(95)90010-1][📁^]

// Compares RTL/HDL to assembly
// High level languages improved productivity
// HDL have enabled wide adoption of simulation tools
// First HLS tools 1990s
// In the 2000s: shift to electronic system level (ESL) paradigm that facilitates exploration sythesis and verification of complex SoCs
// Intro of first languages with system-level abstraction like SystemC or SystemVerilog
// 2000s transaction level modeling
// ESL paradigm shift caused by rising system complexities
// HLS reduced time for creating hardware
// HLS reduced time for verification
// HLS enables the reuse of the same specification for different targets (ASICs, FPGAs, different ASICS and FPGAs)
// functional specification = untimed highlevel description
// NOTE: contains more info about HLS design flow
// HLS tools transform an untimed specification into a fully timed implementation
// HLS tools generate custom architecture to efficiently implement the specification
// HLS tools generate a RTL implementation
// DIAGRAM: Highlevel synthesis design steps
// Generated architecture (usually) consists of a datapath and a controller.
// Generated architecture also has memorybanks  and communications interfactes.
// HLS tools usually perform 7 tasks:
// 1. Compiling the specification
// 2. Allocating/Creating hardware resources
// 3. Scheduling operations to clock cycles
// 4. Binding operations to functional units
// 5. Binding variables to storage elements
// 6. Binding transfers to connection units
// 7. Generating the RTL architecture
// Steps 2-6 are called interdependent
// Compiling transforms the specification into a formal description
// Compiling performs optimizations
// Formal model clasically exhibits data and control dependencies.
// Data flow graph: Every operation is a node and the edges are values (input, temporary and output)
// A pure data flow graph (DFG) models data flow only
// In some cases a pure DFG can be created. Can be done by completly unrolling loops and multiplexing conditional assignments.
// Pure DFG is big and impractical
// Cannot support unbounded iteration and nonstatic control flow (like goto)
// control and data flow graph (CDFG) models data and control flow
// CDFG nodes are called basic blocks and are a straight sequence of statements
// CDFG edges can be conditional and represent if or switch constructs.
// CDFGs are more expressive ecause the can represent loops with unbounded iteration (those that cannot be unrolled)
// Allocation defines the types and number of resources the are needed to satisfy the design constraints
// Resources are functional units, storage elements and communication interfaces
// HLS tools have a RTL component library with basic resources.
// Scheduling determines which operations run in which clock cycle.
// All operations must be scheduled into cycles
// If there are no data dependencies between operations they can be scheduled in parallel
// Every variable that carries values over multiple cycles must be bound to a storage element
// Variables with nonoverlapping lifetimes can be bound to the same storage element
// Every operation must be bound to a functional unit that can perform the operation
// Every connection between functional units and storage element must be bound to a connection unit
// After allocation, scheduling and binding the RTL architecture can be generated and output
// The architecture is classically includes a controller and a data path
// storeage elements: registers, memories, ...
// functional units: ALUs, multipliers, DSPs, other custom functions, ...
// connection units: buses, tristate drivers, multiplexers, ...
// The data path consists of the the storage elements, functional units and connection units
// All these components can be connected arbitrarily through buses
// They can also be pipelined
// The controller is a finite state machine (FSM)
// The controller orchestrates the data path by setting values of control signals of the data path
// The inputs of the controller can come from primary inputs or from the data path
// The controller consists of three parts: the next state logic, a state register and the output logic
// The next state logic computes the next state of the FSM from the current state and the inputs
// The state register contains the current state of the controller
// The output logic sets the control signals according to the current state
// The output logic also sets control outputs that can be used as inputs for the data path
// The controller is usually build with hardwired logic, but can be more complex with memories and such
// If the controller is more complex it is usually a custom processor.
// The state register is then call program counter. This shows that it is just a processor.
// NOTE: Only read until "Several design flows"
* [[[Cou09]]]
+Philippe Coussy, Daniel D. Gajski, Michael Meredith, Andres Takach+,
_An Introduction to High-Level Synthesis_,
IEEE Design & Test of Computers, Volume 26, Issue 4, Pages 8-17,
2009
link:pass:[https://doi.org/10.1109/MDT.2009.69][🔗^]
link:pass:[https://sci-hub.st/10.1109/MDT.2009.69][📁^]

* [[[Ber09]]]
+Guido Bertoni, Joan Daemen, Michaël Peeters, Gilles Van Assche+
_Keccak sponge function family main document._,
Submission to NIST (Round 2) 3, no. 30 (2009): 320-337,
2009
link:pass:[https://keccak.team/files/Keccak-submission-3.pdf][📁^]

* [[[keccak-crate]]]
_keccak - crates.io_,
Pure Rust implementation of the Keccak sponge function including the keccak-f and keccak-p variants,
2023
link:pass:[https://crates.io/crates/keccak][🔗^]

<<<

== Appendix

.`minmax` function in {cpp}
[source#minmax-cpp-listing.linenums,cpp]
----
include::experiments/cpp-hls-experiments/minmax.cpp[tag=function]
----

.`md5` function in {cpp}
[source#md5-cpp-listing.linenums,cpp]
----
include::experiments/cpp-hls-experiments/md5.cpp[tag=function]
----

.`keccak` function in {cpp}
[source#keccak-cpp-listing.linenums,cpp]
----
include::experiments/cpp-hls-experiments/keccak.cpp[tag=function]
----

.`keccak` function in Rust
[source#keccak-rust-listing.linenums,rust]
----
include::../rust-keccak/src/keccak.rs[tag=function]
----

.idiomatic `keccak` function in Rust
[source#keccak-idiom-listing.linenums,rust]
----
include::../rust-keccak/src/keccak_idiomatic.rs[tag=function]
----

.`keccak_p` function from <<keccak-crate>>
[source#keccak-crates-implementation-listing.linenums.hundred_max,rust]
----
include::samples/keccak_crates_implementation.rs[tag=function]
----


<<<

// TODO: Make sure this graph is displayed correctly,
.Simplified CFG for the LLVM IR generated by the Rust compiler from the minmax function with optimizations for performance
[graphviz#minmax-speed-cfg.slightly-oversized-content,opts=inline,width=15cm]
----
include::samples/minmax_speed_control_flow.dot[]
----

// Reference thesis:
// * https://webthesis.biblio.polito.it/7573/1/tesi.pdf
// * https://scholarworks.gvsu.edu/cgi/viewcontent.cgi?article=1754&context=theses

include::styles/trailing-scripts.adoc[]


// Final checklist:
// * are all abbreviations defined?
// * are all abbreviations linked to wikipedia (or somewhere else)?
// * are all references used?
// * are all references linked to the correct source?
// * are all TODOs processed?
// * are the product names consistent? (BambU)
// * check for duplicate references
// * check for broken references
// * archive.org all links
// * Check for duplication of information
// * oxford comma
// * style code blocks
https://github.com/RustCrypto/sponges/blob/master/keccak/src/lib.rs
:doctype: book
:last-update-label!:
:imagesdir: images
:source-highlighter: rouge
:rouge-style: github
:toclevels: 2
:stem:
:toc: macro
:sectanchors:
:notitle:
:title-page: false
:stylesheet: Readme.css
:toclevels: 3

image::logo_hda.svg[role=logo]

[.university.text-center]
Darmstadt University of Applied Sciences

[.faculty.text-center]
Faculty of Computer Science

[discrete]
= Is High-level synthesis from Rust possible using existing tools?

[.description.text-center]
Submitted in partial fulfilment of the requirements for the degree of +
Bachelor of Science (B.Sc.)

[.presented-by.text-center]
by +
*Lennart Eichhorn* +
[small]+Matriculation number: 759253+ +


[.other-people]
First Examiner:: Prof. Dr. Stefan Rapp
Second Examiner:: Prof. Dr. Ronald Charles Moore

<<<

[discrete]
== Abstract

// A short summary of the contents in English of about one page. The following
// points should be addressed in particular:

// * Motivation: Why did this work come about? Why is the topic of the
// work interesting (for the general public)? The motivation should be
// abstracted as far as possible from the specific tasks that may be given
// by a company.
// * Content: What is the content of this thesis? What exactly is covered in
// the thesis? The methodology and working method should be briefly
// discussed here.
// * Results: What are the results of this work? A brief overview of the
// most important results as a teaser to read the complete thesis.

// [NOTE]
// ====
// A great guide by Kent Beck how to write good abstracts can be found here:

// <https://plg.uwaterloo.ca/~migod/research/beckOOPSLA.html>
// ====

<<<

toc::[]

<<<

// Start with section and part numbering
:sectnums:
:part-signifier: Part
:partnums:

= Thesis

== Introduction

The popularity of the Rust programming language is rising and it is one of the most loved new programming languages. It integrates modern tooling like standardized dependency management, testing, documentation generation, formatting and building. In the future, the adoption will probably increase further and it could replace {cpp} as the most common system programming language <<Bug22>>.
It is possible that Rust also provides benefits in domains other than systems programming. It has been shown that Rust can be used for other domains such as GPU programming, web development or logic programming <<Sah22>> <<Byc22>> <<Kyr22>>. This enables us to use some of the benefits of Rust <<Bug22>> <<Cos19>>.

This paper will explore how Rust can be used in the domain of FPGA firmware development. Usually, FPGA firmware is developed in a Hardware Description Language (HDL) such as Verilog or VHDL. In these languages, the programmer has to describe the hardware in detail. This is a low-level approach that can lead to efficient designs, but it is quite time-consuming <<Mil20>>. The rust-hdl project enables us to express hardware descriptions in Rust similar to traditional HDLs <<rusthdl>>. In addition to manually writing hardware descriptions in an HDL, it is also possible to use High-level synthesis (HLS) to generate hardware descriptions in HDLs from an algorithmic description written in high-level programming languages. This provides increased productivity at the cost of slightly less optimized designs <<Mil20>>. This process is currently mostly used with C or {cpp} because they are the most common system programming languages. There are multiple HLS tools available that can synthesize HDL code from {cpp} code. Multiple of these tools are based around the LLVM compiler infrastructure <<Nan16>>. There is one reported use of Rust as an HLS language but that is of no use to us as their tools are not available <<Har22>>. As Rust is also based on LLVM, it is possible that it can be used with these tools too. 

To figure out whether Rust can be used as a source language for HLS, we searched for HLS tools that can be used for Rust. We developed a modular process for integrating HLS tools with rust-hdl <<rusthdl>>. We used this process to implement a proof-of-concept integration with the PandA Bambu HLS framework and showed that it is possible to use Rust as a source language for HLS. We also evaluated the resulting solution by synthesizing a simple example. We found that the solution is not yet suitable for production use, but that it is a promising starting point for further development.

// What exactly is the problem
// Why is this a problem

== How is hardware design done today

It is assumed that you have some knowledge of the Rust programming language and systems programming. This section will provide an overview about the topics that are relevant for this paper. It will also provide an overview about the related work that has been done for HLS from Rust.

=== Target platforms

// TODO: Maybe custom IC instead of ASIC
There are two target platforms for logic design: field-programmable gate arrays (FPGA) and application specific integrated circuits (ASIC).

==== What is an FPGA

// What are CPUs?
//TODO: Citations
// TODO: Idk if the intro is appropriate
CPUs process their instructions one by one. They are purpose-built machines that are carefully designed for processing lots of instructions in sequence. Traditional programming languages reflect this design for the most part. They are designed to make it easy for humans to write programs that can be executed sequentially one instruction at a time. There are approaches to parallelism, but they are either limited to having multiple threads of execution that run from top to bottom simultaneously. Or they have some instructions that perform the same operation on a fixed amount of data elements at the same time.


// TODO: citation needed
Field programmable gate arrays (FPGAs) are not designed to process instructions. As the name field programmable gate array implies, they consist of programmable logic gates with programmable connections between them. So while a CPU is a logic circuit designed to do one thing, an FPGA is a circuit designed to emulate other logic circuits. 

// TODO: FPGA 

// An FPGA can be used to emulate a CPU.
// TODO: citation needed
For example, a design describing the logic gates and connections that make up a CPU can be programed to the FPGA, so it will behave exactly like the CPU and can process instructions. Such a 'soft' CPU will be much bigger and therefore slower than a real CPU because a circuit emulated on an FPGA takes up more space than an application-specific circuit. This is however really useful for prototyping because you can test the CPU design in hardware with external peripherals, like memory and I/O devices.

// More details on using FPGAs for hardware development
// TODO: citation needed
In the past, FPGAs were mostly used like this for prototyping hardware development. Hardware designers designed a circuit in a hardware description language (HDL) and simulated it as a first stage of verification. If that worked it can be deployed to an FPGA, and get tested there. If everything works as expected the design can be taped out and manufactured as an ASIC.

// In comparison to an FPGA a CPU is an ASIC, I think. But I have no source for that

// FPGA as computational accelerators
// TODO: citation needed
This is still the most common use case for FPGAs, but it is not the only one. In recent years the usage of FPGAs as pseudo-general-purpose computational accelerators became more relevant. Here you do not use them to prototype circuits that will eventually be taped out but as the final platform. FPGAs used in this context are known as _computational FPGAs_. It is somewhat comparable to the use of GPUs as computing accelerators. But where GPUs excel at tasks that perform the same operations in parallel on massive amounts of data, FPGAs can be used for some kind of computations with irregular parallelism with static structure. In opposition to GPUs, it is not yet clear what an appropriate abstraction for the computational pattern used with computational FPGAs is.

// Modern FPGAs have hardware blocks






// What is an CPU

// What is an GPU

// What is an ASIC / Custom accelerator
// TODO: This is a stub
Another solution is to produce a custom chip that implements the operations in hardware.

// What is an FPGA
Instead of producing the circuit into a chip, it is possible to program the FPGA to emulate the circuit. FPGAs are off-the-shelf reconfigurable ICs capable of implementing any digital circuit. Design cost and time with FPGAs is much lower than for custom chips. Another benfits of using FPGAs is that the circuit can be upgraded after deployment. The downsides of using FPGAs are lower performance and higher power consumption compared to custom chips. They are considered compelling options for small to medium sized projects. Because FPGAs are off-the-shelf components they dont have the high upfront cost of ASICs. <<Bot21>>

// CN: FPGAs are suitable for small to medium sized projects. Because they dont have the high upfront cost of ASICs.
// CN: They use up more space than ASICs, so they are more expensive than the same ASIC produced at high volume.

// TODO: Early FPGA diagram
// TODO: improve note

NOTE: Different FPGA vendors use different names for the same things. <<Lah19>> This paper uses LB, made of LEs, made of LUTs and FFs.

// How do basic FPGAs work
At the core FPGAs are programmable logic elements that can represent a simple logic function. Every logic element has around 4-6 input bits (vendor specific). The inputs are used to address a programmable lookup table. The lookup table contains the truth table of the logic function. The output of the lookup table is coupled with an output register of the LE. This way the LE can also act as a flip-flop. <<Bot21>>
Multiple logic elements are grouped into a logic block (LB). A logic block provides a local interconnect between the logic elements. The local interconnect is used to connect the inputs of the logic block and the outputs of the logic elements to the inputs of the logic elements. The local interconnect is usually realized as a programmable crossbar. <<Bot21>>
An FPGA is made up of multiple logic blocks that are connected by programmable routing. There are multiple ways in which global routing can be realized, but the most popular one is an island-style architecture. It is shown in FIGURE XXX. <<Bot21>>

// TODO: Insert a figure on island style architecture
// FPGAs consist of configurable logic blocks (CLB) that are connected by programmable interconnects. The CLBs consist of multiple basic logic elements (BLE) and a local interconnect. At the core of each BLE is a Lookup table (LUT)<<Lah19>> <<Bot21>> 

// How are FPGAs programmed
// TODO: Cititation needed
All the programmable components are controlled by configuration stored in SRAM cells. FPGAs are programmed with a bitstream that contains the individual bits for every SRAM cell in the correct order. They usually have a serial interface where the FPGA can accept the bitstream and program itself. <<citation-needed>>

// The logic blocks are connected by a network of programmable interconnects. The logic blocks and interconnects are programmed by a bitstream. The bitstream is a binary file that configures the FPGA. The bitstream is generated from a hardware description language (HDL) design. <<Bot21>>

// Explain the concept of the critical path
For every path that the data takes through the circuit it needs a certain amount of time. The longer the path and the more things are in it, the longer it takes to take the path. A critical path is a path that must meet certain timing requirements for the design to function properly. <<Cri95>> If the circuit for example is clocked, then there are probably some actions that should be finished before the next clock cycle. If they do not the clock needs to be slowed down or the circuit will behave in unexpected ways. For this reason the critical path limits the maximum frequency. <<Bot21>>

// What is the difference in modern FPGAs
Modern FPGAs improve critical path delay by hardening certain features of the FPGA. Hardening means that a feature is implemented in a fixed way instead of being programmable. This Which features are hardened depends on the FPGA model. All modern FPGAs include hardened circuitry for arithmetic operations in their logic blocks. The exact details of the hardening depend on the FPGA model, but it usualy involes a fastpath for a carry bit. This is at least three times faster than a pure LUT based implementation. <<Bot21>>

DSP blocks are another common feature on modern FPGAs. They minimize the number of soft logic operations needed to implement common DSP algorithms. Like LBs they are connected to the programmable routing. Depending on the FPGA model, they can be configured to perform operations like multiplication, addition, subtraction, accumulation, and/or multiplication-accumulation in various sizes. <<Bot21>> <<Lah19>>

Bigger FPGA designs almost always require a memory buffer. Building a memory buffer out of logic blocks is possible, but not very efficient. Modern FPGAs include hardened memory blocks that can be used as memory buffers. They are called block RAM (BRAM). BRAMs are over 100 timer more dense than soft memory made from LUTs. BRAMs are about 25% of the area of modern FPGAs. <<Bot21>> <<Lah19>>

// What are the steps to convert an RTL design to an FPGA
// TODO: Mapping the netlist to the FPGA architecture is called technology mapping. Citation needed
Converting an RTL design to a bitstream is a multi-step process. The first step is synthesis. The RTL design is converted to a structural gate-level description. This is called a netlist. The netlist is then mapped to the block types availabe on the specific FPGA model. A series of complex optimizations can also be performed at this stage. After that the blocks  are placed to specific blocks on the FPGA and the connections between them are routed. It is crucial to minimize the routing distance between logic blocks, because routing accounts for over 50% of the critical path delay. As it is now known how every part of the FPGA should behave, the bitstream can be generated. <<Bot21>> 

All these steps are performed by a CAD tool usually provided by the FPGA vendor. For the biggest FPGA vendors these tools are integrated into their respective IDE. <<citation-needed>> Recent developments in the open-source community have led to the development of open-source tools that can perform these steps. <<citation-needed>>

// TODO: Mention timing analyzers and shit


// How do they compare to ASICs

// Notable FPGA vendors?

// Context of this paper

=== Netlists are synthesized to hardware

// TODO: This is a stub
All approaches of hardware design end up generating a structural gate-level description called netlist typically in a subste SystemVerilog. This netlist is then used to generate the actual hardware design in a mostly automated fashion. How this step actually works depends on what kind of Hardware is targeted e.g. FPGA, ASIC or fully custom chips.
//<<Fla20>>

This step is basically manufacturing a design. The focus of this paper is only on design and not on manufacturing, so we will not go into detail about this step. It is just important to know that a netlist is something that can be manufactured / deployed to a FPGA.

=== Design using traditional Hardware Description Languages
// What are common HDLs and what are they used for
// TODO: Systema verilog is most common nowadays
Traditionally, logic design is done in hardware description languages (HDL). There are two established ones, Verilog and VHDL. VHDL has a slightly higher level of abstraction and some features that make it easier to manage bigger projects. In some ways, the relation between Verilog and VHDL is comparable to the relation between C and {cpp} in terms of features and abstraction. Both can be used to model the structure of hardware equally efficiently, so the choice is mostly a matter of personal preference. <<Smi96>>

// TODO: Mention timing

// What do you describe in HDLs
// TODO: Citation needed
Most commonly HDLs are used to describe circuits in a register-transfer level (RTL) abstraction. On RTL these languages describe registers that can hold state and the combinational (time-independent) logic that connects them. In HDLs the registers and combinational logic can be bundled into a module to make it reusable. These modules can then be connected to form a larger circuit. This is the basic structure of an HDL design.

// What distinguishes them from progamming languages

// What is a typical HDL design flow
A typical HDL design flow consists of four phases, design, verification, synthesis and implementation. In the design phase, the circuit is designed in an HDL. The design is then verified in various ways. To verify the behaviour of a design testbenches are defined. These testbenches (usually also written in HDL) instantiate the module of the design under test (DUT), exercise the inputs and verify that the outputs behave as expected. A logic simulator is used to execute the testbenches. This is comparable to unittesting in programming languages. After the design is verified, a logic synthesis tool is used to synthesize the design into a optimized gate-level logic description (netlist). A formal equivalence tool can then be used to verify that the netlist is equivalent to the original design. In the implementation phase, the netlist is mapped to the target hardware. <<Fla20>>
 
// What are the problems with HDLs
The use of HDLs is mostly limited to hardware engineers

// What are the possible solutions

==== SystemVerilog sample

=== Design using alternative Hardware description languages

There are multiple modern HDLs that try to improve on the shortcomings of Verilog and VHDL. Most of them try to bring some features from modern programming languages to hardware design. (linting, formatting, dependency management, namespaces/scoping, better support for multifile projects, ...). These languages are usually transpiled to Verilog and then synthesized to netlists.

==== Chisel

// TODO: This is a stub
Probably the most popular alternative HDL. It is a Scala DSL that is transpiled to Verilog.

==== rust-hdl

rust-hdl is a Rust crate that allows describing RTL logic in Rust. The logic can then be transpiled to Verilog. rust-hdl also includes tools for simulation and verification. Because a rust-hdl based design is just a Rust program, it can use most of the Rust ecosystem features. This includes the Rust testing framework for verification and simulation. It also makes it possible to reuse and share designs using the Rust packagemanager cargo. <<rusthdl>>

.rust-hdl struct for a simple blinker module.
[source,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-struct]
----

In rust-hdl logic blocks (equivalent to Verilog modules) are defined as structs. The field of these logic blocks corresponds to the external and internal ports of the module. External ports are defined as `Signal` with a direction and a data type. If the logic block is contains another logic block it is also defined here. rust-hdl provides some basic logic blocks for common functions like D flip-flops (DFF) or constants. <<rusthdl>> An example for a logic block definition is shown above in listing XXX.

The combinational logic that connects these signals is described in the update function. Every rust-hdl signal has a .next field that determines the value of the signal after the update function. The update function must assign a value to all the .next fields. If there are multiple assignments to the .next fields the last one is valid, like in normal Rust. As Rust can express much more than just combinational logic, rust-hdl only allows a limited synthesizable subset of Rust in the update function. <<rusthdl>> 

.The restrictions of the synthesizable subset include:
- No local variables
- Assignments are only allowed to .next fields of signals
- Function and method calls are limited to a small subset of library functions
- Only range based for loops are allowed

<<rusthdl>> 

.rust-hdl update function for the blinker.
[source,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-logic]
----

rust-hdl also enforces an additional set of semantic rules about what the update function can contain. For example it does not allow undriven nets, so there must be at least one assignment to the `.next` value of every field. If there are multiple assignment to the next value of a signal, the last one is valid, like in normal Rust. rust-hdl also forbids to use the `.next` value on the right side of an expression. rust-hdl enforces these rules at compiletime and generates somewhat helpful error messages. <<rusthdl>> An example for a update function is shown above in listing XXX.

.Simulating and verifying the blinker
[source,rust]
----
include::rusthdl-blinker/src/blinker.rs[tag=rust-hdl-test]
----

The Rust unit testing framework can be used to simulate and verify rust-hdl modules. Listing XXX shows a testcase that simulates and verifies that the blinker module toggles its output with the specified frequency (10 cycles in that case). The test case can be run with `cargo test` like any other Rust unit test.

.Generating verilog from the rusthdl blinker
[source,rust]
----
include::rusthdl-blinker/src/main.rs[tag=generate-verilog]
----

rust-hdl can then be used to generate a Verilog description of the design as shown in listing XXX. The generated Verilog code can then be used with any Verilog toolchain to deploy the design to a FPGA. <<rusthdl>>

// Justify why rust-hdl is a good choice even though it is more verbose
// TODO: Compare LoC
rust-hdl code is a more verbose than Verilog code. This is mostly because of the additional type annotations and the need to explicitly assign the .next value of every signal. 


=== Design using High-level synthesis
// What is HLS
High-level synthesis (HLS) is a process that can generate a RTL specification of a circuit from a description in a high-level programming language. This is a more productive approach than writing RTL code directly, but the resulting designs are usually less efficient. The generated RTL code can then be used in the same way as manually written RTL code. <<Mil20>> <<Nan16>> <<Lah19>>

// HLS compared to RTL
The main advantages of HLS are reduced design time and lower development cost. On average the development time of HLS designs is only a third than that of equivalent RTL designs. The tradeoff for the lower development time seems to be that HLS designs on average take up around 40% more basic FPGA resources than RTL designs. The performance of HLS designs is around two thirds of that of a RTL design. These metrics generalize over many different HLS tools and input languages and there is a big variance between different experiments. In about 40% of the cases the HLS design was more efficient or performant than the RTL design. These metrics for performance and resource usage show a lot of variance between experiments, in about 40% of designs the HLS design was actually more performant than the RTL design. In about 30% it was more resource efficient. However in terms of development time there is little variance, in 90% of the experiments the HLS design was faster than the RTL design. <<Lah19>>

HLS tools can be distingushed into two major categories. Those that accept a general purpose language and those that accept a domain specific language (DSL) a an input. Using a DSL as input can lead to better performing designs, but it also raises challenges for adoption. A general purpose language makes it easier for the algorithm designer, who is usually a software developer, to write code. <<Nan16>> The most common input languages for HLS are C based, including C, {cpp}, and SystemC. <<Lah19>> 

// What HLS tools are there
// TODO: Add citation for usually.
// TODO: Add citation for bambu is the biggest. 
HLS tools are usually available as a part of the IDE provided by the FPGA vendors. For example, AMD Xilinx provides the Vivado HLS tool as part of its IDE and Intel Altera includes the Intel HLS Compiler in its Quartus Prime IDE. <<intel-hls>> The most popular HLS tool in academia is Vivado HLS <<Lah19>>. There are also a few open-source HLS tools available. The biggest open-source HLS tool that is currently activly maintained is panda bambu. <<Nan16>> 

bambu provides as a research environment to experiment with new ideas in HLS. It can take C/C++ and LLVM IR as input. <<Fer21>>

In this paper we will use bambu as a HLS tool, because it is open-source and supports the latest LLVM version. 

// What are the problems with those languages
// TODO: This section does probably not belong in this part
Old programming languages, we have better alternatives nowadays
More steps in the design flow
More explicit. Register can be anything


// ==== Bambu HLS

==== Languages used for High-level synthesis

// TODO explain systems programming language

=== LLVM

// What is LLVM
// TODO: Find source and describe how LLVM is used today
LLVM is a modular compiler framework that can be used to build compilers for many different programming languages. It defines a low-level code representation called LLVM intermediate representation (LLVM IR). <<Lat04>>

// What is LLVM IR
LLVM IR is a strongly typed, static single assignment (SSA) based intermediate representation. LLVM IR is designed to be easy to compile to machine code and to optimize. LLVM IR has a low-level, language independent type system. The type system captures enough type information to safely perform a number of aggressive transformations that would traditionally be attempted only on type-safe languages in source-level compilers. LLVM IR is not intended to be an universal compiler IR, so it does not capture all of the language specific type information. Some of the concepts that are not represented in LLVM IR are classes, inheritance, or exceptionhandling semantics. <<Lat04>>

// How is LLVM used in compilers
Because of the this compilers based on LLVM provide a front-end that process the program code in the source language. The frontend is able to perform language specific optimizations. The compiler frontend emits LLVM IR which is passed to the LLVM backend which performs a variety of transformation and optimizations. The processed LLVM IR is then passed to a code generator backent to translate it into native code for a given target. <<Lat04>> The LLVM project includes code generator backends for many targets. <<Lat04>> <<rustc-guide>>


=== The Rust programming language

// What is Rust
Rust is a modern systems programming language aiming to replace C and {cpp} as the industry standard systems programming language. It offers zero cost memory saftey, a strong type system and a modern toolchain. Rust surpasses all other common memory-safe languages in terms of performance while offering many modern features that more established systems programming languages tend to lack. For these reasons it has been voted the most loved programming language every year since 2016. <<Bug22>> <<Cos19>> <<Kla23>>

// TODO: Insert rust performance diagram

// What is a systems programming language
Systems programming languages are programming languages that can deal with low-level details of memory management, data representation and concurrency.
They are often designed for use in resource-constrained, performance-critical or close-to-hardware programs. They are used to implement operating systems, embedded systems, device drivers and other software that interacts with hardware. Common systems programming languages include C, {CPP} and Rust. <<Kla23>> <<Str12>>

// TODO: Explain how the rust compiler uses LLVM
// Details for this are already summarized at <<rustc-guide>>

// Explain the borrow checker and memory saftey
// TODO: Ask about the first sentence being an exact quote
// TODO: Make the first quote shorter
Rust is the first industry-supported computer programming language to overcome the longstanding trade-off between the control over resource management provided by lower-level languages for systems programming, and the safety guarantees of higher-level languages. <<Bug22>> <<Jun17>> Rust achieves this by enforcing that every variable is always owned by exactly one scope. When that scope ends the variable is destroyed. For this the time between creation and distruction of a variable is called its lifetime. Rust provides semantics for moving ownership between scopes. <<Kla23>>
This model of scope based resource management is called RAII. It is used by many systems programming languages including {cpp}. While {cpp} enables the programmer to break out of that system by using pointers to variables that are not owned by the current scope or its parents, Rust does not allow this. <<Str12>> Rust instead uses a type of reference with attached lifetime information called a borrow. The borrow checker statically guarantees at compiletime that borrows always point to valid objects. It does this by ensuring that the lifetime of every borrow ends at or after the lifetime of the current scope. The compiler also makes sure that there can only be either one mutable borrow or multiple immutable borrows to a value at the same time. This ensures that the values of borrows dont change unexpectedly. The programmer can opt out of the borrow checker by annotation code as `unsafe`. This allows the programmer to use raw pointers and other unsafe constructs. Unsafe code is sometimes neccessary to implement lowlevel data structures, such as Heap memory (`Box` or `Vec`) or types with internal mutability (`Ref`). The standard library provides most of these constructs, so most Rust programs do not need to use unsafe code. <<Kla23>> <<Bug22>> There is ongoing work on verifying that the unsafe code in the standard library is safely encapsulated by its types. <<Jun17>>

// Overview about how the compiler processes Rust code
// TODO: Explain desugaring (the word)
Rust is a compiled language. The Rust compiler (rustc) can compile Rust code to native code for many different platforms. It accomplishes this by compiling Rust to LLVM IR and then using LLVM for code generation. This allows Rust to support many different platforms without having to implement a backend for every platform. It also enables Rust to utilize the large suite of advanced optimizations collected by the LLVM project. The compiler does not generate LLVM IR directly from the input Rust code. Instead it the input code gets passed through multiple IRs, HIR, THIR and MIR. HIR and THIR still resemble Rust code, but some constructs get desugared. The rust compiler uses these stages to perform typechecking and verification. Verified THIR is converted into MIR, which is a CFG based representation of the code. rustc performs flow-sensitive safety checks like borrow-checking on this level. The MIR is also used to apply various Rust-specific optimizations to the code. The Rust compiler can be configured to output the various intermediate representations instead of the generating machine code. <<rustc-guide>>

// Overview about the tooling related features of Rust
A important part of what makes the Rust ecosystem so productive is that Rust offers standardized tooling. Every Rust project (also called crate) contains a `Cargo.toml` file specifying the project metadata and dependencies. The `cargo` tool can then be used to perform all standard tasks like building, executing, unit testing, integration testing, formatting the code, generating documentation, downloading dependencies, building dependencies, managing dependencies, publishing the project, benchmarking, setting up projects and some more. The official community crate registry `crates.io` can be used to easily find dependencies. It also links to the automatically generated documentation for every crate. The tooling alone makes Rust a much better development experience than most systems languages. <<Bug22>>

// 

=== Design using accelerator design languages

Accelerator design languages (ADL) are a family of programming languages that are specifically designed to be synthesized to HDL. They are mostly similar to programming languages but offer many features that are usually only found in HDLs, like more fine-grained control over timing and memories memory access. <<hdl-to-adl>>



== Concept, implementation and architecture

// Short overview of the solution and this section
The goal of this paper is to use Rust as a source language for HLS using an existing HLS tool. First a suitable toolchain has to be designed. Then a concept for integrating the toolchain with rust-hdl has to be developed. Finally, a proof-of-concept implementation will be done to show that the process works and enable evaluation of the resulting solution.

// TODO: Maybe define some criteria for our solution
// * Can synthesize a simple Rust program
// * Can synthesize our md5 implementation
// * Can synthesize most stateless Rust function
// * The synthesized function can use Rust crates from crates.io
// * The existing Rust tooling (linter, formatter, etc.) work with out function

=== Basic toolchain for synthesizing Rust

As there is currently no tool that can synthesize Rust directly the first step in the toolchain needs to convert the Rust code into a language that can be used by an existing HLS tool. There are multiple HLS tools that support C or {cpp} by using LLVM compiler infrastructure. As a result of using LLVM some of these tools can also use LLVMs intermediate representation (LLVM IR) directly as an input language. <<Nan16>> The Rust compiler can compile Rust code to LLVM IR. This will be the starting point of the toolchain

The Rust compiler is frequently updated and the generated LLVM IR usually uses the latest LLVM version. <<citation-needed>> This means that a suited HLS tool needs to be activly maintained to support the generated LLVM IR. The only HLS tool that fulfills these requirements seems to be panda bambu. <<Fer21>> SmartHLS and Vivado HLS may also be capable of operating on LLVM IR, but they are not open source and only available as part of an IDE and not as standalone programs, which makes them unsuitable for our use case.

.The toolchain
[pikchr]
....
   arrow right 150% "Rust" "function"
   box rad 10px "Rust Compiler" fit
   arrow right 190% "LLVM IR" "function"
   box rad 10px "PandA Bambu" fit
   arrow right 130% "Verilog" "RTL"
....

The basic toolchain is relativly straightforward. In a first step the Rust compiler is used to convert a Rust function to a LLVM IR function. The second step passes the generated LLVM IR function to panda bambu which converts it to Verilog. The resulting Verilog can then be used in a larger HDL design.

=== Using the toolchain manually

This toolchain can be used to synthesize Rust functions. In the following example a Rust function that finds the minimum and maximum of an array of integers will be synthesized.

// TODO: Find a good example and explain it.

=== Integrating the toolchain with rust-hdl

// TODO: Remove intro
We want to try using Rust as a source language for HLS, because of the developer experience. As seen in the previous example integrating it with Verilog involves a few manual steps. This is not a good developer experience. In this section the integration with rust-hdl will be explored. This makes it possible to create a single Rust crate containing both RTL and HLS code in the same project.

// With rust macros it is possible to execute arbitrary code transformations at compile-time. This can be used 



//TODO: This is a stub
Being able to synthesize Rust to Verilog in a vacuum is not that usefull. We should be able to use it in a larger HDL design. This paper will use rust-hdl as a HDL. This way the HDL design can be completly written in Rust.

The generated Verilog is wrapped in a rust-hdl module. Rust-hdl is not able to simulate embedded verilog, so we use Verilator to build a library that can be used to simulate the generated Verilog and link the rust program against it.

This also allows us to perform verification directly in rust unit tests. Nice integrated development experience.

Challenge: integrating bambu

//TODO: diagram of the integrated toolchain


=== Proof-of-concept implementation

// TODO: This is a stub
The proof-of-concept implementation uses a Rust procedural macros to transform a Rust function into a rust-hdl module.

// TODO: Show of an example

==== Explain neccessary compiler flags


== Experiments and results

=== Show limitations of the synthesizable subset of Rust

Show the limitations of the toolchain. Limitations of the synthesizable subset of Rust.

=== Show how the Rust ecosystem can be used

=== Generated modules

Take a look at the generated modules

=== Performance compared to HLS from {cpp}

Compare performance against {cpp}. basically the same as in first-toolchain.adoc


// Perform experiments and collect data about our solution

== Discussion and evaluation


Discuss limitations

Discuss the results of the performance experiments

Discuss compiler flags? 

// Why is this a solution
// Discuss the results of the experiments

== Conclusion and future work
// TODO: This is a stub
// This is completely written by copilot and should be rewritten by hand
In general the toolchain is working quite well. HLS from Rust is definitly possible with little restrictions. The performance of the generated modules is also quite good. The generated modules are not as good as hand written modules, but they are good enough for most applications. The generated modules are also quite large, but this is not a problem for most applications.

// TODO: This is a stub
// This is completely written by copilot and should be rewritten by hand
The toolchain is also quite easy to use. The only thing that needs to be done is to add a procedural macro to the function that should be synthesized. This is a very simple change and does not require any changes to the function itself. The procedural macro can also be used to add additional information to the function that can be used by the HLS tool. This is very useful for example to specify the clock frequency of the function.


[glossary]
== List of abbreviations
// Abbreviations from here will automatically be linked into the document

// Abbreviations in a random order and links to read more about them
[glossary]
[[FPGA]]FPGA:: Field-programmable gate array link:pass:[https://en.wikipedia.org/wiki/Field-programmable_gate_array][üîó^]
[[HLS]]HLS:: High-level synthesis link:pass:[https://en.wikipedia.org/wiki/High-level_synthesis][üîó^]
[[HDL]]HDL:: Hardware description language link:pass:[https://en.wikipedia.org/wiki/Hardware_description_language][üîó^]
[[ADL]]ADL:: Accelerator design language link:pass:[https://www.sigarch.org/hdl-to-adl/][üîó^]
[[GPU]]GPU:: Graphics processing unit link:pass:[https://en.wikipedia.org/wiki/Graphics_processing_unit][üîó^]
[[LLVM_IR]]LLVM IR:: LLVM intermediate representation link:pass:[https://en.wikipedia.org/wiki/LLVM#Intermediate_representation][üîó^]
[[RTL]]RTL:: Register-transfer level link:pass:[https://en.wikipedia.org/wiki/Register-transfer_level][üîó^]
[[DUT]]DUT:: Design/Device under test link:pass:[https://en.wikipedia.org/wiki/Test_bench][üîó^]
[[ASIC]]ASIC:: Application specific integrated circuit
[[QoR]]QoR:: Quality of results
[[CPU]]CPU:: Central processing unit
[[LUT]]LUT:: Look-up table
[[FF]]FF:: Flip-Flop
[[DFF]]DFF:: D Flip-Flop
[[BRAM]]BRAM:: Block RAM
[[DSP]]DSP:: Digital signal processor 
[[CLB]]CLB:: Configurable logic block
[[LB]]LB:: Logic block
[[LE]]LE:: Logic element
[[RAII]]RAII:: Resource Acquisition is Initialization / Scope-Bound Resource Management
[[HIR]]HIR:: High-Level Intermediate Representation link:pass:[https://rustc-dev-guide.rust-lang.org/hir.html][üîó^]
[[THIR]]THIR:: Typed HIR link:pass:[https://rustc-dev-guide.rust-lang.org/thir.html][üîó^]
[[MIR]]MIR:: Mid-level Intermediate Representation link:pass:[https://rustc-dev-guide.rust-lang.org/mir/index.html][üîó^]
[[PAL]]PAL:: Programmable array logic


[bibliography]
== References

// Claims to have a transpiler from a subset of Rust (RAR) to restriceted algrithmic C (RAC) that can be synthesized to FPGA. No source.
// The first paper to mention HLS from Rust. 
* [[[Har22]]]
Hardin, David,
_Hardware/Software Co-Assurance using the Rust Programming Language and ACL2_,
arXiv preprint arXiv:2205.11709,
2022.
link:pass:[https://arxiv.org/abs/2205.11709v1][üîó^]

* [[[Rog20]]]
Rogers, Samuel and Slycord, Joshua and Baharani, Mohammadreza and Tabkhi, Hamed,
_gem5-SALAM: A System Architecture for LLVM-based Accelerator Modeling_,
2020 53rd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), 471-482,
2020.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9251937][üîó^]

* [[[Li21]]]
Li, Rui and Berkley, Lincoln and Yang, Yihang and Manohar, Rajit,
_Fluid: An Asynchronous High-level Synthesis Tool for Complex Program Structures_,
2021 27th IEEE International Symposium on Asynchronous Circuits and Systems (ASYNC), 1-8,
2020.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9565447][üîó^]

* [[[Lia23]]]
Liang, Geng-Ming and Yuan, Chuan-Yue and Yuan, Meng-Shiun and Chen, Tai-Liang and Chen, Kuan-Hsun and Lee, Jenq-Kuen,
_The Support of MLIR HLS Adaptor for LLVM IR_,
Workshop Proceedings of the 51st International Conference on Parallel Processing, 1-8,
2020.
link:pass:[https://doi.org/10.1145/3547276.3548515][üîó^]

* [[[Fer21]]]
+F. Ferrandi et al.+,
_Invited: Bambu: an Open-Source Research Framework for the High-Level Synthesis of Complex Applications_,
2021 58th ACM/IEEE Design Automation Conference (DAC), 1327-1330,
2021.
link:pass:[https://ieeexplore.ieee.org/abstract/document/9586110][üîó^]
link:pass:[https://re.public.polimi.it/retrieve/668507/dac21_bambu.pdf][üìÅ^]

* [[[Rot10]]]
+Nadav Rotem,+
_C-to-Verilog. com: High-Level Synthesis Using LLVM_,
University of Haifa,
2010.
link:pass:[https://llvm.org/devmtg/2010-11/Rotem-CToVerilog.pdf][üîó^]

* [[[Sch20]]]
Fabian Schuiki, Andreas Kurth, Tobias Grosser, and Luca Benini,
_LLHD: a multi-level intermediate representation for hardware description languages_,
In Proceedings of the 41st ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI 2020), 258-271,
2020.
link:pass:[https://doi.org/10.1145/3385412.3386024][üîó^]

// Multiple HLS tools use LLVM
// C/Cpp are most popular languages for HLS
// NOTE: I focused on FPGA descriptions
// Clock frequency scaling in CPU stalled around 2005
// A alternative approach for high-throughput and energy efficient processing is to use specific accelerators
// Specialized accelerators are hard to design and program
// RTL requires advanced hardware expertise
// RTL specifies cycle-by-cycle behavior explicitly
// RTL is a low-level abstraction
// RTL leads to longer development times
// FPGAs with HLS can reduce that.
// FPGAs are configurable integrated circuits
// Most FPGAs are reconfigurable
// FPGA vendors provide toolchains to synthesize HTL to bitstream
// bitstream gets programmed to the FPGA
// HLS tools start from a HLL and automatically produce a circuit specification in RTL
// HLS offers enable software engineers to benefit from the performance and energy efficiency of hardware without having hardware expertise
// HLS tools enable hardware engineers to design systems faster
// HLS tools enable hardware engineers to rapidly explore the desing space
// Microsoft uses FPGAs for accelerating bing search
//
// 
* [[[Nan16]]]
+R. Nane et al.+,
_A Survey and Evaluation of FPGA High-Level Synthesis Tools_,
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 1591-1604,
2016.
link:pass:[https://ieeexplore.ieee.org/abstract/document/7368920][üîó^]
link:pass:[https://sci-hub.st/10.1109/tcad.2015.2513673][üìÅ^]

* [[[Nor18]]]
+D. H. Noronha, B. Salehpour and S. J. E. Wilton+,
_LeFlow: Enabling Flexible FPGA High-Level Synthesis of Tensorflow Deep Neural Networks_,
Fifth International Workshop on FPGAs for Software Programmers, 1-8,
2018.
link:pass:[https://ieeexplore.ieee.org/abstract/document/8470462][üîó^]

* [[[Soz22]]]
Sozzo, Emanuele Del, et al.,
_Pushing the level of abstraction of digital system design: A survey on how to program FPGAs_,
ACM Computing Surveys, 1-48,
2022.
link:pass:[https://dl.acm.org/doi/abs/10.1145/3532989][üîó^]

* [[[XLS]]]
_XLS project page_
link:pass:[https://google.github.io/xls/][üîó^]

* [[[DSLX]]]
_DSLX Reference_
link:pass:[https://google.github.io/xls/dslx_reference/][üîó^]

* [[[rusthdl]]]
_rust-hdl project overview_
link:pass:[https://github.com/samitbasu/rust-hdl][üîó^]


* [[[Zen12]]]
_Identifying Barriers to Adoption for Rust through Online Discourse_
link:pass:[https://arxiv.org/pdf/1901.01001.pdf][üîó^]

* [[[so-trends]]]
_Stack Overflow Trends_
https://insights.stackoverflow.com/trends?tags=rust%2Cc%2B%2B

// Rust has an ecosystem that greatly simplifies any software projec
// Rust is great
// Rust has been the "most loved" language since 2016
// Rust is meant to supersede C/C++
// Rust focus is on saftey and performance
// Libraries exist for any need you may have
// Dependencies can be installed using the official cargo tool
// Rust is the first industry-supported computer programming language to overcome the longstanding trade-off between the control over resource management provided by lower-level languages for systems programming, and the safety guarantees of higher-level languages
// Rust enables many common systems programming pitfalls to be detected at compiletime
// Rust surpasses all other common memory-safe languages in terms of performance
// Rust has data-race prevention
// Considering performance Rust is one of the best languages
// Considering saftey Rust is the best language
// Rust offers many modern features that the more established systems languages tend to lack.
// Cargo is the package manager for Rust
// Cargo is the build system for Rust
// Cargo facilitates downloading and building dependencies
// Cargo facilitates unit testing and integration testing
// Cargo facilitates benchmarking
// Cargo facilitates build management with different profiles
// Cargo facilitates documentation generation from comments
// Rustfmt facilitates code formatting
// Dependency management is handled with a configuration file
// Dependencies are automatically installed during compilation
// Dependencies can be easliy found on the official community crates registry
// Cargo allows to view unified documentation for all dependencies
// Unit tests are written in the same file as the code they test
// Benchmarking is done in a similar fashion to unit testing
// The tooling alone makes Rust a much better development experience than most systems languages
// The tooling is most likely a considerable contributor to its rise.
// Rust is approaching the status of a mainstream language in health informatics applications
// The criticisms of Rust tend to originate from its lack of maturity
// C and C++ are well-adopted and much more established in the industry than Rust
// lack of demand for Rust developers in the market
* [[[Bug22]]]
+William Bugden, Ayman Alahmar+,
_Rust: The Programming Language for Safety and Performance_,
asXiv,
2022.
link:pass:[https://arxiv.org/pdf/2206.05503.pdf][üîó^]

// Rust can be used for GPU programming
* [[[Byc22]]]
+Andrey Bychkov, Vsevolod Nikolskiy+,
_Rust Language for GPU Programming_,
In: Voevodin, V., Sobolev, S., Yakobovskiy, M., Shagaliev, R. (eds) Supercomputing. RuSCDays 2022. Lecture Notes in Computer Science, vol 13708. Springer, Cham, 2022, pp. 522-32,
2022
link:pass:[https://doi.org/10.1007/978-3-031-22941-1_38][üîó^]

// Rust can be used for web programming
* [[[Kyr22]]]
+Kyriakou K-ID, Tselikas ND+,
_Complementing JavaScript in High-Performance Node.js and Web Applications with Rust and WebAssembly._,
Electronics 11, no. 19: 3217,
2022
link:pass:[https://doi.org/10.3390/electronics11193217][üîó^]

// Probably one of the greatest features of the language is the package manager, called cargo.
// Rust is a high-level language
// Rust is very efficient in terms of performance
// Rust is based on the principle of zero cost abstractions
// Rust provides a memory safety mechanism without using a garbage collector called the borrow checker
// Rust is a strongly typed language
// Rust is provides out of the box a package manager used for importing dependencies, building and distribution of a project.
// If a variable is declared in a specific context it will be freed when the context is over.
// The ownership of a variable can be passed to another context
// More basic description of Rust ownership stuff, will skip that for now
// Development in {cpp} on a production level requires the use of additional tools such as CMake, Make, etc. This adds a layer of complexity.
// Rust has mandatory tooling for building, distributing and depending on a project
// In Rust only a manifest file is needed to configure the project for any scenario possible
// Rust can be compiled to webassembly
// Rust is more energy efficient than any other language expect C for IoT applications
// Rust is faster than any other language expect C for IoT applications
// Rust can easily integrate with C or C++ code
// Rust solves the problem of memory safety without using a garbage collector
// Microsoft states that 70% of security flaws discovered in their systems are related to memory safety
* [[[Cos19]]]
+Cosmin Cartas+,
_Rust - The Programming Language for Every Industry_,
ECONOMY INFORMATICS JOURNAL, 19, 45-51,
2019
link:pass:[https://doi.org/10.12948/ei2019.01.05][üîó^]

// state-of-art bottom-up logic programming within the Rust ecosystem
* [[[Sah22]]]
+Arash Sahebolamri, Thomas Gilray, Kristopher Micinski+,
_Seamless Deductive Inference via Macros_,
Proceedings of the 31st ACM SIGPLAN International Conference on Compiler Construction, 77-88,
2022
link:pass:[https://doi.org/10.1145/3497776.3517779][üîó^]

// Productivity in HLS is better than HDL
// HLS offers easier design and testing
// HDL implementation is better than HLS
* [[[Mil20]]]
+Roberto Mill√≥n, Emmanuel Frati, Enzo Rucci+,
_A Comparative Study between HLS and HDL on SoC for Image Processing Applications_,
Revista elektron, Vol. 4, No. 2, 100-106,
2020
link:pass:[https://doi.org/10.37537/rev.elektron.4.2.117.2020][üîó^]
http://elektron.fi.uba.ar/index.php/elektron/article/view/117/219[üìÅ^]

// Describing the traditional HDL design flow (in 1996)
// TODO: Find newer source
* [[[Smi96]]]
+Douglas J. Smith+,
_VHDL & Verilog compared & contrasted‚Äîplus modeled example written in VHDL, Verilog and C._,
In Proceedings of the 33rd annual Design Automation Conference, pp. 771-776,
1996
link:pass:[https://dl.acm.org/doi/pdf/10.1145/240518.240664][üîó^]

// 
* [[[Fla20]]]
+Peter Flake, Phil Moorby, Steve Golson, Arturo Salz, and Simon J. Davidmann+,
_Verilog HDL and its ancestors and descendants._,
Proc. ACM Program. Lang. 4, no. HOPL (2020): 87-1,
2020
link:pass:[https://www.researchgate.net/profile/Arturo-Salz-2/publication/342137214_Verilog_HDL_and_its_ancestors_and_descendants/links/613fc7b45d9d0e131b427dbb/Verilog-HDL-and-its-ancestors-and-descendants.pdf][üîó^]

* [[[intel-hls]]]
_Intel¬Æ High Level Synthesis Compiler_
https://www.intel.de/content/www/de/de/software/programmable/quartus-prime/hls-compiler.html

* [[[hdl-to-adl]]]
_From Hardware Description Languages to Accelerator Design Languages_
https://www.sigarch.org/hdl-to-adl/


// Survey literature from 2010 to 2016
// Probably the best comparison of HLS and RTL
// ALso the newest
// Shows that the quality of results of RTL is better than that of HLS
// Shows that development time with HLS is a third of that the RTL flow
// Shows that the productivity of a designer is over four times higher with HLS than with RTL
// Vivado HLS is the most common HLS tool. At least it is used significantly more than any other HLS tool in papers.
// Xilinx is the leading FPGA vendor
// FPGAs are made of configurable logic blocks (CLB, different vendors, different names).
// The CLBs are connected with programmable interconnects.
// The CLBs consist of a few logic cells, logic elements, or adaptive logic modules (ALM) (LC, LE and ALM are the same. Different vendors, different names).
// Logic cells are made of a comination of programmable look-up tables (LUTs) and flip-flops (FFs).
// FPGAs can also have other resources, but these are vendor specific. Most commonly DSP blocks and BRAM blocks.
// There are 4 performance metrics that are commonly used to compare HLS and RTL: performance, execution time, latency, maximum frequency
// For project bigger than 250 lines of code HLS also needs fewer lines of code than RTL
// Reduction in development time for HLS seems independet of project size.
// On average HLS uses 41% more basic FPGA resources than RTL
// The usage of advanced FPGA resources of HLS is similar to RTL
// C based languages are the most common, then OpenCL based, then high-level language based.
// CUDA/OpenCL based HLS is especially resource consuming and has the worst performance
// The performance of HLS designs is similar to the performance of RTL designs.
// THe only example in academia where the development time of HLS was more than RTL was when the developer had to learn the HLS tool in the process.
// Only looks at small to medium designs, 50-500 lines of code
// It is easier to adopt HLS than RTL for people who have experience in software design
// HLS allows for efficient behavioral verification
// The HLS output must still be verified for non-behavioral aspects. This traditional verification is difficult, because there is no direct relationship to the source code.
// HLS halves verification time in many cases
// HLS is a particularly good choice when time to market is a dominant issue and there is no compelling need to gain the ultimate performance or smallest resource usage for the product
// There is no standart example to compare HLS and RTL
* [[[Lah19]]]
+Sakari Lahti, Panu Sj√∂vall, Jarno Vanne, Timo D. H√§m√§l√§inen+,
_Are We There Yet? A Study on the State of High-Level Synthesis_,
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, vol. 38, no. 5, pp. 898-911,
2019
link:pass:[https://doi.org/10.1109/TCAD.2018.2834439][üîó^]
link:pass:[https://sci-hub.st/10.1109/tcad.2018.2834439][üìÅ^]


// Studied Rust‚Äôs ownership discipline in the presence of unsafe code.
// Shows that various important Rust libraries with unsafe implementations, many of them involving interior mutability, are safely encapsulated by their type
// NOTE: Did only read abstract and conclusion
* [[[Jun17]]]
+Ralf Jung, Jacques-Henri Jourdan, Robbert Krebbers, Derek Dreyer+,
_RustBelt: Securing the Foundations of the Rust Programming Language_,
Proc. ACM Program. Lang. 2, POPL, Article 66 (January 2018), 34 pages.,
2017
link:pass:[https://doi.org/10.1145/3158154][üîó^]

// Cpp uses RAII
// "In particular, a programmer can choose to write a low-level-C style and/or violate every rule of good programming. That is not my topic here."
* [[[Str12]]]
+Bjarne Stroustrup+,
_Foundations of C++_,
Programming Languages and Systems. ESOP. Springer, pp. 1-25,
2012
link:pass:[https://doi.org/10.1007/978-3-642-28869-2_1][üîó^]

* [[[Kla23]]]
+Steve Klabnik, Carol Nichols+,
_The Rust programming language_,
No Starch Press,
2023
link:pass:[https://doc.rust-lang.org/book/foreword.html][üîó^]

// Rust compiler has multiple intermediate representations (IRs)
// * MIR (Mid-level IR)
// * HIR (High-level IR)
// * THIR (Typed HIR)
// * LLVM IR
// Typechecking happens on HIR
// Optimization happens on MIR
// MIR is a typed SSA
// Borrowchecking happens at MIR level
// Optimizations also happen in LLVM
// LLVM is used as the backend
// LLVM can generate machine code for many architectures
// LLVM is a collection of modular and reusable compiler and toolchain technologies
// LLVM contains a pluggable compiler backend used by rustc and clang
// Clang is a C compiler
// LLVM takes LLVM IR
// Rust compiler uses LLVM because
// * They don't have to write their own backend. Reduces implementation and maintenance effort.
// * Benefit from the large suite of advanced optimizations that LLVM provides
// * Rust can be compiled to any of the platforms that LLVM supports.
// * Community benfits. Things like spectre and meltdown only need to be fixed in LLVM and many compilers benfits from that
// rustc groups LLVM IR into "modules" known as codegen units
// Rustc can use LLVM to codegen multiple of these modules in parallel utilizing multiple CPU cores
// The resulting object files are then linked together by the linker
* [[[rustc-guide]]]
_Rust Compiler Development Guide_,
2023
link:pass:[https://rustc-dev-guide.rust-lang.org/backend/codegen.html][üîó^]
link:pass:[https://rustc-dev-guide.rust-lang.org/mir/optimizations.html][üîó^]

// LLVM is a compiler framework
// Defines a a low-level code representation in static single assignment (SSA) form
// LLVM IR
// Describes a program using an abstract RISC-like instruction set with higher level information
// LLVM IR contains type information
// LLVM IR contains explicit control flow graphs
// LLVM IR contains explicit dataflow representation (using SSA)
// Has a low-level, language independent type system
// Has instructions for performing type conversions and low-level address arithmetic while preserving type information.
// Low-level exception handling instructions
// LLVM is not intended to be a universal compiler IR
// does not represent high-level language features directly
// LLVM has no notion of highlevel constructs such as classes, inheritance, or exceptionhandling semantics
// LLVM does not specify a runtime system or particular object model
// "Type information captured by LLVM is enough to safely perform a number of aggressive transformations that would traditionally be attempted only on type-safe languages in source-level compilers"
// NOTE: I skipped section 2
// "The goal of the LLVM compiler framework is to enable sophisticated transformations at link-time, install-time, runtime, and idle-time, by operating on the LLVM representation of a program at all stages"
// static compiler front-ends emit code in the LLVM representation
// combined by the LLVM linker
// Linker performs a variety of link time optimizations
// The resulting code is then translated to native code for a given target.
// Language specific optimizations must be performed in the frontend
// External static LLVM compilers are known as front-ends
// Frontends translate source language programs into LLVM IR
// Can perform aggressive interprocedural optimizations across the entire program
// Some of the interprocedural optimizations are:  inlining, dead global elimination, dead argument elimination, dead type elimination, constant propagation, array bounds check elimination, simple structure field reordering, and Automatic Pool Allocation
// Uses code generator backends to translate LLVM IR into native code for a given target
* [[[Lat04]]]
+C. Lattner, V. Adve+,
_LLVM: a compilation framework for lifelong program analysis & transformation_,
International Symposium on Code Generation and Optimization, 2004. CGO 2004., San Jose, CA, USA, 2004, pp. 75-86
link:pass:[https://doi.org/10.1109/CGO.2004.1281665][üîó^]
link:pass:[https://sci-hub.st/10.1109/cgo.2004.1281665][üìÅ^]
// Shows that HLS is twice as fast as HDL 
// M. Pelcat, C. Bourrasset, L. Maggiani and F. Berry, "Design productivity of a high level synthesis compiler versus HDL," 2016 International Conference on Embedded Computer Systems: Architectures, Modeling and Simulation (SAMOS), Agios Konstantinos, Greece, 2016, pp. 140-147, doi: 10.1109/SAMOS.2016.7818341.
// https://ieeexplore.ieee.org/abstract/document/7818341

// FPGA inception 30 years ago
// FPGAs bring faster design cycles then custom chips
// FPGAs lower dev cost then custom chips
// low-level hardware recnfigurability
// FPGA architecutre offers many design choices
// FPGAs consist of different types of programmable blocks
// "FPGAs are recofigurable computer chips that can be programmed to implement any digital circuit"
// prefabricated routing tracks with programmable switches
// Functionality of all FPGA blocks is controlled by SRAM cells
// Milloions of SRAM cells
// HDL is converted to bitstream
// Bitstream is used to programm all configuration SRAM cells
// Lower NRE cost then ASICs
// Shorter time to market then ASICs
// off the shelf FPGA can be used to implement design in a matter of weeks
// Skipping phisical design, layout, fabrication and verification
// Allow continous hardware upgrades by loading new bitstreams in the field
// Considered a compelling solution for small and medium sized designs 
// Exact hardware for every application
// Exact datapath width, pipeline stages, parallel units as required
// Can achieve higher efficiency than CPU or GPU
// Can implement instruction-free streaming hardware
// Can implement a custom instruction set
// Adopted in many domains.
// "adoption of FPGAs in many application domains including wireless communi- cations, embedded signal processing, networking, ASIC prototyping, high-frequency trading, and many more"
// Deployed on large scale in datacenters, packet processing, machine learning
// Lower efficiency than ASICs
// FPGA on average 35 times larger than ASIC implementation
// FPGA on average 4 times slower than ASIC implementation
// For designs that utilize other FPGA blocks the gap is smaller, still 9 times large
// FPGA architects seek to reduce gap while maintaining programmability
// Early FPGAs ere simple arrays of logic blocks
// Modern FPGAs are complex heterogeneous architectures that have more block types
// Modern FPGA have blocks like BRAM, DSP, processors, external interfaces
// FPGA architectures are evaluated based on the efficiency implementing a wide variety of designs
// There are academic test suites for evaluating FPGA architectures
// VTR is a CAD system for layouting designs on FPGAs
// CAD system applies a series of complex optimizations 
// CAD system converts RTL design to netlist.
// CAD system maps netlist to FPGA blocks
// CAD system places blocks on FPGA and routes the connections between them
// CAD system outputs bitstream implementation
// total area is a key metric
// "Total area is the sum of the areas of the FPGA blocks used by the application, along with the programmable routing included with them."
// Timing analyzer finds the critical path through blocks and routing
// Critical path limits maximum clock frequency
// Power consumption is estimated based on resources used and signal toggle rate
// _hardened_ blocks are blocks that are implemented as ASICs
// What functionality to harden is a design choice
// What area of the FPGA to use for hardened blocks is a design choice
// Hardened blocks can still have some level of configurability
// How flexible the hardened blocks are is a design choice
// Hardened blocks are faster, smaller and more power efficient than programmable blocks
// Tradeoff between flexibility and efficiency
// Unused hardened blocks are wasted silicon
// Problems with slow routing to hardened blocks, if they are far away
// PAL first reconfigurable computing devices
// PAL does not scale well, area increases quadratically with IO size
// CPLD includes mulitple PALs and programmable routing in a package
// 1984 Xilinx pioneers first LUT based FPGA
// SRAM based LUTs with interconnects between them
// Scales well
// Much higher area efficiency than and/or based designs
// LUTs form the fundamental logic element in all commercial FPGAs
// Alternative designs perform worse than LUTs
// K-LUT implements a K-input LUT
// K-LUT stores truth table in SRAM cells,
// K input signals are used as multiplexer to select line
// truth table contains 2^K values
// A basic logic element (BLE) is a K-LUT with an output register
// A BLE can implement DFF or a K-LUT
// A BLE has K inputs and 2 outputs, one for routing and one for feedback inside the LE
// Logic blocks are composed of multiple (N) BLEs
// Logic blocks have a local interconnect
// The local interconnect connects the inputs of the LB and the feedback outputs of the BLEs to the inputs of the BLEs.
// The local interconnect is often a arranged as a local full or partial crossbar.
// See figure 4 in the paper.
// Over time K and N have increased.
// More K means more functionality in a single LUT
// More K leads to less logic in the critical path
// More N means less demand for fast inter-LB routing
// The area of the LUT increases exponentially with K as more SRAM cells are needed (2^K)
// More K linearly degrades the speed of the LUT
// if the local interconnect is a crossbbar, its size increases quadratically with N
// if the local interconnect is a crossbbar, its size decreases linearly with N
// Empirically the best size for K is 4-6 and for N it is 3-10
// First LUT-based FPGA from Xilinx: N = 2, K = 3
// around 2000: 4-LUTs common
// Study: 4-LUTs vs 6-LUTS: 6-LUTS:14% more perf, 17% bigger
// Fracturable LUTs can be broken down into smaller LUTs, but limitations like shared inputs
// FPGA architectures from Xilinx and Altera converge to relatively large LBs wth 8 and 10 N
// Future designs even bigger LBs
// inter-LB wire delay scales poorly with process shrink
// Larger LB sizes can lead to faster CAD tool runtimes
// Modern FPGAs have more than 1 FFs per BLE
// Even though there are optimization the core ideas stayed similiar
// 22% of logic elements in FPGAs are implementing arithmetic
// These operations can be implemented with LUTs but are inefficient
// A ripple carry adder requires 2 * the number of bits LUTs
// This leads to high logic utilization and long critical paths
// All modern FPGAs include hardened arithmetic circuitry in their logic blocks
// How the arithmetic is accelerated is a design choice
// Can be a dedicated adder between two LUTs
// Can be just a fastpath for the carry bit
// At least 3x faster than LUT based implementations
// NOTE: there is more detail on the different types of arithmetic optimizations in the paper
// Recently deeplearning has become a key workload
// Deeplearning has multiply-accumulate operations at its core, which could benefit from hardened bigger hardened arithmetic
// Programmabble routing is over 50% of the area of an FPGA
// Programmable routing accounts for over half the critical path delay
// High multiplier density in signal processing and communication applications
// Main design philosophy of DSP block is to minimize the number of soft logic used to implement common DSP algorithms
// FPGA CAD tools will automatically map multiplication to DSP blocks
// Bigger FPGA designs always always require a memory buffer
// Making soft memory out of LUTs is over 100x less dense than SRAM cells
// Modern FPGAs are about 25% BRAM
* [[[Bot21]]]
+Andrew Boutros, Betz Vaughn+,
_FPGA architecture: Principles and progression_,
IEEE Circuits and Systems Magazine 21.2 (2021): 4-29.,
2021
link:pass:[https://doi.org/10.1109/MCAS.2021.3071607][üîó^]
link:pass:[https://sci-hub.st/10.1109/MCAS.2021.3071607][üìÅ^]

// TODO: This is an application note, how to cite?
// TODO: Especially Cri in the link is wrong
// TODO: Source for one definition. Necessary?
// A critical path is a path in the design which must meet certain critical timing requirements in order for the system to function properly
* [[[Cri95]]]
_Critical path analysis for field-programmable gate arrays_,
Microprocessors and Microsystems, Volume 19, Issue 7, Pages 435-439,
1995
link:pass:[https://doi.org/10.1016/0141-9331(95)90010-1][üîó^]
link:pass:[https://sci-hub.st/10.1016/0141-9331(95)90010-1][üìÅ^]

// Reference thesis:
// * https://webthesis.biblio.polito.it/7573/1/tesi.pdf
// * https://scholarworks.gvsu.edu/cgi/viewcontent.cgi?article=1754&context=theses

include::styles/trailing-scripts.adoc[]
